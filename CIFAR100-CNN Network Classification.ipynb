{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ILkT4z8zw0t",
        "outputId": "4a896efd-4cb5-46dd-dc67-84fd9c58a3fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 2.3.0+cu121\n"
          ]
        }
      ],
      "source": [
        "# necessary imports\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import numpy as np # for transformation\n",
        "\n",
        "import torch\n",
        "import torchvision # load datasets\n",
        "import torchvision.transforms as transforms # transform data\n",
        "import torch.nn as nn # basic building block for neural networks\n",
        "import torch.nn.functional as F # import functions like Relu\n",
        "import torch.optim as optim # optimizer\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "print(\"Using torch\", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omVQuj6vz2S-",
        "outputId": "93bb6306-aab9-4aa6-a5da-f6b0daf9b411"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c66c01d8cb0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "torch.manual_seed(26)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrn-as2uz6SD"
      },
      "source": [
        "#### Classification for CIFAR-100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzRL1s3rz87V"
      },
      "source": [
        "##### Load the dataset from torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09V0MoQHz3wX",
        "outputId": "2500c04b-7e72-44f2-d6ab-c162ee03cce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:05<00:00, 30620428.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "# composing several transforms together\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ]\n",
        ")\n",
        "\n",
        "batch_size = 256\n",
        "num_workers = 2\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                         download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=num_workers)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6LDtDqa0cc2"
      },
      "source": [
        "#### Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial network design, base network"
      ],
      "metadata": {
        "id": "r35-o166jX8r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pKF1smZC0YYO"
      },
      "outputs": [],
      "source": [
        "d_in = 3  # Input channels (RGB images)\n",
        "d_out = 100  # Number of classes in CIFAR100\n",
        "activation_fn = nn.ReLU()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIap149k0iiG",
        "outputId": "a235049a-ec80-420a-cf8d-5e2a678660be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNetDA520(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class ConvNetDA520(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNetDA520, self).__init__()\n",
        "    # Define the layers of your neural network\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.fc1 = nn.Linear(64 * 4 * 4, 256)\n",
        "    self.fc2 = nn.Linear(256, 100)  # Output layer for 100 classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Define the forward pass of your neural network\n",
        "    x = self.pool1(F.relu(self.conv1(x)))\n",
        "    x = self.pool2(F.relu(self.conv2(x)))\n",
        "    x = self.pool3(F.relu(self.conv3(x)))\n",
        "    x = x.view(-1, 64 * 4 * 4)  # Flatten the tensor for the fully connected layer\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "model = ConvNetDA520()\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKUgPirk0qAT"
      },
      "source": [
        "##### Create and Train CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lLwOLvz80ik0"
      },
      "outputs": [],
      "source": [
        "n_epoch = 200\n",
        "model = ConvNetDA520()\n",
        "\n",
        "# Function to initialize model weights\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "# Initialize model weights\n",
        "model.apply(weights_init)\n",
        "loss_fn = nn.CrossEntropyLoss()  # Suitable for classification tasks with multiple classes\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()  # Set the model to training mode\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "        data, target = data.to('cuda'), target.to('cuda')\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        output = model(data)  # Forward pass\n",
        "        loss = loss_fn(output, target)  # Calculate loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update model parameters\n",
        "        train_loss += loss.item()  # Sum up batch loss\n",
        "    train_loss /= len(trainloader)  # Average the loss over the number of batches\n",
        "    train_losses.append(train_loss)\n",
        "    print(f'Epoch {epoch}, Training Loss: {train_loss:.4f}')"
      ],
      "metadata": {
        "id": "VcLPUmQ_WmDN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mrMEhhk_0inM"
      },
      "outputs": [],
      "source": [
        "#Test the model\n",
        "def test():\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in testloader:\n",
        "            data, target = data.to('cuda'), target.to('cuda')\n",
        "            output = model(data)\n",
        "            test_loss += loss_fn(output, target).item()  # Sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()  # Count correct predictions\n",
        "            # Average the loss over the number of batches\n",
        "    test_loss /= len(testloader)\n",
        "    test_losses.append(test_loss)\n",
        "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(testloader.dataset)} ({100. * correct / len(testloader.dataset):.2f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4gBlLHOlm4yk",
        "outputId": "f1145cf4-130d-409c-bfa4-4130ff9a2da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Batch: 0, Loss: 2.3373680114746094\n",
            "Epoch: 1, Batch: 100, Loss: 2.7677581310272217\n",
            "Epoch: 1, Batch: 200, Loss: 2.557374954223633\n",
            "Epoch: 1, Batch: 300, Loss: 2.916543483734131\n",
            "Epoch: 1, Batch: 400, Loss: 2.944985866546631\n",
            "Epoch: 1, Batch: 500, Loss: 2.6073224544525146\n",
            "Epoch: 1, Batch: 600, Loss: 2.77018141746521\n",
            "Epoch: 1, Batch: 700, Loss: 2.790825843811035\n",
            "Epoch: 1, Batch: 800, Loss: 1.9067667722702026\n",
            "Epoch: 1, Batch: 900, Loss: 2.0743439197540283\n",
            "Epoch: 1, Batch: 1000, Loss: 2.10469651222229\n",
            "Epoch: 1, Batch: 1100, Loss: 2.5405633449554443\n",
            "Epoch: 1, Batch: 1200, Loss: 2.504849433898926\n",
            "Epoch: 1, Batch: 1300, Loss: 2.6175758838653564\n",
            "Epoch: 1, Batch: 1400, Loss: 2.0971932411193848\n",
            "Epoch: 1, Batch: 1500, Loss: 2.3966684341430664\n",
            "Epoch 1 complete, Average Loss: 2.484972962643653\n",
            "Test set: Average loss: 0.0791, Accuracy: 3615/10000 (36.15%)\n",
            "Epoch: 2, Batch: 0, Loss: 2.1904184818267822\n",
            "Epoch: 2, Batch: 100, Loss: 1.8324291706085205\n",
            "Epoch: 2, Batch: 200, Loss: 1.7541518211364746\n",
            "Epoch: 2, Batch: 300, Loss: 2.5211968421936035\n",
            "Epoch: 2, Batch: 400, Loss: 2.2282867431640625\n",
            "Epoch: 2, Batch: 500, Loss: 2.8705594539642334\n",
            "Epoch: 2, Batch: 600, Loss: 2.036111831665039\n",
            "Epoch: 2, Batch: 700, Loss: 2.6986727714538574\n",
            "Epoch: 2, Batch: 800, Loss: 2.572864294052124\n",
            "Epoch: 2, Batch: 900, Loss: 2.429844617843628\n",
            "Epoch: 2, Batch: 1000, Loss: 2.0484790802001953\n",
            "Epoch: 2, Batch: 1100, Loss: 1.9062774181365967\n",
            "Epoch: 2, Batch: 1200, Loss: 2.3442142009735107\n",
            "Epoch: 2, Batch: 1300, Loss: 2.708784580230713\n",
            "Epoch: 2, Batch: 1400, Loss: 2.2911312580108643\n",
            "Epoch: 2, Batch: 1500, Loss: 1.855992317199707\n",
            "Epoch 2 complete, Average Loss: 2.2456625466612157\n",
            "Test set: Average loss: 0.0754, Accuracy: 3841/10000 (38.41%)\n",
            "Epoch: 3, Batch: 0, Loss: 1.3195743560791016\n",
            "Epoch: 3, Batch: 100, Loss: 2.198513984680176\n",
            "Epoch: 3, Batch: 200, Loss: 1.9579635858535767\n",
            "Epoch: 3, Batch: 300, Loss: 2.168083667755127\n",
            "Epoch: 3, Batch: 400, Loss: 2.0212619304656982\n",
            "Epoch: 3, Batch: 500, Loss: 1.845155954360962\n",
            "Epoch: 3, Batch: 600, Loss: 2.1459710597991943\n",
            "Epoch: 3, Batch: 700, Loss: 1.5949490070343018\n",
            "Epoch: 3, Batch: 800, Loss: 1.9582037925720215\n",
            "Epoch: 3, Batch: 900, Loss: 1.513210415840149\n",
            "Epoch: 3, Batch: 1000, Loss: 2.491880416870117\n",
            "Epoch: 3, Batch: 1100, Loss: 2.1084771156311035\n",
            "Epoch: 3, Batch: 1200, Loss: 1.9645400047302246\n",
            "Epoch: 3, Batch: 1300, Loss: 2.3414576053619385\n",
            "Epoch: 3, Batch: 1400, Loss: 2.1843578815460205\n",
            "Epoch: 3, Batch: 1500, Loss: 2.2429754734039307\n",
            "Epoch 3 complete, Average Loss: 2.0553502559814403\n",
            "Test set: Average loss: 0.0741, Accuracy: 3978/10000 (39.78%)\n",
            "Epoch: 4, Batch: 0, Loss: 1.6137282848358154\n",
            "Epoch: 4, Batch: 100, Loss: 1.615934133529663\n",
            "Epoch: 4, Batch: 200, Loss: 2.119621992111206\n",
            "Epoch: 4, Batch: 300, Loss: 2.3893418312072754\n",
            "Epoch: 4, Batch: 400, Loss: 1.9122474193572998\n",
            "Epoch: 4, Batch: 500, Loss: 1.9172321557998657\n",
            "Epoch: 4, Batch: 600, Loss: 1.8071277141571045\n",
            "Epoch: 4, Batch: 700, Loss: 2.4053688049316406\n",
            "Epoch: 4, Batch: 800, Loss: 2.3320419788360596\n",
            "Epoch: 4, Batch: 900, Loss: 1.8433862924575806\n",
            "Epoch: 4, Batch: 1000, Loss: 1.8252931833267212\n",
            "Epoch: 4, Batch: 1100, Loss: 2.1227526664733887\n",
            "Epoch: 4, Batch: 1200, Loss: 1.3342281579971313\n",
            "Epoch: 4, Batch: 1300, Loss: 1.5898674726486206\n",
            "Epoch: 4, Batch: 1400, Loss: 1.975030779838562\n",
            "Epoch: 4, Batch: 1500, Loss: 1.9114264249801636\n",
            "Epoch 4 complete, Average Loss: 1.9063295113758179\n",
            "Test set: Average loss: 0.0733, Accuracy: 4129/10000 (41.29%)\n",
            "Epoch: 5, Batch: 0, Loss: 1.328078031539917\n",
            "Epoch: 5, Batch: 100, Loss: 1.6950284242630005\n",
            "Epoch: 5, Batch: 200, Loss: 1.893574833869934\n",
            "Epoch: 5, Batch: 300, Loss: 1.0776712894439697\n",
            "Epoch: 5, Batch: 400, Loss: 1.6106555461883545\n",
            "Epoch: 5, Batch: 500, Loss: 1.4614660739898682\n",
            "Epoch: 5, Batch: 600, Loss: 2.099771499633789\n",
            "Epoch: 5, Batch: 700, Loss: 1.5968170166015625\n",
            "Epoch: 5, Batch: 800, Loss: 1.9607470035552979\n",
            "Epoch: 5, Batch: 900, Loss: 1.3589141368865967\n",
            "Epoch: 5, Batch: 1000, Loss: 1.7814815044403076\n",
            "Epoch: 5, Batch: 1100, Loss: 1.78098464012146\n",
            "Epoch: 5, Batch: 1200, Loss: 2.146580696105957\n",
            "Epoch: 5, Batch: 1300, Loss: 1.9043385982513428\n",
            "Epoch: 5, Batch: 1400, Loss: 1.8565802574157715\n",
            "Epoch: 5, Batch: 1500, Loss: 1.889999508857727\n",
            "Epoch 5 complete, Average Loss: 1.7731470923658676\n",
            "Test set: Average loss: 0.0747, Accuracy: 4035/10000 (40.35%)\n",
            "Epoch: 6, Batch: 0, Loss: 1.3698583841323853\n",
            "Epoch: 6, Batch: 100, Loss: 1.6435024738311768\n",
            "Epoch: 6, Batch: 200, Loss: 1.3395473957061768\n",
            "Epoch: 6, Batch: 300, Loss: 1.6473487615585327\n",
            "Epoch: 6, Batch: 400, Loss: 1.5471702814102173\n",
            "Epoch: 6, Batch: 500, Loss: 1.7153403759002686\n",
            "Epoch: 6, Batch: 600, Loss: 2.122390031814575\n",
            "Epoch: 6, Batch: 700, Loss: 1.2450168132781982\n",
            "Epoch: 6, Batch: 800, Loss: 1.8170243501663208\n",
            "Epoch: 6, Batch: 900, Loss: 2.074695110321045\n",
            "Epoch: 6, Batch: 1000, Loss: 1.8579479455947876\n",
            "Epoch: 6, Batch: 1100, Loss: 1.7120684385299683\n",
            "Epoch: 6, Batch: 1200, Loss: 1.3898450136184692\n",
            "Epoch: 6, Batch: 1300, Loss: 1.9783740043640137\n",
            "Epoch: 6, Batch: 1400, Loss: 1.3363244533538818\n",
            "Epoch: 6, Batch: 1500, Loss: 1.7252309322357178\n",
            "Epoch 6 complete, Average Loss: 1.6509653094176366\n",
            "Test set: Average loss: 0.0767, Accuracy: 4012/10000 (40.12%)\n",
            "Epoch: 7, Batch: 0, Loss: 0.9548422694206238\n",
            "Epoch: 7, Batch: 100, Loss: 1.2678616046905518\n",
            "Epoch: 7, Batch: 200, Loss: 1.9358874559402466\n",
            "Epoch: 7, Batch: 300, Loss: 1.7993638515472412\n",
            "Epoch: 7, Batch: 400, Loss: 1.2803031206130981\n",
            "Epoch: 7, Batch: 500, Loss: 1.0467299222946167\n",
            "Epoch: 7, Batch: 600, Loss: 1.4878578186035156\n",
            "Epoch: 7, Batch: 700, Loss: 1.2467248439788818\n",
            "Epoch: 7, Batch: 800, Loss: 1.2692800760269165\n",
            "Epoch: 7, Batch: 900, Loss: 1.4762629270553589\n",
            "Epoch: 7, Batch: 1000, Loss: 1.3740363121032715\n",
            "Epoch: 7, Batch: 1100, Loss: 1.7958933115005493\n",
            "Epoch: 7, Batch: 1200, Loss: 2.5374228954315186\n",
            "Epoch: 7, Batch: 1300, Loss: 1.653843641281128\n",
            "Epoch: 7, Batch: 1400, Loss: 1.7450025081634521\n",
            "Epoch: 7, Batch: 1500, Loss: 1.6076879501342773\n",
            "Epoch 7 complete, Average Loss: 1.546892213653618\n",
            "Test set: Average loss: 0.0765, Accuracy: 4116/10000 (41.16%)\n",
            "Epoch: 8, Batch: 0, Loss: 1.6141096353530884\n",
            "Epoch: 8, Batch: 100, Loss: 1.125641107559204\n",
            "Epoch: 8, Batch: 200, Loss: 1.100766897201538\n",
            "Epoch: 8, Batch: 300, Loss: 1.6338483095169067\n",
            "Epoch: 8, Batch: 400, Loss: 1.3731012344360352\n",
            "Epoch: 8, Batch: 500, Loss: 1.7348122596740723\n",
            "Epoch: 8, Batch: 600, Loss: 0.8021436333656311\n",
            "Epoch: 8, Batch: 700, Loss: 1.4660052061080933\n",
            "Epoch: 8, Batch: 800, Loss: 1.1467901468276978\n",
            "Epoch: 8, Batch: 900, Loss: 1.5502595901489258\n",
            "Epoch: 8, Batch: 1000, Loss: 1.3328278064727783\n",
            "Epoch: 8, Batch: 1100, Loss: 1.1111637353897095\n",
            "Epoch: 8, Batch: 1200, Loss: 1.4160510301589966\n",
            "Epoch: 8, Batch: 1300, Loss: 1.5092334747314453\n",
            "Epoch: 8, Batch: 1400, Loss: 1.6052910089492798\n",
            "Epoch: 8, Batch: 1500, Loss: 1.793614149093628\n",
            "Epoch 8 complete, Average Loss: 1.4506168696106967\n",
            "Test set: Average loss: 0.0789, Accuracy: 4079/10000 (40.79%)\n",
            "Epoch: 9, Batch: 0, Loss: 1.2640432119369507\n",
            "Epoch: 9, Batch: 100, Loss: 1.3171539306640625\n",
            "Epoch: 9, Batch: 200, Loss: 1.4240784645080566\n",
            "Epoch: 9, Batch: 300, Loss: 1.400346279144287\n",
            "Epoch: 9, Batch: 400, Loss: 1.2779673337936401\n",
            "Epoch: 9, Batch: 500, Loss: 0.9392971992492676\n",
            "Epoch: 9, Batch: 600, Loss: 1.2258617877960205\n",
            "Epoch: 9, Batch: 700, Loss: 0.9878720045089722\n",
            "Epoch: 9, Batch: 800, Loss: 1.161332130432129\n",
            "Epoch: 9, Batch: 900, Loss: 1.6695157289505005\n",
            "Epoch: 9, Batch: 1000, Loss: 0.9230855703353882\n",
            "Epoch: 9, Batch: 1100, Loss: 1.4531748294830322\n",
            "Epoch: 9, Batch: 1200, Loss: 1.0538989305496216\n",
            "Epoch: 9, Batch: 1300, Loss: 1.8493177890777588\n",
            "Epoch: 9, Batch: 1400, Loss: 1.613332986831665\n",
            "Epoch: 9, Batch: 1500, Loss: 1.1613105535507202\n",
            "Epoch 9 complete, Average Loss: 1.3573216385972553\n",
            "Test set: Average loss: 0.0820, Accuracy: 4008/10000 (40.08%)\n",
            "Epoch: 10, Batch: 0, Loss: 1.4517439603805542\n",
            "Epoch: 10, Batch: 100, Loss: 1.4051532745361328\n",
            "Epoch: 10, Batch: 200, Loss: 0.8987423181533813\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-e07ca8e503f5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-122-5fad243a7355>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train and test the model\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # Learning rate scheduler\n",
        "\n",
        "for epoch in range(1, n_epoch + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCMRstw11yKQ"
      },
      "source": [
        "##### Experiment with Hyperparameters and Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameter grid and used for loops to experiment with different hyperparameters programmatically."
      ],
      "metadata": {
        "id": "DveC5cLcFgaU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nMIRfxUr2BT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969dc568-d73c-4d01-afde-68b023220580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mGörüntülenen çıkış son 5000 satıra kısaltıldı.\u001b[0m\n",
            "Epoch 26, Loss: 2.403191789489268\n",
            "Epoch 27, Loss: 2.3959359997678598\n",
            "Epoch 28, Loss: 2.39312101355599\n",
            "Epoch 29, Loss: 2.383316360623635\n",
            "Epoch 30, Loss: 2.3934703457081103\n",
            "Epoch 31, Loss: 2.3824440236286737\n",
            "Epoch 32, Loss: 2.379054815110648\n",
            "Epoch 33, Loss: 2.3878768754127386\n",
            "Epoch 34, Loss: 2.380481389020105\n",
            "Epoch 35, Loss: 2.3862892008193617\n",
            "Epoch 36, Loss: 2.3759565046986046\n",
            "Epoch 37, Loss: 2.3887488726154924\n",
            "Epoch 38, Loss: 2.381001355855361\n",
            "Epoch 39, Loss: 2.3836162693970038\n",
            "Epoch 40, Loss: 2.395454063744801\n",
            "Epoch 41, Loss: 2.3861158519144863\n",
            "Epoch 42, Loss: 2.4064861151873305\n",
            "Epoch 43, Loss: 2.3969866270604343\n",
            "Epoch 44, Loss: 2.405819177475122\n",
            "Epoch 45, Loss: 2.4262562331641115\n",
            "Epoch 46, Loss: 2.405751840690213\n",
            "Epoch 47, Loss: 2.416478221983556\n",
            "Epoch 48, Loss: 2.433315477102919\n",
            "Epoch 49, Loss: 2.4227990724546524\n",
            "Epoch 50, Loss: 2.4300280990807908\n",
            "Epoch 51, Loss: 2.4353068557846576\n",
            "Epoch 52, Loss: 2.4225086330452843\n",
            "Epoch 53, Loss: 2.4507895698937614\n",
            "Epoch 54, Loss: 2.4704941390725352\n",
            "Epoch 55, Loss: 2.462760410040541\n",
            "Epoch 56, Loss: 2.48202691998933\n",
            "Epoch 57, Loss: 2.4797733903236097\n",
            "Epoch 58, Loss: 2.4811810104133527\n",
            "Epoch 59, Loss: 2.4736240311047\n",
            "Epoch 60, Loss: 2.4678376466416947\n",
            "Epoch 61, Loss: 2.5162321708696274\n",
            "Epoch 62, Loss: 2.4944406107563495\n",
            "Epoch 63, Loss: 2.5072910296337683\n",
            "Epoch 64, Loss: 2.529585279467161\n",
            "Epoch 65, Loss: 2.5112449490200834\n",
            "Epoch 66, Loss: 2.5044136966585806\n",
            "Epoch 67, Loss: 2.5105861507718217\n",
            "Epoch 68, Loss: 2.5326588505979086\n",
            "Epoch 69, Loss: 2.550067927221508\n",
            "Epoch 70, Loss: 2.5363842387638433\n",
            "Epoch 71, Loss: 2.548954142633911\n",
            "Epoch 72, Loss: 2.5527068739352017\n",
            "Epoch 73, Loss: 2.5681933751496513\n",
            "Epoch 74, Loss: 2.5753144091352476\n",
            "Epoch 75, Loss: 2.5921050052508674\n",
            "Epoch 76, Loss: 2.6143114775647898\n",
            "Epoch 77, Loss: 2.5916621558501594\n",
            "Epoch 78, Loss: 2.5733981489220543\n",
            "Epoch 79, Loss: 2.5922496213632473\n",
            "Epoch 80, Loss: 2.6038465409937417\n",
            "Epoch 81, Loss: 2.622197268564073\n",
            "Epoch 82, Loss: 2.596498717584878\n",
            "Epoch 83, Loss: 2.6137031322854862\n",
            "Epoch 84, Loss: 2.6270907275816975\n",
            "Epoch 85, Loss: 2.6072485994195085\n",
            "Epoch 86, Loss: 2.6102517998736836\n",
            "Epoch 87, Loss: 2.6201273555036093\n",
            "Epoch 88, Loss: 2.62732053534759\n",
            "Epoch 89, Loss: 2.601614990502672\n",
            "Epoch 90, Loss: 2.6500860835280258\n",
            "Epoch 91, Loss: 2.6512258901925345\n",
            "Epoch 92, Loss: 2.6279415680319453\n",
            "Epoch 93, Loss: 2.6131765741826323\n",
            "Epoch 94, Loss: 2.6293853275916157\n",
            "Epoch 95, Loss: 2.643060700210464\n",
            "Epoch 96, Loss: 2.628154056456388\n",
            "Epoch 97, Loss: 2.6625830415264726\n",
            "Epoch 98, Loss: 2.617656912187786\n",
            "Epoch 99, Loss: 2.6250239668599784\n",
            "Epoch 100, Loss: 2.664546256937334\n",
            "Epoch 101, Loss: 2.6328164108878815\n",
            "Epoch 102, Loss: 2.6615857625251538\n",
            "Epoch 103, Loss: 2.6458422716926124\n",
            "Epoch 104, Loss: 2.6381207995707423\n",
            "Epoch 105, Loss: 2.646104880915883\n",
            "Epoch 106, Loss: 2.6380695121367568\n",
            "Epoch 107, Loss: 2.6455918927021953\n",
            "Epoch 108, Loss: 2.65603571611902\n",
            "Epoch 109, Loss: 2.6716826370609996\n",
            "Epoch 110, Loss: 2.6735889492437357\n",
            "Epoch 111, Loss: 2.656561704852697\n",
            "Epoch 112, Loss: 2.6611932503902698\n",
            "Epoch 113, Loss: 2.673118005018405\n",
            "Epoch 114, Loss: 2.6815403654142416\n",
            "Epoch 115, Loss: 2.669853726311413\n",
            "Epoch 116, Loss: 2.69773707670324\n",
            "Epoch 117, Loss: 2.72444980681095\n",
            "Epoch 118, Loss: 2.6949583115175253\n",
            "Epoch 119, Loss: 2.685731320277504\n",
            "Epoch 120, Loss: 2.6969290610469514\n",
            "Epoch 121, Loss: 2.71254617539818\n",
            "Epoch 122, Loss: 2.6822142823577844\n",
            "Epoch 123, Loss: 2.7011925326588817\n",
            "Epoch 124, Loss: 2.70489181841121\n",
            "Epoch 125, Loss: 2.7026904356449157\n",
            "Epoch 126, Loss: 2.6694695071491132\n",
            "Epoch 127, Loss: 2.674495105548283\n",
            "Epoch 128, Loss: 2.7051704675340287\n",
            "Epoch 129, Loss: 2.7160628416654093\n",
            "Epoch 130, Loss: 2.710023181822599\n",
            "Epoch 131, Loss: 2.7102525818073535\n",
            "Epoch 132, Loss: 2.696930868698813\n",
            "Epoch 133, Loss: 2.693682121651252\n",
            "Epoch 134, Loss: 2.7108473428679853\n",
            "Epoch 135, Loss: 2.6900481805777\n",
            "Epoch 136, Loss: 2.708161085462936\n",
            "Epoch 137, Loss: 2.713954177353998\n",
            "Epoch 138, Loss: 2.733706037711609\n",
            "Epoch 139, Loss: 2.7362118947231555\n",
            "Epoch 140, Loss: 2.7319155172313874\n",
            "Epoch 141, Loss: 2.697837897121449\n",
            "Epoch 142, Loss: 2.717280158149007\n",
            "Epoch 143, Loss: 2.7702269354439757\n",
            "Epoch 144, Loss: 2.7112990280856257\n",
            "Epoch 145, Loss: 2.714408026174511\n",
            "Epoch 146, Loss: 2.864795224440982\n",
            "Epoch 147, Loss: 2.71590232940586\n",
            "Epoch 148, Loss: 2.738148013344201\n",
            "Epoch 149, Loss: 2.7413912385016146\n",
            "Epoch 150, Loss: 2.7283144302075475\n",
            "Epoch 151, Loss: 2.7136818603481476\n",
            "Epoch 152, Loss: 2.7178905636758146\n",
            "Epoch 153, Loss: 2.724218386061051\n",
            "Epoch 154, Loss: 2.709104561287424\n",
            "Epoch 155, Loss: 2.7038738354087792\n",
            "Epoch 156, Loss: 2.7286332083480134\n",
            "Epoch 157, Loss: 2.706425928093893\n",
            "Epoch 158, Loss: 2.7714777453171324\n",
            "Epoch 159, Loss: 2.737739018009752\n",
            "Epoch 160, Loss: 2.865935444526965\n",
            "Epoch 161, Loss: 2.722000938547237\n",
            "Epoch 162, Loss: 2.7266278661730343\n",
            "Epoch 163, Loss: 2.7900763144883354\n",
            "Epoch 164, Loss: 2.736891538590726\n",
            "Epoch 165, Loss: 2.7406232607029284\n",
            "Epoch 166, Loss: 2.736712145226081\n",
            "Epoch 167, Loss: 2.733567554627538\n",
            "Epoch 168, Loss: 2.7191764800749776\n",
            "Epoch 169, Loss: 2.735077479001506\n",
            "Epoch 170, Loss: 2.724115633293796\n",
            "Epoch 171, Loss: 2.760411325470566\n",
            "Epoch 172, Loss: 2.7189639649732644\n",
            "Epoch 173, Loss: 2.7270049907057485\n",
            "Epoch 174, Loss: 2.7575017573583462\n",
            "Epoch 175, Loss: 2.7326918561440294\n",
            "Epoch 176, Loss: 2.7017479622760394\n",
            "Epoch 177, Loss: 2.6964499897054393\n",
            "Epoch 178, Loss: 2.737304778050279\n",
            "Epoch 179, Loss: 2.7243311667381347\n",
            "Epoch 180, Loss: 2.748263673404294\n",
            "Epoch 181, Loss: 2.747068723449317\n",
            "Epoch 182, Loss: 2.739541810034486\n",
            "Epoch 183, Loss: 2.742414674490614\n",
            "Epoch 184, Loss: 2.7298741825401325\n",
            "Epoch 185, Loss: 2.7765881916141266\n",
            "Epoch 186, Loss: 2.703083701908131\n",
            "Epoch 187, Loss: 2.736024975471789\n",
            "Epoch 188, Loss: 2.732384746946642\n",
            "Epoch 189, Loss: 2.7734923867313452\n",
            "Epoch 190, Loss: 2.8812159877603927\n",
            "Epoch 191, Loss: 2.7015920997885488\n",
            "Epoch 192, Loss: 2.906929567494356\n",
            "Epoch 193, Loss: 2.7128812431374474\n",
            "Epoch 194, Loss: 2.7410161725395477\n",
            "Epoch 195, Loss: 2.7447591728871434\n",
            "Epoch 196, Loss: 2.7157034032485066\n",
            "Epoch 197, Loss: 2.6965185265102045\n",
            "Epoch 198, Loss: 2.7054374035057203\n",
            "Epoch 199, Loss: 2.7092472641059504\n",
            "Accuracy: 25.05%\n",
            "Training with params: {'batch_size': 128, 'lr': 0.1, 'optimizer': 'Adam'}\n",
            "Epoch 1, Loss: 41.064938352540935\n",
            "Epoch 2, Loss: 4.626507576164382\n",
            "Epoch 3, Loss: 4.628150348468205\n",
            "Epoch 4, Loss: 4.626866878450984\n",
            "Epoch 5, Loss: 4.62747122259701\n",
            "Epoch 6, Loss: 4.627884369676985\n",
            "Epoch 7, Loss: 4.627629159356627\n",
            "Epoch 8, Loss: 4.6289300577110035\n",
            "Epoch 9, Loss: 4.627622805592959\n",
            "Epoch 10, Loss: 4.627216074472803\n",
            "Epoch 11, Loss: 4.627486104550569\n",
            "Epoch 12, Loss: 4.626833566924191\n",
            "Epoch 13, Loss: 4.627319801798867\n",
            "Epoch 14, Loss: 4.627438589130216\n",
            "Epoch 15, Loss: 4.627966399083052\n",
            "Epoch 16, Loss: 4.62728403291434\n",
            "Epoch 17, Loss: 4.627047816810705\n",
            "Epoch 18, Loss: 4.6278703682258\n",
            "Epoch 19, Loss: 4.627615545716737\n",
            "Epoch 20, Loss: 4.62794803780363\n",
            "Epoch 21, Loss: 4.627841650677459\n",
            "Epoch 22, Loss: 4.626861263723934\n",
            "Epoch 23, Loss: 4.6282877946448755\n",
            "Epoch 24, Loss: 4.628180786776726\n",
            "Epoch 25, Loss: 4.627568181518399\n",
            "Epoch 26, Loss: 4.626607924166238\n",
            "Epoch 27, Loss: 4.626464368132374\n",
            "Epoch 28, Loss: 4.627246011553518\n",
            "Epoch 29, Loss: 4.626213113975037\n",
            "Epoch 30, Loss: 4.628142616632954\n",
            "Epoch 31, Loss: 4.627034443418693\n",
            "Epoch 32, Loss: 4.628171765895756\n",
            "Epoch 33, Loss: 4.626553602535706\n",
            "Epoch 34, Loss: 4.6273863760711595\n",
            "Epoch 35, Loss: 4.6280345063075385\n",
            "Epoch 36, Loss: 4.626783359995889\n",
            "Epoch 37, Loss: 4.6273720685173485\n",
            "Epoch 38, Loss: 4.628195441897263\n",
            "Epoch 39, Loss: 4.627921248335973\n",
            "Epoch 40, Loss: 4.628407783215613\n",
            "Epoch 41, Loss: 4.627730508594562\n",
            "Epoch 42, Loss: 4.627634992379972\n",
            "Epoch 43, Loss: 4.62647978789971\n",
            "Epoch 44, Loss: 4.62622930814543\n",
            "Epoch 45, Loss: 4.627769810464376\n",
            "Epoch 46, Loss: 4.627352853565265\n",
            "Epoch 47, Loss: 4.627902619978961\n",
            "Epoch 48, Loss: 4.627730735427583\n",
            "Epoch 49, Loss: 4.627645108401013\n",
            "Epoch 50, Loss: 4.627420957131154\n",
            "Epoch 51, Loss: 4.627486374067224\n",
            "Epoch 52, Loss: 4.627494727863985\n",
            "Epoch 53, Loss: 4.6275933502275315\n",
            "Epoch 54, Loss: 4.626658268901698\n",
            "Epoch 55, Loss: 4.627050124165957\n",
            "Epoch 56, Loss: 4.627248484765173\n",
            "Epoch 57, Loss: 4.627428127981513\n",
            "Epoch 58, Loss: 4.6279790407556405\n",
            "Epoch 59, Loss: 4.628148665513529\n",
            "Epoch 60, Loss: 4.627586565968936\n",
            "Epoch 61, Loss: 4.626801440782864\n",
            "Epoch 62, Loss: 4.628056229837715\n",
            "Epoch 63, Loss: 4.627020799290493\n",
            "Epoch 64, Loss: 4.627211287808235\n",
            "Epoch 65, Loss: 4.6273979128474165\n",
            "Epoch 66, Loss: 4.628531681607141\n",
            "Epoch 67, Loss: 4.627046369225778\n",
            "Epoch 68, Loss: 4.627664981900579\n",
            "Epoch 69, Loss: 4.628687957363665\n",
            "Epoch 70, Loss: 4.628024134184698\n",
            "Epoch 71, Loss: 4.626787625920132\n",
            "Epoch 72, Loss: 4.628580400705947\n",
            "Epoch 73, Loss: 4.628238035284954\n",
            "Epoch 74, Loss: 4.627916250692304\n",
            "Epoch 75, Loss: 4.627915336043023\n",
            "Epoch 76, Loss: 4.6274610395016875\n",
            "Epoch 77, Loss: 4.626757763230892\n",
            "Epoch 78, Loss: 4.627157544236049\n",
            "Epoch 79, Loss: 4.627809100138867\n",
            "Epoch 80, Loss: 4.627391665183064\n",
            "Epoch 81, Loss: 4.627106967789437\n",
            "Epoch 82, Loss: 4.627856672877241\n",
            "Epoch 83, Loss: 4.627620965318607\n",
            "Epoch 84, Loss: 4.628165754820684\n",
            "Epoch 85, Loss: 4.626786292971247\n",
            "Epoch 86, Loss: 4.627829215105842\n",
            "Epoch 87, Loss: 4.62674049587201\n",
            "Epoch 88, Loss: 4.626644929656592\n",
            "Epoch 89, Loss: 4.627470660392586\n",
            "Epoch 90, Loss: 4.629097567799756\n",
            "Epoch 91, Loss: 4.627948728058954\n",
            "Epoch 92, Loss: 4.628944970152872\n",
            "Epoch 93, Loss: 4.627609127317853\n",
            "Epoch 94, Loss: 4.627588265997065\n",
            "Epoch 95, Loss: 4.62780138049894\n",
            "Epoch 96, Loss: 4.6272389638759295\n",
            "Epoch 97, Loss: 4.627165299242415\n",
            "Epoch 98, Loss: 4.627792240103798\n",
            "Epoch 99, Loss: 4.627967638127944\n",
            "Epoch 100, Loss: 4.627703578880681\n",
            "Epoch 101, Loss: 4.626364978682965\n",
            "Epoch 102, Loss: 4.628090108447062\n",
            "Epoch 103, Loss: 4.628621256260006\n",
            "Epoch 104, Loss: 4.628456548656649\n",
            "Epoch 105, Loss: 4.626450071554355\n",
            "Epoch 106, Loss: 4.628013894990887\n",
            "Epoch 107, Loss: 4.627161339420796\n",
            "Epoch 108, Loss: 4.627318618852464\n",
            "Epoch 109, Loss: 4.628418976083741\n",
            "Epoch 110, Loss: 4.627904344397738\n",
            "Epoch 111, Loss: 4.6273213652393705\n",
            "Epoch 112, Loss: 4.626143744534544\n",
            "Epoch 113, Loss: 4.627159464999538\n",
            "Epoch 114, Loss: 4.629016474994552\n",
            "Epoch 115, Loss: 4.627293713562324\n",
            "Epoch 116, Loss: 4.627006674666539\n",
            "Epoch 117, Loss: 4.626689729178348\n",
            "Epoch 118, Loss: 4.628425882295574\n",
            "Epoch 119, Loss: 4.627357336566272\n",
            "Epoch 120, Loss: 4.627465267620428\n",
            "Epoch 121, Loss: 4.627481655696469\n",
            "Epoch 122, Loss: 4.6273203798572125\n",
            "Epoch 123, Loss: 4.627252121410711\n",
            "Epoch 124, Loss: 4.627768182388657\n",
            "Epoch 125, Loss: 4.627693721400503\n",
            "Epoch 126, Loss: 4.62649891382593\n",
            "Epoch 127, Loss: 4.628485751578875\n",
            "Epoch 128, Loss: 4.627800074379767\n",
            "Epoch 129, Loss: 4.6271265376254425\n",
            "Epoch 130, Loss: 4.628159935212197\n",
            "Epoch 131, Loss: 4.628214484895282\n",
            "Epoch 132, Loss: 4.627793230364085\n",
            "Epoch 133, Loss: 4.627773270277721\n",
            "Epoch 134, Loss: 4.628074036229907\n",
            "Epoch 135, Loss: 4.626421669864898\n",
            "Epoch 136, Loss: 4.62776128471355\n",
            "Epoch 137, Loss: 4.627364364731342\n",
            "Epoch 138, Loss: 4.627832998095266\n",
            "Epoch 139, Loss: 4.6273020844325385\n",
            "Epoch 140, Loss: 4.628841651370154\n",
            "Epoch 141, Loss: 4.627919693432196\n",
            "Epoch 142, Loss: 4.626864679634114\n",
            "Epoch 143, Loss: 4.62678880642747\n",
            "Epoch 144, Loss: 4.626289784755853\n",
            "Epoch 145, Loss: 4.626828849772968\n",
            "Epoch 146, Loss: 4.627939537662984\n",
            "Epoch 147, Loss: 4.629180491123053\n",
            "Epoch 148, Loss: 4.627167956603458\n",
            "Epoch 149, Loss: 4.6281378958231345\n",
            "Epoch 150, Loss: 4.627825207722461\n",
            "Epoch 151, Loss: 4.627541667665057\n",
            "Epoch 152, Loss: 4.626221405575647\n",
            "Epoch 153, Loss: 4.6269349832364055\n",
            "Epoch 154, Loss: 4.628377847354431\n",
            "Epoch 155, Loss: 4.627369148956845\n",
            "Epoch 156, Loss: 4.626669483721408\n",
            "Epoch 157, Loss: 4.628248245209989\n",
            "Epoch 158, Loss: 4.62702211272686\n",
            "Epoch 159, Loss: 4.627655054907055\n",
            "Epoch 160, Loss: 4.627759387121176\n",
            "Epoch 161, Loss: 4.626828858309694\n",
            "Epoch 162, Loss: 4.627612731036018\n",
            "Epoch 163, Loss: 4.627503819477832\n",
            "Epoch 164, Loss: 4.628187124686473\n",
            "Epoch 165, Loss: 4.627794874293725\n",
            "Epoch 166, Loss: 4.628232740075387\n",
            "Epoch 167, Loss: 4.627960073368628\n",
            "Epoch 168, Loss: 4.628572115202999\n",
            "Epoch 169, Loss: 4.627316494427069\n",
            "Epoch 170, Loss: 4.627296155066136\n",
            "Epoch 171, Loss: 4.627222446529457\n",
            "Epoch 172, Loss: 4.62807168497149\n",
            "Epoch 173, Loss: 4.627897013788638\n",
            "Epoch 174, Loss: 4.626849150108864\n",
            "Epoch 175, Loss: 4.627328654384369\n",
            "Epoch 176, Loss: 4.6275566752304504\n",
            "Epoch 177, Loss: 4.6270188858441985\n",
            "Epoch 178, Loss: 4.628239414576069\n",
            "Epoch 179, Loss: 4.627577185325915\n",
            "Epoch 180, Loss: 4.627990934854883\n",
            "Epoch 181, Loss: 4.628020946327073\n",
            "Epoch 182, Loss: 4.627596603939905\n",
            "Epoch 183, Loss: 4.627516302611212\n",
            "Epoch 184, Loss: 4.627214422006436\n",
            "Epoch 185, Loss: 4.627055952311172\n",
            "Epoch 186, Loss: 4.627685522484352\n",
            "Epoch 187, Loss: 4.628020856081677\n",
            "Epoch 188, Loss: 4.626788250320708\n",
            "Epoch 189, Loss: 4.6281834148689915\n",
            "Epoch 190, Loss: 4.628099597628464\n",
            "Epoch 191, Loss: 4.628793138372319\n",
            "Epoch 192, Loss: 4.627057507214949\n",
            "Epoch 193, Loss: 4.62706497197261\n",
            "Epoch 194, Loss: 4.627075096530378\n",
            "Epoch 195, Loss: 4.62665961038731\n",
            "Epoch 196, Loss: 4.62720875240043\n",
            "Epoch 197, Loss: 4.627412436258457\n",
            "Epoch 198, Loss: 4.627464766392622\n",
            "Epoch 199, Loss: 4.627098121301597\n",
            "Accuracy: 1.00%\n",
            "Training with params: {'batch_size': 128, 'lr': 0.1, 'optimizer': 'SGD'}\n",
            "Epoch 1, Loss: 4.4474411193672045\n",
            "Epoch 2, Loss: 3.7873982084376734\n",
            "Epoch 3, Loss: 3.417487305448488\n",
            "Epoch 4, Loss: 3.1533119245563324\n",
            "Epoch 5, Loss: 2.9225768146612454\n",
            "Epoch 6, Loss: 2.7248448215787064\n",
            "Epoch 7, Loss: 2.538251757316882\n",
            "Epoch 8, Loss: 2.380457473532928\n",
            "Epoch 9, Loss: 2.2257367942644204\n",
            "Epoch 10, Loss: 2.0866831784967874\n",
            "Epoch 11, Loss: 1.9404335003679671\n",
            "Epoch 12, Loss: 1.8186237470573172\n",
            "Epoch 13, Loss: 1.695299281488599\n",
            "Epoch 14, Loss: 1.5662462336328022\n",
            "Epoch 15, Loss: 1.4420469651746628\n",
            "Epoch 16, Loss: 1.3321950713082042\n",
            "Epoch 17, Loss: 1.2193179377509504\n",
            "Epoch 18, Loss: 1.1114202338411374\n",
            "Epoch 19, Loss: 1.0198226398824122\n",
            "Epoch 20, Loss: 0.9299097315734609\n",
            "Epoch 21, Loss: 0.8527506015184895\n",
            "Epoch 22, Loss: 0.7787891959442812\n",
            "Epoch 23, Loss: 0.7069134040714224\n",
            "Epoch 24, Loss: 0.6474170089530213\n",
            "Epoch 25, Loss: 0.5976184846266456\n",
            "Epoch 26, Loss: 0.5452925250734515\n",
            "Epoch 27, Loss: 0.533367042765593\n",
            "Epoch 28, Loss: 0.47579406060831014\n",
            "Epoch 29, Loss: 0.4417799608329373\n",
            "Epoch 30, Loss: 0.4173490696055505\n",
            "Epoch 31, Loss: 0.391683932612924\n",
            "Epoch 32, Loss: 0.3778117237722172\n",
            "Epoch 33, Loss: 0.34443593385350674\n",
            "Epoch 34, Loss: 0.36248588889761046\n",
            "Epoch 35, Loss: 0.33143314827814735\n",
            "Epoch 36, Loss: 0.3079965806678128\n",
            "Epoch 37, Loss: 0.28966822237005013\n",
            "Epoch 38, Loss: 0.29311593728678303\n",
            "Epoch 39, Loss: 0.26887786188317686\n",
            "Epoch 40, Loss: 0.2936571258241716\n",
            "Epoch 41, Loss: 0.25856382958114604\n",
            "Epoch 42, Loss: 0.2597384828683513\n",
            "Epoch 43, Loss: 0.26411670737940335\n",
            "Epoch 44, Loss: 0.2385753175181806\n",
            "Epoch 45, Loss: 0.23274233881050668\n",
            "Epoch 46, Loss: 0.20702689280137992\n",
            "Epoch 47, Loss: 0.2583233277739771\n",
            "Epoch 48, Loss: 0.2425182194298948\n",
            "Epoch 49, Loss: 0.2526146897765072\n",
            "Epoch 50, Loss: 0.2277207219082376\n",
            "Epoch 51, Loss: 0.2587480000303606\n",
            "Epoch 52, Loss: 0.23645639875927543\n",
            "Epoch 53, Loss: 0.20895755007062727\n",
            "Epoch 54, Loss: 0.24387653102464688\n",
            "Epoch 55, Loss: 0.21063318155953648\n",
            "Epoch 56, Loss: 0.1586099216557296\n",
            "Epoch 57, Loss: 0.23473900258350555\n",
            "Epoch 58, Loss: 0.26286456604366715\n",
            "Epoch 59, Loss: 0.21684387678285236\n",
            "Epoch 60, Loss: 0.26356568585728746\n",
            "Epoch 61, Loss: 0.27413684192120724\n",
            "Epoch 62, Loss: 0.19686619718285167\n",
            "Epoch 63, Loss: 0.1979962281663628\n",
            "Epoch 64, Loss: 0.17888909025246377\n",
            "Epoch 65, Loss: 0.1959144194298388\n",
            "Epoch 66, Loss: 0.24613911402709496\n",
            "Epoch 67, Loss: 0.2822797430960266\n",
            "Epoch 68, Loss: 0.23726028742273444\n",
            "Epoch 69, Loss: 0.22374425047193952\n",
            "Epoch 70, Loss: 0.2242654994120607\n",
            "Epoch 71, Loss: 0.24940706973852556\n",
            "Epoch 72, Loss: 0.3442487123677188\n",
            "Epoch 73, Loss: 0.2769880809004197\n",
            "Epoch 74, Loss: 0.16291027645225567\n",
            "Epoch 75, Loss: 0.15686036302658068\n",
            "Epoch 76, Loss: 0.28285078399950436\n",
            "Epoch 77, Loss: 0.3131648725913385\n",
            "Epoch 78, Loss: 0.2668902120169471\n",
            "Epoch 79, Loss: 0.28858222544688705\n",
            "Epoch 80, Loss: 0.3303411785427414\n",
            "Epoch 81, Loss: 0.32418707909677036\n",
            "Epoch 82, Loss: 0.251106150715095\n",
            "Epoch 83, Loss: 0.27004601720653837\n",
            "Epoch 84, Loss: 0.22260219104054485\n",
            "Epoch 85, Loss: 0.24792392610970054\n",
            "Epoch 86, Loss: 0.2379212192166835\n",
            "Epoch 87, Loss: 0.28144392564587883\n",
            "Epoch 88, Loss: 0.3911893705949378\n",
            "Epoch 89, Loss: 0.2909987403075104\n",
            "Epoch 90, Loss: 0.25764377076950523\n",
            "Epoch 91, Loss: 0.22622451133301955\n",
            "Epoch 92, Loss: 0.17788478312418438\n",
            "Epoch 93, Loss: 0.25554185237287713\n",
            "Epoch 94, Loss: 0.4342613878047756\n",
            "Epoch 95, Loss: 0.20410443064959152\n",
            "Epoch 96, Loss: 0.22007420529489932\n",
            "Epoch 97, Loss: 0.2659326985745174\n",
            "Epoch 98, Loss: 0.47828422881224575\n",
            "Epoch 99, Loss: 0.3093938543592268\n",
            "Epoch 100, Loss: 0.3804522647577174\n",
            "Epoch 101, Loss: 0.3559459086002596\n",
            "Epoch 102, Loss: 0.3342545741497327\n",
            "Epoch 103, Loss: 0.3306158599836747\n",
            "Epoch 104, Loss: 0.46997127835364905\n",
            "Epoch 105, Loss: 0.25722972857658666\n",
            "Epoch 106, Loss: 0.2711937636079843\n",
            "Epoch 107, Loss: 0.402832868332734\n",
            "Epoch 108, Loss: 0.31826995444648404\n",
            "Epoch 109, Loss: 0.2375159203396429\n",
            "Epoch 110, Loss: 0.5043597008766192\n",
            "Epoch 111, Loss: 0.2410319593670728\n",
            "Epoch 112, Loss: 0.29012061137220135\n",
            "Epoch 113, Loss: 0.5021956692666502\n",
            "Epoch 114, Loss: 0.3913022408933591\n",
            "Epoch 115, Loss: 0.36594819095547854\n",
            "Epoch 116, Loss: 0.37522280226697396\n",
            "Epoch 117, Loss: 0.3527949754329746\n",
            "Epoch 118, Loss: 0.5104411297794574\n",
            "Epoch 119, Loss: 0.33550174236583435\n",
            "Epoch 120, Loss: 0.4604863569121379\n",
            "Epoch 121, Loss: 0.44437489468994956\n",
            "Epoch 122, Loss: 0.33094717841357224\n",
            "Epoch 123, Loss: 0.24440289488838762\n",
            "Epoch 124, Loss: 0.31046204585248555\n",
            "Epoch 125, Loss: 0.4808195227437922\n",
            "Epoch 126, Loss: 0.4944909801682853\n",
            "Epoch 127, Loss: 0.4007040343111586\n",
            "Epoch 128, Loss: 0.4093450443519046\n",
            "Epoch 129, Loss: 0.43454923449308064\n",
            "Epoch 130, Loss: 0.5159061717445893\n",
            "Epoch 131, Loss: 0.5011699773428385\n",
            "Epoch 132, Loss: 0.47025925531754714\n",
            "Epoch 133, Loss: 0.47788506772969386\n",
            "Epoch 134, Loss: 0.459411725954479\n",
            "Epoch 135, Loss: 0.3293005634680429\n",
            "Epoch 136, Loss: 0.43716177697796044\n",
            "Epoch 137, Loss: 0.38257760896592796\n",
            "Epoch 138, Loss: 0.3401914296905174\n",
            "Epoch 139, Loss: 0.42835267315930725\n",
            "Epoch 140, Loss: 0.5986507038974092\n",
            "Epoch 141, Loss: 0.6117561307861982\n",
            "Epoch 142, Loss: 0.5152045823919499\n",
            "Epoch 143, Loss: 0.485376095127724\n",
            "Epoch 144, Loss: 0.5290640781507315\n",
            "Epoch 145, Loss: 0.47089370677385795\n",
            "Epoch 146, Loss: 0.5756243404901356\n",
            "Epoch 147, Loss: 0.5682059005283944\n",
            "Epoch 148, Loss: 0.5447642514315407\n",
            "Epoch 149, Loss: 0.4378199465096454\n",
            "Epoch 150, Loss: 0.6639127602319583\n",
            "Epoch 151, Loss: 0.5262319264014054\n",
            "Epoch 152, Loss: 0.580172855397472\n",
            "Epoch 153, Loss: 0.4902099937086215\n",
            "Epoch 154, Loss: 0.48479834180849285\n",
            "Epoch 155, Loss: 0.5139845128521285\n",
            "Epoch 156, Loss: 0.5976779259588865\n",
            "Epoch 157, Loss: 0.5316144595556247\n",
            "Epoch 158, Loss: 0.6827944350212126\n",
            "Epoch 159, Loss: 0.40997004038308893\n",
            "Epoch 160, Loss: 0.4113719830541964\n",
            "Epoch 161, Loss: 0.45600060261714526\n",
            "Epoch 162, Loss: 0.5874376625984984\n",
            "Epoch 163, Loss: 0.6197800626954459\n",
            "Epoch 164, Loss: 0.6321072018207492\n",
            "Epoch 165, Loss: 0.7461305775910693\n",
            "Epoch 166, Loss: 0.5184762507219753\n",
            "Epoch 167, Loss: 0.5446838312365515\n",
            "Epoch 168, Loss: 0.7157926525529998\n",
            "Epoch 169, Loss: 0.49348998401323546\n",
            "Epoch 170, Loss: 0.5228298602773406\n",
            "Epoch 171, Loss: 0.6378422655412913\n",
            "Epoch 172, Loss: 0.7592963195975174\n",
            "Epoch 173, Loss: 0.7736393379052277\n",
            "Epoch 174, Loss: 0.4812668099942262\n",
            "Epoch 175, Loss: 0.7210050424765748\n",
            "Epoch 176, Loss: 0.5856072629237419\n",
            "Epoch 177, Loss: 0.6610781820800603\n",
            "Epoch 178, Loss: 0.5835921254266253\n",
            "Epoch 179, Loss: 0.6558424613398054\n",
            "Epoch 180, Loss: 0.6731198305249824\n",
            "Epoch 181, Loss: 0.6714240258650097\n",
            "Epoch 182, Loss: 0.645459971169148\n",
            "Epoch 183, Loss: 0.6401495299566432\n",
            "Epoch 184, Loss: 0.8958717451604736\n",
            "Epoch 185, Loss: 0.6944841879712956\n",
            "Epoch 186, Loss: 0.5622853439425111\n",
            "Epoch 187, Loss: 0.6368662665033584\n",
            "Epoch 188, Loss: 0.6644362863105582\n",
            "Epoch 189, Loss: 0.7238209456815135\n",
            "Epoch 190, Loss: 0.7357832346959492\n",
            "Epoch 191, Loss: 0.9584349200815496\n",
            "Epoch 192, Loss: 0.7399177318605621\n",
            "Epoch 193, Loss: 0.55758173716114\n",
            "Epoch 194, Loss: 0.6101392069855309\n",
            "Epoch 195, Loss: 0.7413567609113195\n",
            "Epoch 196, Loss: 0.7485240931096284\n",
            "Epoch 197, Loss: 0.8957090171630425\n",
            "Epoch 198, Loss: 0.926684622836235\n",
            "Epoch 199, Loss: 0.6747208249271678\n",
            "Accuracy: 28.30%\n",
            "Training with params: {'batch_size': 128, 'lr': 0.1, 'optimizer': 'RMSprop'}\n",
            "Epoch 1, Loss: 1444548.8165984116\n",
            "Epoch 2, Loss: 6.036483731721064\n",
            "Epoch 3, Loss: 4.722558239841705\n",
            "Epoch 4, Loss: 4.629545509357891\n",
            "Epoch 5, Loss: 4.628387521904752\n",
            "Epoch 6, Loss: 4.62784181531433\n",
            "Epoch 7, Loss: 4.628285683634336\n",
            "Epoch 8, Loss: 4.628259852719124\n",
            "Epoch 9, Loss: 4.628113253951987\n",
            "Epoch 10, Loss: 4.629116866899573\n",
            "Epoch 11, Loss: 4.628321674473755\n",
            "Epoch 12, Loss: 4.628372053356122\n",
            "Epoch 13, Loss: 4.628792284699657\n",
            "Epoch 14, Loss: 4.629134396457916\n",
            "Epoch 15, Loss: 4.628028651332611\n",
            "Epoch 16, Loss: 4.628820483946739\n",
            "Epoch 17, Loss: 4.628775135635415\n",
            "Epoch 18, Loss: 4.628556746656023\n",
            "Epoch 19, Loss: 4.628303986376204\n",
            "Epoch 20, Loss: 4.62850204453139\n",
            "Epoch 21, Loss: 4.627575226756923\n",
            "Epoch 22, Loss: 4.628247750079845\n",
            "Epoch 23, Loss: 4.628811904536489\n",
            "Epoch 24, Loss: 4.627847771510444\n",
            "Epoch 25, Loss: 4.628815116784762\n",
            "Epoch 26, Loss: 4.628912026924855\n",
            "Epoch 27, Loss: 4.628295377697176\n",
            "Epoch 28, Loss: 4.628321217149115\n",
            "Epoch 29, Loss: 4.628392571988313\n",
            "Epoch 30, Loss: 4.627925099619209\n",
            "Epoch 31, Loss: 4.62842046513277\n",
            "Epoch 32, Loss: 4.627713044890967\n",
            "Epoch 33, Loss: 4.62916806286863\n",
            "Epoch 34, Loss: 4.628878147095976\n",
            "Epoch 35, Loss: 4.62916074201579\n",
            "Epoch 36, Loss: 4.6286621642539565\n",
            "Epoch 37, Loss: 4.628344932175659\n",
            "Epoch 38, Loss: 4.628452783960211\n",
            "Epoch 39, Loss: 4.629030034975018\n",
            "Epoch 40, Loss: 4.628936115128305\n",
            "Epoch 41, Loss: 4.627718816937693\n",
            "Epoch 42, Loss: 4.629318604383932\n",
            "Epoch 43, Loss: 4.62866779117633\n",
            "Epoch 44, Loss: 4.629520993098579\n",
            "Epoch 45, Loss: 4.6284920809518955\n",
            "Epoch 46, Loss: 4.628398715992413\n",
            "Epoch 47, Loss: 4.628568385873\n",
            "Epoch 48, Loss: 4.628628421012703\n",
            "Epoch 49, Loss: 4.629000294239015\n",
            "Epoch 50, Loss: 4.628492815110385\n",
            "Epoch 51, Loss: 4.628570777375985\n",
            "Epoch 52, Loss: 4.6284693049652805\n",
            "Epoch 53, Loss: 4.6288096776703735\n",
            "Epoch 54, Loss: 4.627877261022777\n",
            "Epoch 55, Loss: 4.627805343979155\n",
            "Epoch 56, Loss: 4.627817024660232\n",
            "Epoch 57, Loss: 4.627926625254209\n",
            "Epoch 58, Loss: 4.628730580020134\n",
            "Epoch 59, Loss: 4.6286743803097465\n",
            "Epoch 60, Loss: 4.628392257348961\n",
            "Epoch 61, Loss: 4.628315693886994\n",
            "Epoch 62, Loss: 4.629383124963707\n",
            "Epoch 63, Loss: 4.628357542140405\n",
            "Epoch 64, Loss: 4.628669776575035\n",
            "Epoch 65, Loss: 4.628788448050809\n",
            "Epoch 66, Loss: 4.629921502164563\n",
            "Epoch 67, Loss: 4.628167053622663\n",
            "Epoch 68, Loss: 4.62857543964825\n",
            "Epoch 69, Loss: 4.628629907622667\n",
            "Epoch 70, Loss: 4.628083971760157\n",
            "Epoch 71, Loss: 4.629564188935262\n",
            "Epoch 72, Loss: 4.628268585790454\n",
            "Epoch 73, Loss: 4.628869994522056\n",
            "Epoch 74, Loss: 4.6293677466604715\n",
            "Epoch 75, Loss: 4.6273691867623485\n",
            "Epoch 76, Loss: 4.628454985216146\n",
            "Epoch 77, Loss: 4.628039469804301\n",
            "Epoch 78, Loss: 4.628647116443995\n",
            "Epoch 79, Loss: 4.628133093304647\n",
            "Epoch 80, Loss: 4.6286558982966195\n",
            "Epoch 81, Loss: 4.628141048314322\n",
            "Epoch 82, Loss: 4.628273422455849\n",
            "Epoch 83, Loss: 4.627444202637734\n",
            "Epoch 84, Loss: 4.628909669568777\n",
            "Epoch 85, Loss: 4.628065211693649\n",
            "Epoch 86, Loss: 4.627657924466731\n",
            "Epoch 87, Loss: 4.628286777554876\n",
            "Epoch 88, Loss: 4.628362177582958\n",
            "Epoch 89, Loss: 4.628616446424323\n",
            "Epoch 90, Loss: 4.628473806259272\n",
            "Epoch 91, Loss: 4.628390832935148\n",
            "Epoch 92, Loss: 4.62886528590756\n",
            "Epoch 93, Loss: 4.629042822991491\n",
            "Epoch 94, Loss: 4.628406125871117\n",
            "Epoch 95, Loss: 4.62789539668871\n",
            "Epoch 96, Loss: 4.6284706074258555\n",
            "Epoch 97, Loss: 4.629505937971422\n",
            "Epoch 98, Loss: 4.628098158580263\n",
            "Epoch 99, Loss: 4.629022645828364\n",
            "Epoch 100, Loss: 4.629332205828498\n",
            "Epoch 101, Loss: 4.628558111312749\n",
            "Epoch 102, Loss: 4.627766938465635\n",
            "Epoch 103, Loss: 4.628261762506821\n",
            "Epoch 104, Loss: 4.62847360991456\n",
            "Epoch 105, Loss: 4.628746225400959\n",
            "Epoch 106, Loss: 4.628936512695859\n",
            "Epoch 107, Loss: 4.628199943191255\n",
            "Epoch 108, Loss: 4.628966076599667\n",
            "Epoch 109, Loss: 4.629048524305339\n",
            "Epoch 110, Loss: 4.628618381822201\n",
            "Epoch 111, Loss: 4.628891274142449\n",
            "Epoch 112, Loss: 4.6289184343479475\n",
            "Epoch 113, Loss: 4.627912903075938\n",
            "Epoch 114, Loss: 4.627891173448099\n",
            "Epoch 115, Loss: 4.628303774177571\n",
            "Epoch 116, Loss: 4.62770213495435\n",
            "Epoch 117, Loss: 4.6280675153903035\n",
            "Epoch 118, Loss: 4.629036689963182\n",
            "Epoch 119, Loss: 4.628157008334499\n",
            "Epoch 120, Loss: 4.6286238063022\n",
            "Epoch 121, Loss: 4.628234901086754\n",
            "Epoch 122, Loss: 4.628165593842411\n",
            "Epoch 123, Loss: 4.628520257027863\n",
            "Epoch 124, Loss: 4.627955006211615\n",
            "Epoch 125, Loss: 4.629360496540508\n",
            "Epoch 126, Loss: 4.62812413096123\n",
            "Epoch 127, Loss: 4.628673296145466\n",
            "Epoch 128, Loss: 4.628694878209887\n",
            "Epoch 129, Loss: 4.628963533874668\n",
            "Epoch 130, Loss: 4.628624053867272\n",
            "Epoch 131, Loss: 4.628064876322246\n",
            "Epoch 132, Loss: 4.629057624455913\n",
            "Epoch 133, Loss: 4.628140726357775\n",
            "Epoch 134, Loss: 4.629377411454535\n",
            "Epoch 135, Loss: 4.628929047938198\n",
            "Epoch 136, Loss: 4.628844441660225\n",
            "Epoch 137, Loss: 4.628828226757781\n",
            "Epoch 138, Loss: 4.628877944653602\n",
            "Epoch 139, Loss: 4.62798110054582\n",
            "Epoch 140, Loss: 4.62874863519693\n",
            "Epoch 141, Loss: 4.627996993491717\n",
            "Epoch 142, Loss: 4.628625529501444\n",
            "Epoch 143, Loss: 4.628565390701489\n",
            "Epoch 144, Loss: 4.628720345704452\n",
            "Epoch 145, Loss: 4.628060770156743\n",
            "Epoch 146, Loss: 4.629033169173218\n",
            "Epoch 147, Loss: 4.6292349359263545\n",
            "Epoch 148, Loss: 4.628744164391247\n",
            "Epoch 149, Loss: 4.6291876119725845\n",
            "Epoch 150, Loss: 4.629472239852866\n",
            "Epoch 151, Loss: 4.630078587690583\n",
            "Epoch 152, Loss: 4.628945772605174\n",
            "Epoch 153, Loss: 4.628245132963371\n",
            "Epoch 154, Loss: 4.627683766357734\n",
            "Epoch 155, Loss: 4.628107903863463\n",
            "Epoch 156, Loss: 4.628067069041455\n",
            "Epoch 157, Loss: 4.6287472741988\n",
            "Epoch 158, Loss: 4.628176287921798\n",
            "Epoch 159, Loss: 4.628387010920688\n",
            "Epoch 160, Loss: 4.627909050573169\n",
            "Epoch 161, Loss: 4.627874047554973\n",
            "Epoch 162, Loss: 4.628130507896014\n",
            "Epoch 163, Loss: 4.628675862041581\n",
            "Epoch 164, Loss: 4.628063379956023\n",
            "Epoch 165, Loss: 4.629734367360849\n",
            "Epoch 166, Loss: 4.628246793966464\n",
            "Epoch 167, Loss: 4.628928200363198\n",
            "Epoch 168, Loss: 4.627839615277927\n",
            "Epoch 169, Loss: 4.628552780736743\n",
            "Epoch 170, Loss: 4.628582278785803\n",
            "Epoch 171, Loss: 4.629099369049072\n",
            "Epoch 172, Loss: 4.627619742127636\n",
            "Epoch 173, Loss: 4.628810132555949\n",
            "Epoch 174, Loss: 4.628479645380279\n",
            "Epoch 175, Loss: 4.629079658966845\n",
            "Epoch 176, Loss: 4.6283435675189315\n",
            "Epoch 177, Loss: 4.629525472440988\n",
            "Epoch 178, Loss: 4.62922883582542\n",
            "Epoch 179, Loss: 4.6284614597135185\n",
            "Epoch 180, Loss: 4.629039690012822\n",
            "Epoch 181, Loss: 4.628746660774016\n",
            "Epoch 182, Loss: 4.628466882973986\n",
            "Epoch 183, Loss: 4.628341944321342\n",
            "Epoch 184, Loss: 4.628459958469167\n",
            "Epoch 185, Loss: 4.628823169357027\n",
            "Epoch 186, Loss: 4.629062747711416\n",
            "Epoch 187, Loss: 4.628971199855171\n",
            "Epoch 188, Loss: 4.628381965715257\n",
            "Epoch 189, Loss: 4.627438519616871\n",
            "Epoch 190, Loss: 4.62984482284702\n",
            "Epoch 191, Loss: 4.628694102587297\n",
            "Epoch 192, Loss: 4.6291315464107585\n",
            "Epoch 193, Loss: 4.628802601943541\n",
            "Epoch 194, Loss: 4.628554031976958\n",
            "Epoch 195, Loss: 4.628701416122944\n",
            "Epoch 196, Loss: 4.628271446813403\n",
            "Epoch 197, Loss: 4.628105902610837\n",
            "Epoch 198, Loss: 4.629014784722682\n",
            "Epoch 199, Loss: 4.628724553091142\n",
            "Accuracy: 1.00%\n",
            "Training with params: {'batch_size': 128, 'lr': 0.01, 'optimizer': 'Adam'}\n",
            "Epoch 1, Loss: 4.609112999323384\n",
            "Epoch 2, Loss: 4.608167678803739\n",
            "Epoch 3, Loss: 4.608206979454021\n",
            "Epoch 4, Loss: 4.607920212513956\n",
            "Epoch 5, Loss: 4.608123217092451\n",
            "Epoch 6, Loss: 4.608022304447106\n",
            "Epoch 7, Loss: 4.60805966969951\n",
            "Epoch 8, Loss: 4.608157212776906\n",
            "Epoch 9, Loss: 4.607983934300025\n",
            "Epoch 10, Loss: 4.608269581709371\n",
            "Epoch 11, Loss: 4.608114125478603\n",
            "Epoch 12, Loss: 4.608157572538956\n",
            "Epoch 13, Loss: 4.607982479397903\n",
            "Epoch 14, Loss: 4.608135364854427\n",
            "Epoch 15, Loss: 4.608271814673148\n",
            "Epoch 16, Loss: 4.608107561955366\n",
            "Epoch 17, Loss: 4.608219871130745\n",
            "Epoch 18, Loss: 4.608123875639933\n",
            "Epoch 19, Loss: 4.608196015857979\n",
            "Epoch 20, Loss: 4.60798028423963\n",
            "Epoch 21, Loss: 4.608043459675196\n",
            "Epoch 22, Loss: 4.608117626756049\n",
            "Epoch 23, Loss: 4.608154996886583\n",
            "Epoch 24, Loss: 4.6082794989466365\n",
            "Epoch 25, Loss: 4.607983539171536\n",
            "Epoch 26, Loss: 4.607920882037229\n",
            "Epoch 27, Loss: 4.608096017861915\n",
            "Epoch 28, Loss: 4.608037762019945\n",
            "Epoch 29, Loss: 4.608075110198897\n",
            "Epoch 30, Loss: 4.608199156153843\n",
            "Epoch 31, Loss: 4.608049129281203\n",
            "Epoch 32, Loss: 4.608061261189258\n",
            "Epoch 33, Loss: 4.608162688477265\n",
            "Epoch 34, Loss: 4.608182347339133\n",
            "Epoch 35, Loss: 4.6079263101758245\n",
            "Epoch 36, Loss: 4.608281234341204\n",
            "Epoch 37, Loss: 4.608127296428242\n",
            "Epoch 38, Loss: 4.608112152275222\n",
            "Epoch 39, Loss: 4.608203637935317\n",
            "Epoch 40, Loss: 4.608229085917363\n",
            "Epoch 41, Loss: 4.608120060942667\n",
            "Epoch 42, Loss: 4.6082133954138405\n",
            "Epoch 43, Loss: 4.608272908593688\n",
            "Epoch 44, Loss: 4.608017843397682\n",
            "Epoch 45, Loss: 4.608040121815089\n",
            "Epoch 46, Loss: 4.608272179313328\n",
            "Epoch 47, Loss: 4.608038227881312\n",
            "Epoch 48, Loss: 4.608192506043808\n",
            "Epoch 49, Loss: 4.608181752207334\n",
            "Epoch 50, Loss: 4.608005051722612\n",
            "Epoch 51, Loss: 4.608165683648775\n",
            "Epoch 52, Loss: 4.607903747607375\n",
            "Epoch 53, Loss: 4.608192904830894\n",
            "Epoch 54, Loss: 4.608076463879832\n",
            "Epoch 55, Loss: 4.607965541312762\n",
            "Epoch 56, Loss: 4.607925374794494\n",
            "Epoch 57, Loss: 4.608175275270896\n",
            "Epoch 58, Loss: 4.608167954418056\n",
            "Epoch 59, Loss: 4.608040398648938\n",
            "Epoch 60, Loss: 4.608040009618111\n",
            "Epoch 61, Loss: 4.6083089177260925\n",
            "Epoch 62, Loss: 4.60813676365806\n",
            "Epoch 63, Loss: 4.6081213377930625\n",
            "Epoch 64, Loss: 4.6081749203869755\n",
            "Epoch 65, Loss: 4.608056202568971\n",
            "Epoch 66, Loss: 4.60815267367741\n",
            "Epoch 67, Loss: 4.608113927914358\n",
            "Epoch 68, Loss: 4.608161266502517\n",
            "Epoch 69, Loss: 4.608145110137627\n",
            "Epoch 70, Loss: 4.608120131675545\n",
            "Epoch 71, Loss: 4.6080144872445885\n",
            "Epoch 72, Loss: 4.60814324669216\n",
            "Epoch 73, Loss: 4.608330825405657\n",
            "Epoch 74, Loss: 4.608198813465245\n",
            "Epoch 75, Loss: 4.608175675277515\n",
            "Epoch 76, Loss: 4.60797517805758\n",
            "Epoch 77, Loss: 4.6080096054564965\n",
            "Epoch 78, Loss: 4.607962322966827\n",
            "Epoch 79, Loss: 4.608339858481951\n",
            "Epoch 80, Loss: 4.607963722989992\n",
            "Epoch 81, Loss: 4.608111762024862\n",
            "Epoch 82, Loss: 4.608270910999659\n",
            "Epoch 83, Loss: 4.608051582980339\n",
            "Epoch 84, Loss: 4.607990550263153\n",
            "Epoch 85, Loss: 4.608118276766804\n",
            "Epoch 86, Loss: 4.608074371162278\n",
            "Epoch 87, Loss: 4.607913509964028\n",
            "Epoch 88, Loss: 4.608220079670781\n",
            "Epoch 89, Loss: 4.607941673844672\n",
            "Epoch 90, Loss: 4.608193990214707\n",
            "Epoch 91, Loss: 4.60808788845911\n",
            "Epoch 92, Loss: 4.607934479823198\n",
            "Epoch 93, Loss: 4.6081813851280895\n",
            "Epoch 94, Loss: 4.608070311339005\n",
            "Epoch 95, Loss: 4.608268148758832\n",
            "Epoch 96, Loss: 4.608235192115959\n",
            "Epoch 97, Loss: 4.607933309072119\n",
            "Epoch 98, Loss: 4.607978672017832\n",
            "Epoch 99, Loss: 4.608191826764275\n",
            "Epoch 100, Loss: 4.60796645474251\n",
            "Epoch 101, Loss: 4.608246582548332\n",
            "Epoch 102, Loss: 4.607952932567548\n",
            "Epoch 103, Loss: 4.608345484184792\n",
            "Epoch 104, Loss: 4.6082753257068525\n",
            "Epoch 105, Loss: 4.608203561104777\n",
            "Epoch 106, Loss: 4.6080848664578875\n",
            "Epoch 107, Loss: 4.60831815324476\n",
            "Epoch 108, Loss: 4.607996025963512\n",
            "Epoch 109, Loss: 4.607871492195617\n",
            "Epoch 110, Loss: 4.6083169763960194\n",
            "Epoch 111, Loss: 4.60817079226989\n",
            "Epoch 112, Loss: 4.608088744570837\n",
            "Epoch 113, Loss: 4.607962351016072\n",
            "Epoch 114, Loss: 4.608100749647526\n",
            "Epoch 115, Loss: 4.60807024182566\n",
            "Epoch 116, Loss: 4.608047765844009\n",
            "Epoch 117, Loss: 4.608246626451497\n",
            "Epoch 118, Loss: 4.608092873907455\n",
            "Epoch 119, Loss: 4.607814318078863\n",
            "Epoch 120, Loss: 4.608318260563609\n",
            "Epoch 121, Loss: 4.60812665373468\n",
            "Epoch 122, Loss: 4.608024797171278\n",
            "Epoch 123, Loss: 4.608052508605411\n",
            "Epoch 124, Loss: 4.608050550036418\n",
            "Epoch 125, Loss: 4.607930755371328\n",
            "Epoch 126, Loss: 4.6080362558974635\n",
            "Epoch 127, Loss: 4.608130718436082\n",
            "Epoch 128, Loss: 4.608374370028601\n",
            "Epoch 129, Loss: 4.608265035292682\n",
            "Epoch 130, Loss: 4.608135466075614\n",
            "Epoch 131, Loss: 4.608151839517267\n",
            "Epoch 132, Loss: 4.607945075120463\n",
            "Epoch 133, Loss: 4.6082746195976085\n",
            "Epoch 134, Loss: 4.608073916276703\n",
            "Epoch 135, Loss: 4.608116530396444\n",
            "Epoch 136, Loss: 4.607975580503264\n",
            "Epoch 137, Loss: 4.6082546448768555\n",
            "Epoch 138, Loss: 4.608250544809015\n",
            "Epoch 139, Loss: 4.608046603629656\n",
            "Epoch 140, Loss: 4.608109953458352\n",
            "Epoch 141, Loss: 4.6081021582074175\n",
            "Epoch 142, Loss: 4.608071116230372\n",
            "Epoch 143, Loss: 4.608009127399805\n",
            "Epoch 144, Loss: 4.608142991809894\n",
            "Epoch 145, Loss: 4.608064352703826\n",
            "Epoch 146, Loss: 4.608038665693434\n",
            "Epoch 147, Loss: 4.60802296421412\n",
            "Epoch 148, Loss: 4.60812520980835\n",
            "Epoch 149, Loss: 4.607958553392259\n",
            "Epoch 150, Loss: 4.608233853069413\n",
            "Epoch 151, Loss: 4.607927267508739\n",
            "Epoch 152, Loss: 4.608225517565637\n",
            "Epoch 153, Loss: 4.608121717067631\n",
            "Epoch 154, Loss: 4.608177683847335\n",
            "Epoch 155, Loss: 4.608016978749228\n",
            "Epoch 156, Loss: 4.608230921313586\n",
            "Epoch 157, Loss: 4.607933021262479\n",
            "Epoch 158, Loss: 4.608290367419152\n",
            "Epoch 159, Loss: 4.608084209129938\n",
            "Epoch 160, Loss: 4.607947236131829\n",
            "Epoch 161, Loss: 4.608255171714841\n",
            "Epoch 162, Loss: 4.608098397169576\n",
            "Epoch 163, Loss: 4.608138696616873\n",
            "Epoch 164, Loss: 4.608226956613838\n",
            "Epoch 165, Loss: 4.6081209438841055\n",
            "Epoch 166, Loss: 4.608246332544195\n",
            "Epoch 167, Loss: 4.608124945169824\n",
            "Epoch 168, Loss: 4.6079066318014394\n",
            "Epoch 169, Loss: 4.608072284542386\n",
            "Epoch 170, Loss: 4.608088301880585\n",
            "Epoch 171, Loss: 4.607955149677403\n",
            "Epoch 172, Loss: 4.608067496658286\n",
            "Epoch 173, Loss: 4.608340385319937\n",
            "Epoch 174, Loss: 4.608000701650634\n",
            "Epoch 175, Loss: 4.607962083938482\n",
            "Epoch 176, Loss: 4.60803711688732\n",
            "Epoch 177, Loss: 4.608024581314048\n",
            "Epoch 178, Loss: 4.608005767588115\n",
            "Epoch 179, Loss: 4.6080638917205885\n",
            "Epoch 180, Loss: 4.608152300500504\n",
            "Epoch 181, Loss: 4.608041958430844\n",
            "Epoch 182, Loss: 4.608074295551271\n",
            "Epoch 183, Loss: 4.607802209341922\n",
            "Epoch 184, Loss: 4.608101692346051\n",
            "Epoch 185, Loss: 4.608202502550676\n",
            "Epoch 186, Loss: 4.6079171624634885\n",
            "Epoch 187, Loss: 4.608186082766794\n",
            "Epoch 188, Loss: 4.608155690800503\n",
            "Epoch 189, Loss: 4.608049429286167\n",
            "Epoch 190, Loss: 4.608184586400571\n",
            "Epoch 191, Loss: 4.608026598420595\n",
            "Epoch 192, Loss: 4.608186132767621\n",
            "Epoch 193, Loss: 4.607868082383099\n",
            "Epoch 194, Loss: 4.608433056365499\n",
            "Epoch 195, Loss: 4.608071986976487\n",
            "Epoch 196, Loss: 4.60779485678124\n",
            "Epoch 197, Loss: 4.607964647395532\n",
            "Epoch 198, Loss: 4.6080655844315235\n",
            "Epoch 199, Loss: 4.608203664765028\n",
            "Accuracy: 1.00%\n",
            "Training with params: {'batch_size': 128, 'lr': 0.01, 'optimizer': 'SGD'}\n",
            "Epoch 1, Loss: 4.604757927262875\n",
            "Epoch 2, Loss: 4.601101678960464\n",
            "Epoch 3, Loss: 4.5933785755616015\n",
            "Epoch 4, Loss: 4.566278145441314\n",
            "Epoch 5, Loss: 4.455100919279601\n",
            "Epoch 6, Loss: 4.274903718162985\n",
            "Epoch 7, Loss: 4.141118352065611\n",
            "Epoch 8, Loss: 3.9517481070955087\n",
            "Epoch 9, Loss: 3.804422599885165\n",
            "Epoch 10, Loss: 3.7242100391241597\n",
            "Epoch 11, Loss: 3.6498061256945284\n",
            "Epoch 12, Loss: 3.5749676343425154\n",
            "Epoch 13, Loss: 3.507928620214048\n",
            "Epoch 14, Loss: 3.450585553713162\n",
            "Epoch 15, Loss: 3.395592176091031\n",
            "Epoch 16, Loss: 3.341924776506546\n",
            "Epoch 17, Loss: 3.290156899205864\n",
            "Epoch 18, Loss: 3.2411031125451597\n",
            "Epoch 19, Loss: 3.190532316027395\n",
            "Epoch 20, Loss: 3.1455765959551876\n",
            "Epoch 21, Loss: 3.096622398747203\n",
            "Epoch 22, Loss: 3.052639342939762\n",
            "Epoch 23, Loss: 3.006129172756849\n",
            "Epoch 24, Loss: 2.9618652440093056\n",
            "Epoch 25, Loss: 2.9166609381165958\n",
            "Epoch 26, Loss: 2.87347387962634\n",
            "Epoch 27, Loss: 2.8268898667581857\n",
            "Epoch 28, Loss: 2.7832935319837095\n",
            "Epoch 29, Loss: 2.7398067557293437\n",
            "Epoch 30, Loss: 2.696621634466264\n",
            "Epoch 31, Loss: 2.6536584595585113\n",
            "Epoch 32, Loss: 2.608041505984333\n",
            "Epoch 33, Loss: 2.567175823099473\n",
            "Epoch 34, Loss: 2.5269371761995205\n",
            "Epoch 35, Loss: 2.4858953501562326\n",
            "Epoch 36, Loss: 2.449568108219625\n",
            "Epoch 37, Loss: 2.4077148379572213\n",
            "Epoch 38, Loss: 2.3667140473490176\n",
            "Epoch 39, Loss: 2.329272728137043\n",
            "Epoch 40, Loss: 2.2922521344840985\n",
            "Epoch 41, Loss: 2.2550219291311397\n",
            "Epoch 42, Loss: 2.2151783784027295\n",
            "Epoch 43, Loss: 2.179160827870869\n",
            "Epoch 44, Loss: 2.146169535339336\n",
            "Epoch 45, Loss: 2.1077462267082976\n",
            "Epoch 46, Loss: 2.0689500655664506\n",
            "Epoch 47, Loss: 2.033930589171017\n",
            "Epoch 48, Loss: 1.9991302173155958\n",
            "Epoch 49, Loss: 1.964297449802194\n",
            "Epoch 50, Loss: 1.9322180723595193\n",
            "Epoch 51, Loss: 1.8949080376368959\n",
            "Epoch 52, Loss: 1.8647240915566758\n",
            "Epoch 53, Loss: 1.8283809057586944\n",
            "Epoch 54, Loss: 1.793198308371522\n",
            "Epoch 55, Loss: 1.7624594906102056\n",
            "Epoch 56, Loss: 1.7233359027091804\n",
            "Epoch 57, Loss: 1.687318482667284\n",
            "Epoch 58, Loss: 1.6574661082318982\n",
            "Epoch 59, Loss: 1.6213671450724687\n",
            "Epoch 60, Loss: 1.587677504400463\n",
            "Epoch 61, Loss: 1.5476992328453552\n",
            "Epoch 62, Loss: 1.5158131552474272\n",
            "Epoch 63, Loss: 1.4844688378331605\n",
            "Epoch 64, Loss: 1.4481777172259358\n",
            "Epoch 65, Loss: 1.4153938709622453\n",
            "Epoch 66, Loss: 1.3774842401904523\n",
            "Epoch 67, Loss: 1.3481158336715016\n",
            "Epoch 68, Loss: 1.31106920559388\n",
            "Epoch 69, Loss: 1.279917177032022\n",
            "Epoch 70, Loss: 1.2472562396617801\n",
            "Epoch 71, Loss: 1.209843382658556\n",
            "Epoch 72, Loss: 1.1769523166329658\n",
            "Epoch 73, Loss: 1.1503399043436855\n",
            "Epoch 74, Loss: 1.1089756747950679\n",
            "Epoch 75, Loss: 1.0805606700270378\n",
            "Epoch 76, Loss: 1.048591598372935\n",
            "Epoch 77, Loss: 1.0203292266182278\n",
            "Epoch 78, Loss: 0.9887538355634645\n",
            "Epoch 79, Loss: 0.9554873197279927\n",
            "Epoch 80, Loss: 0.9200863952526961\n",
            "Epoch 81, Loss: 0.9012653877972947\n",
            "Epoch 82, Loss: 0.8619511238754253\n",
            "Epoch 83, Loss: 0.8327725842175886\n",
            "Epoch 84, Loss: 0.8000531280437089\n",
            "Epoch 85, Loss: 0.7725159769777752\n",
            "Epoch 86, Loss: 0.7490260025576863\n",
            "Epoch 87, Loss: 0.7170699289845078\n",
            "Epoch 88, Loss: 0.6921978893182467\n",
            "Epoch 89, Loss: 0.6550983147852866\n",
            "Epoch 90, Loss: 0.6335765993808542\n",
            "Epoch 91, Loss: 0.6056170343133189\n",
            "Epoch 92, Loss: 0.5884867148935947\n",
            "Epoch 93, Loss: 0.5652683198909321\n",
            "Epoch 94, Loss: 0.5326946375467588\n",
            "Epoch 95, Loss: 0.5187271449267102\n",
            "Epoch 96, Loss: 0.4862662389150361\n",
            "Epoch 97, Loss: 0.46372757985463836\n",
            "Epoch 98, Loss: 0.4289165371671662\n",
            "Epoch 99, Loss: 0.418476777765757\n",
            "Epoch 100, Loss: 0.4005349657267256\n",
            "Epoch 101, Loss: 0.3874549612288585\n",
            "Epoch 102, Loss: 0.37367598228442395\n",
            "Epoch 103, Loss: 0.35681109339989664\n",
            "Epoch 104, Loss: 0.3184106963903398\n",
            "Epoch 105, Loss: 0.2982808083981809\n",
            "Epoch 106, Loss: 0.28454065414340907\n",
            "Epoch 107, Loss: 0.29724011314875637\n",
            "Epoch 108, Loss: 0.24810152094992224\n",
            "Epoch 109, Loss: 0.24290365662873553\n",
            "Epoch 110, Loss: 0.2217496650679337\n",
            "Epoch 111, Loss: 0.18653788751043626\n",
            "Epoch 112, Loss: 0.17317960266490726\n",
            "Epoch 113, Loss: 0.17282372106181082\n",
            "Epoch 114, Loss: 0.24416393237879208\n",
            "Epoch 115, Loss: 0.24286051372737835\n",
            "Epoch 116, Loss: 0.19092724201700573\n",
            "Epoch 117, Loss: 0.12755959382866655\n",
            "Epoch 118, Loss: 0.08886784014989958\n",
            "Epoch 119, Loss: 0.06487767864256869\n",
            "Epoch 120, Loss: 0.04108501753062391\n",
            "Epoch 121, Loss: 0.029330562592943765\n",
            "Epoch 122, Loss: 0.024268361215796466\n",
            "Epoch 123, Loss: 0.0204525836445677\n",
            "Epoch 124, Loss: 0.021094053100003764\n",
            "Epoch 125, Loss: 0.01790976055952556\n",
            "Epoch 126, Loss: 0.016859915931625745\n",
            "Epoch 127, Loss: 0.014825960125802728\n",
            "Epoch 128, Loss: 0.012118846799730493\n",
            "Epoch 129, Loss: 0.012499131731536535\n",
            "Epoch 130, Loss: 0.014235283416050399\n",
            "Epoch 131, Loss: 0.012603154359504466\n",
            "Epoch 132, Loss: 0.011071054813335353\n",
            "Epoch 133, Loss: 0.011456262653150483\n",
            "Epoch 134, Loss: 0.01014844901607279\n",
            "Epoch 135, Loss: 0.0105732498854837\n",
            "Epoch 136, Loss: 0.009692762862852849\n",
            "Epoch 137, Loss: 0.009558587522029191\n",
            "Epoch 138, Loss: 0.008988345944015382\n",
            "Epoch 139, Loss: 0.008764112438492077\n",
            "Epoch 140, Loss: 0.009055388009990268\n",
            "Epoch 141, Loss: 0.009054183736062416\n",
            "Epoch 142, Loss: 0.00873547363931032\n",
            "Epoch 143, Loss: 0.008355938176245754\n",
            "Epoch 144, Loss: 0.007765936689055942\n",
            "Epoch 145, Loss: 0.007553647686739254\n",
            "Epoch 146, Loss: 0.007565114401218951\n",
            "Epoch 147, Loss: 0.007122719351945402\n",
            "Epoch 148, Loss: 0.008369136779137965\n",
            "Epoch 149, Loss: 0.008251115032459807\n",
            "Epoch 150, Loss: 0.00813962132229334\n",
            "Epoch 151, Loss: 0.007582300198514519\n",
            "Epoch 152, Loss: 0.007029361233277165\n",
            "Epoch 153, Loss: 0.006806616309632444\n",
            "Epoch 154, Loss: 0.008323953597737318\n",
            "Epoch 155, Loss: 0.007208392536744018\n",
            "Epoch 156, Loss: 0.0067189917980057315\n",
            "Epoch 157, Loss: 0.00848352378311679\n",
            "Epoch 158, Loss: 0.007063925199572692\n",
            "Epoch 159, Loss: 0.006807245979564326\n",
            "Epoch 160, Loss: 0.007448805957887312\n",
            "Epoch 161, Loss: 0.007781717797641254\n",
            "Epoch 162, Loss: 0.006683346949389104\n",
            "Epoch 163, Loss: 0.006496456722059595\n",
            "Epoch 164, Loss: 0.006205381807702048\n",
            "Epoch 165, Loss: 0.0075158402061713935\n",
            "Epoch 166, Loss: 0.007457843952504989\n",
            "Epoch 167, Loss: 0.006886001043033588\n",
            "Epoch 168, Loss: 0.006405162047404234\n",
            "Epoch 169, Loss: 0.00705100630518392\n",
            "Epoch 170, Loss: 0.008421849053593644\n",
            "Epoch 171, Loss: 0.00908640254993954\n",
            "Epoch 172, Loss: 0.006990710013460301\n",
            "Epoch 173, Loss: 0.005876876628788574\n",
            "Epoch 174, Loss: 0.006021266311878705\n",
            "Epoch 175, Loss: 0.005338156458271472\n",
            "Epoch 176, Loss: 0.005826454182081591\n",
            "Epoch 177, Loss: 0.005834915819327774\n",
            "Epoch 178, Loss: 0.006426301019926034\n",
            "Epoch 179, Loss: 0.006631045140471319\n",
            "Epoch 180, Loss: 0.005633830768711236\n",
            "Epoch 181, Loss: 0.005889431914657621\n",
            "Epoch 182, Loss: 0.00643627981648154\n",
            "Epoch 183, Loss: 0.007000761624077892\n",
            "Epoch 184, Loss: 0.005643448063536831\n",
            "Epoch 185, Loss: 0.007499233393779839\n",
            "Epoch 186, Loss: 0.006101045495523211\n",
            "Epoch 187, Loss: 0.005820064463347787\n",
            "Epoch 188, Loss: 0.006230416203327382\n",
            "Epoch 189, Loss: 0.0063245548236081405\n",
            "Epoch 190, Loss: 0.005544154274026099\n",
            "Epoch 191, Loss: 0.006182244261000258\n",
            "Epoch 192, Loss: 0.005495290082279126\n",
            "Epoch 193, Loss: 0.005508637874830238\n",
            "Epoch 194, Loss: 0.006013913865527495\n",
            "Epoch 195, Loss: 0.005814194087656048\n",
            "Epoch 196, Loss: 0.0056638238466847355\n",
            "Epoch 197, Loss: 0.005437600136081905\n",
            "Epoch 198, Loss: 0.0058145162008126335\n",
            "Epoch 199, Loss: 0.005968361929216234\n",
            "Accuracy: 33.77%\n",
            "Training with params: {'batch_size': 128, 'lr': 0.01, 'optimizer': 'RMSprop'}\n",
            "Epoch 1, Loss: 26.956917569460465\n",
            "Epoch 2, Loss: 3.7008250747495297\n",
            "Epoch 3, Loss: 3.576821583311271\n",
            "Epoch 4, Loss: 3.487759350510814\n",
            "Epoch 5, Loss: 3.3791603718877146\n",
            "Epoch 6, Loss: 3.2898871288884934\n",
            "Epoch 7, Loss: 3.23169878742579\n",
            "Epoch 8, Loss: 3.183783264111375\n",
            "Epoch 9, Loss: 3.154033856013852\n",
            "Epoch 10, Loss: 3.1141952462208544\n",
            "Epoch 11, Loss: 3.089567371646462\n",
            "Epoch 12, Loss: 3.0867709644005425\n",
            "Epoch 13, Loss: 3.0581472243189505\n",
            "Epoch 14, Loss: 3.04426643671587\n",
            "Epoch 15, Loss: 3.0267522292368856\n",
            "Epoch 16, Loss: 3.058456107478617\n",
            "Epoch 17, Loss: 3.0184670793430883\n",
            "Epoch 18, Loss: 2.996036984426591\n",
            "Epoch 19, Loss: 3.000207908927937\n",
            "Epoch 20, Loss: 2.992449793974152\n",
            "Epoch 21, Loss: 2.989665630223501\n",
            "Epoch 22, Loss: 2.988332820365496\n",
            "Epoch 23, Loss: 3.0148474255486217\n",
            "Epoch 24, Loss: 2.9677353319914443\n",
            "Epoch 25, Loss: 2.981119458937584\n",
            "Epoch 26, Loss: 2.9687673216280728\n",
            "Epoch 27, Loss: 2.9639821272067097\n",
            "Epoch 28, Loss: 3.1933320843045365\n",
            "Epoch 29, Loss: 2.934594962298108\n",
            "Epoch 30, Loss: 2.9500049245936792\n",
            "Epoch 31, Loss: 2.9651767341682063\n",
            "Epoch 32, Loss: 2.964001230571581\n",
            "Epoch 33, Loss: 2.9605396440266953\n",
            "Epoch 34, Loss: 2.945980729349434\n",
            "Epoch 35, Loss: 2.9517308068092523\n",
            "Epoch 36, Loss: 2.950536166310615\n",
            "Epoch 37, Loss: 2.9528087959874925\n",
            "Epoch 38, Loss: 2.9457204567501916\n",
            "Epoch 39, Loss: 2.9629508985582826\n",
            "Epoch 40, Loss: 2.950025902379809\n",
            "Epoch 41, Loss: 2.969479581584101\n",
            "Epoch 42, Loss: 2.9597721234002075\n",
            "Epoch 43, Loss: 2.9559503034557526\n",
            "Epoch 44, Loss: 3.045275770489822\n",
            "Epoch 45, Loss: 2.9550691315585085\n",
            "Epoch 46, Loss: 2.941954418826286\n",
            "Epoch 47, Loss: 2.9635347466334663\n",
            "Epoch 48, Loss: 2.9673906817765494\n",
            "Epoch 49, Loss: 2.9676955977974036\n",
            "Epoch 50, Loss: 2.959350198126205\n",
            "Epoch 51, Loss: 2.97650742652776\n",
            "Epoch 52, Loss: 3.0038227181300483\n",
            "Epoch 53, Loss: 2.9641289839049434\n",
            "Epoch 54, Loss: 2.965385013833985\n",
            "Epoch 55, Loss: 2.9616129392248287\n",
            "Epoch 56, Loss: 2.9634840409164234\n",
            "Epoch 57, Loss: 3.0152623909513663\n",
            "Epoch 58, Loss: 2.953598313929175\n",
            "Epoch 59, Loss: 2.967468877582599\n",
            "Epoch 60, Loss: 2.9677201811309972\n",
            "Epoch 61, Loss: 2.974325075173927\n",
            "Epoch 62, Loss: 2.961309572619855\n",
            "Epoch 63, Loss: 2.976305727458671\n",
            "Epoch 64, Loss: 2.990023696514042\n",
            "Epoch 65, Loss: 2.9662880806057044\n",
            "Epoch 66, Loss: 2.988645229803022\n",
            "Epoch 67, Loss: 2.970958315198074\n",
            "Epoch 68, Loss: 2.97737739702015\n",
            "Epoch 69, Loss: 2.9938501848284242\n",
            "Epoch 70, Loss: 2.9868454341693305\n",
            "Epoch 71, Loss: 2.986929211775055\n",
            "Epoch 72, Loss: 2.986181459158583\n",
            "Epoch 73, Loss: 2.9802908732763034\n",
            "Epoch 74, Loss: 2.963307760560604\n",
            "Epoch 75, Loss: 2.9729093285777686\n",
            "Epoch 76, Loss: 2.9880203755615313\n",
            "Epoch 77, Loss: 2.9768185847250703\n",
            "Epoch 78, Loss: 2.987482832520819\n",
            "Epoch 79, Loss: 2.998736291895132\n",
            "Epoch 80, Loss: 2.9972697170189275\n",
            "Epoch 81, Loss: 3.0233874442937125\n",
            "Epoch 82, Loss: 2.989187047304705\n",
            "Epoch 83, Loss: 3.004089434128588\n",
            "Epoch 84, Loss: 3.0354126981457177\n",
            "Epoch 85, Loss: 2.9982026909928186\n",
            "Epoch 86, Loss: 3.0352808801109528\n",
            "Epoch 87, Loss: 2.986639568567886\n",
            "Epoch 88, Loss: 2.99548639848714\n",
            "Epoch 89, Loss: 2.9918367100493684\n",
            "Epoch 90, Loss: 3.0412717036274084\n",
            "Epoch 91, Loss: 3.036154221390824\n",
            "Epoch 92, Loss: 3.021902243499561\n",
            "Epoch 93, Loss: 3.025063568368897\n",
            "Epoch 94, Loss: 3.003351396916772\n",
            "Epoch 95, Loss: 2.9934626442697043\n",
            "Epoch 96, Loss: 3.0304267766225674\n",
            "Epoch 97, Loss: 3.0279863807551393\n",
            "Epoch 98, Loss: 2.9925036594995755\n",
            "Epoch 99, Loss: 2.9967684422612497\n",
            "Epoch 100, Loss: 2.986293361010149\n",
            "Epoch 101, Loss: 2.994718435780167\n",
            "Epoch 102, Loss: 2.983695215581323\n",
            "Epoch 103, Loss: 2.990414882255027\n",
            "Epoch 104, Loss: 3.0245787060779072\n",
            "Epoch 105, Loss: 2.9929674665641297\n",
            "Epoch 106, Loss: 2.9828410307159814\n",
            "Epoch 107, Loss: 3.01895304957924\n",
            "Epoch 108, Loss: 2.9746967112011924\n",
            "Epoch 109, Loss: 2.9835001769882945\n",
            "Epoch 110, Loss: 3.002103192117208\n",
            "Epoch 111, Loss: 3.023470386519761\n",
            "Epoch 112, Loss: 2.981164652368297\n",
            "Epoch 113, Loss: 3.007855343391828\n",
            "Epoch 114, Loss: 3.0498439905893466\n",
            "Epoch 115, Loss: 3.0125436673079\n",
            "Epoch 116, Loss: 2.998329783949401\n",
            "Epoch 117, Loss: 3.0216476618481414\n",
            "Epoch 118, Loss: 3.009331905018643\n",
            "Epoch 119, Loss: 2.9699050813074916\n",
            "Epoch 120, Loss: 3.0320374142483373\n",
            "Epoch 121, Loss: 2.998446506002675\n",
            "Epoch 122, Loss: 3.004825312158336\n",
            "Epoch 123, Loss: 2.970192259229967\n",
            "Epoch 124, Loss: 3.011480653987211\n",
            "Epoch 125, Loss: 2.99521877576628\n",
            "Epoch 126, Loss: 2.9727228450043426\n",
            "Epoch 127, Loss: 2.9693014877836417\n",
            "Epoch 128, Loss: 3.0023601719790407\n",
            "Epoch 129, Loss: 3.0524916972040823\n",
            "Epoch 130, Loss: 3.0057345265927524\n",
            "Epoch 131, Loss: 3.0190681234345105\n",
            "Epoch 132, Loss: 3.000217531038367\n",
            "Epoch 133, Loss: 3.0065031490667398\n",
            "Epoch 134, Loss: 2.98759026234717\n",
            "Epoch 135, Loss: 3.010329695918676\n",
            "Epoch 136, Loss: 3.010286826916668\n",
            "Epoch 137, Loss: 3.050278526437862\n",
            "Epoch 138, Loss: 3.0177842424348795\n",
            "Epoch 139, Loss: 3.014930480581415\n",
            "Epoch 140, Loss: 3.0019106230772366\n",
            "Epoch 141, Loss: 3.071995135768295\n",
            "Epoch 142, Loss: 3.0900591553934396\n",
            "Epoch 143, Loss: 3.0233557126711093\n",
            "Epoch 144, Loss: 3.0266043603267816\n",
            "Epoch 145, Loss: 2.984555356642779\n",
            "Epoch 146, Loss: 3.0127610818809254\n",
            "Epoch 147, Loss: 3.0258193857529583\n",
            "Epoch 148, Loss: 2.989311590219093\n",
            "Epoch 149, Loss: 3.034654247791261\n",
            "Epoch 150, Loss: 3.016867090674008\n",
            "Epoch 151, Loss: 3.0603798405288734\n",
            "Epoch 152, Loss: 3.017466723766473\n",
            "Epoch 153, Loss: 3.0617610542365656\n",
            "Epoch 154, Loss: 2.9747673635897427\n",
            "Epoch 155, Loss: 3.0689079359059446\n",
            "Epoch 156, Loss: 2.9939041991367974\n",
            "Epoch 157, Loss: 3.092548085600519\n",
            "Epoch 158, Loss: 3.062914058070658\n",
            "Epoch 159, Loss: 3.0346369731151843\n",
            "Epoch 160, Loss: 2.9794320877250806\n",
            "Epoch 161, Loss: 3.0832364242095167\n",
            "Epoch 162, Loss: 3.0418894394584326\n",
            "Epoch 163, Loss: 3.02678206570618\n",
            "Epoch 164, Loss: 3.0733736158941714\n",
            "Epoch 165, Loss: 3.077783181539277\n",
            "Epoch 166, Loss: 2.9805483500975782\n",
            "Epoch 167, Loss: 3.0735218829815953\n",
            "Epoch 168, Loss: 3.0056875105709064\n",
            "Epoch 169, Loss: 3.089261978178683\n",
            "Epoch 170, Loss: 3.0312726948877127\n",
            "Epoch 171, Loss: 2.9928365668372425\n",
            "Epoch 172, Loss: 3.1058251772390304\n",
            "Epoch 173, Loss: 2.9994704259935854\n",
            "Epoch 174, Loss: 3.0683644976457365\n",
            "Epoch 175, Loss: 3.011460278650074\n",
            "Epoch 176, Loss: 3.0950974290023376\n",
            "Epoch 177, Loss: 3.048941304311728\n",
            "Epoch 178, Loss: 3.0110939421007394\n",
            "Epoch 179, Loss: 3.0315162037949426\n",
            "Epoch 180, Loss: 3.065897479996352\n",
            "Epoch 181, Loss: 3.0193088072949967\n",
            "Epoch 182, Loss: 2.99217645774412\n",
            "Epoch 183, Loss: 3.190808514499908\n",
            "Epoch 184, Loss: 3.0303418160704396\n",
            "Epoch 185, Loss: 3.114149426560268\n",
            "Epoch 186, Loss: 3.0459503877498304\n",
            "Epoch 187, Loss: 3.163974596716254\n",
            "Epoch 188, Loss: 3.0065730794921253\n",
            "Epoch 189, Loss: 3.0241665742586337\n",
            "Epoch 190, Loss: 3.0128629414931587\n",
            "Epoch 191, Loss: 3.047610160944712\n",
            "Epoch 192, Loss: 3.0891677619856033\n",
            "Epoch 193, Loss: 3.0695166026844696\n",
            "Epoch 194, Loss: 3.1069041511896627\n",
            "Epoch 195, Loss: 3.0129928808383015\n",
            "Epoch 196, Loss: 3.020506485039011\n",
            "Epoch 197, Loss: 3.026640509095643\n",
            "Epoch 198, Loss: 3.027708936530306\n",
            "Epoch 199, Loss: 3.0028200942232175\n",
            "Accuracy: 21.70%\n",
            "Training with params: {'batch_size': 128, 'lr': 0.001, 'optimizer': 'Adam'}\n",
            "Epoch 1, Loss: 3.8267984835388105\n",
            "Epoch 2, Loss: 3.122946614194709\n",
            "Epoch 3, Loss: 2.792857556696743\n",
            "Epoch 4, Loss: 2.574333997028868\n",
            "Epoch 5, Loss: 2.4064986586875623\n",
            "Epoch 6, Loss: 2.2636605080436256\n",
            "Epoch 7, Loss: 2.146214730294464\n",
            "Epoch 8, Loss: 2.0262585430194044\n",
            "Epoch 9, Loss: 1.9202644352412894\n",
            "Epoch 10, Loss: 1.8346710141052676\n",
            "Epoch 11, Loss: 1.74023739181821\n",
            "Epoch 12, Loss: 1.6528052234893564\n",
            "Epoch 13, Loss: 1.571893578904974\n",
            "Epoch 14, Loss: 1.5044802710833147\n",
            "Epoch 15, Loss: 1.423985651386973\n",
            "Epoch 16, Loss: 1.3599128096609774\n",
            "Epoch 17, Loss: 1.2998882910174787\n",
            "Epoch 18, Loss: 1.2343066366737152\n",
            "Epoch 19, Loss: 1.17664748719891\n",
            "Epoch 20, Loss: 1.1159474279569543\n",
            "Epoch 21, Loss: 1.068310953619535\n",
            "Epoch 22, Loss: 1.0145387951370395\n",
            "Epoch 23, Loss: 0.9632612628400173\n",
            "Epoch 24, Loss: 0.9259751591536091\n",
            "Epoch 25, Loss: 0.8785016608360173\n",
            "Epoch 26, Loss: 0.8488285392903916\n",
            "Epoch 27, Loss: 0.7909950653610327\n",
            "Epoch 28, Loss: 0.7566823944868639\n",
            "Epoch 29, Loss: 0.7227309621356027\n",
            "Epoch 30, Loss: 0.6900894782884651\n",
            "Epoch 31, Loss: 0.6563358341946322\n",
            "Epoch 32, Loss: 0.6284200130673625\n",
            "Epoch 33, Loss: 0.6038800902531275\n",
            "Epoch 34, Loss: 0.5639218748987788\n",
            "Epoch 35, Loss: 0.5353535964056049\n",
            "Epoch 36, Loss: 0.513940826172719\n",
            "Epoch 37, Loss: 0.4930602474819364\n",
            "Epoch 38, Loss: 0.4724753581730606\n",
            "Epoch 39, Loss: 0.4520695753338392\n",
            "Epoch 40, Loss: 0.43059379597911446\n",
            "Epoch 41, Loss: 0.4078933123661124\n",
            "Epoch 42, Loss: 0.38344892604119335\n",
            "Epoch 43, Loss: 0.3849418185022481\n",
            "Epoch 44, Loss: 0.37599777996235184\n",
            "Epoch 45, Loss: 0.34435781600225307\n",
            "Epoch 46, Loss: 0.35254286846998706\n",
            "Epoch 47, Loss: 0.32224345264379933\n",
            "Epoch 48, Loss: 0.3110047667608846\n",
            "Epoch 49, Loss: 0.3015294713551736\n",
            "Epoch 50, Loss: 0.3256017923393213\n",
            "Epoch 51, Loss: 0.30478162781509294\n",
            "Epoch 52, Loss: 0.2931097426339793\n",
            "Epoch 53, Loss: 0.29321297408674685\n",
            "Epoch 54, Loss: 0.29315495673957687\n",
            "Epoch 55, Loss: 0.22548567147358603\n",
            "Epoch 56, Loss: 0.2595531249137791\n",
            "Epoch 57, Loss: 0.2751664118579282\n",
            "Epoch 58, Loss: 0.2680414214044276\n",
            "Epoch 59, Loss: 0.25334813643980514\n",
            "Epoch 60, Loss: 0.22189033833687263\n",
            "Epoch 61, Loss: 0.24665999391575907\n",
            "Epoch 62, Loss: 0.22667305638341953\n",
            "Epoch 63, Loss: 0.23436579365483332\n",
            "Epoch 64, Loss: 0.23983385498680726\n",
            "Epoch 65, Loss: 0.2325962956451699\n",
            "Epoch 66, Loss: 0.21434019383071634\n",
            "Epoch 67, Loss: 0.23290766848017797\n",
            "Epoch 68, Loss: 0.21536477975299595\n",
            "Epoch 69, Loss: 0.22996745490090317\n",
            "Epoch 70, Loss: 0.19902926354723818\n",
            "Epoch 71, Loss: 0.21295266575596827\n",
            "Epoch 72, Loss: 0.20905573490787954\n",
            "Epoch 73, Loss: 0.2030162187221715\n",
            "Epoch 74, Loss: 0.22631978548472495\n",
            "Epoch 75, Loss: 0.19257934688759581\n",
            "Epoch 76, Loss: 0.17029897787172318\n",
            "Epoch 77, Loss: 0.2357238933939458\n",
            "Epoch 78, Loss: 0.2262629092387531\n",
            "Epoch 79, Loss: 0.18029210602154816\n",
            "Epoch 80, Loss: 0.16440358370199532\n",
            "Epoch 81, Loss: 0.19843537063168748\n",
            "Epoch 82, Loss: 0.20501772577271743\n",
            "Epoch 83, Loss: 0.17561264435672547\n",
            "Epoch 84, Loss: 0.19086917697468683\n",
            "Epoch 85, Loss: 0.18014989797111666\n",
            "Epoch 86, Loss: 0.1985356762166828\n",
            "Epoch 87, Loss: 0.18163999067166883\n",
            "Epoch 88, Loss: 0.17118740805884455\n",
            "Epoch 89, Loss: 0.17521920608704353\n",
            "Epoch 90, Loss: 0.18305361917828356\n",
            "Epoch 91, Loss: 0.16244013655139966\n",
            "Epoch 92, Loss: 0.1444295686776833\n",
            "Epoch 93, Loss: 0.2014645531849788\n",
            "Epoch 94, Loss: 0.20463659085542954\n",
            "Epoch 95, Loss: 0.17562335606693003\n",
            "Epoch 96, Loss: 0.16337094927573448\n",
            "Epoch 97, Loss: 0.13939949795794304\n",
            "Epoch 98, Loss: 0.1694432809624983\n",
            "Epoch 99, Loss: 0.1973707160871962\n",
            "Epoch 100, Loss: 0.16545137546746932\n",
            "Epoch 101, Loss: 0.15049962319738572\n",
            "Epoch 102, Loss: 0.1706388225717008\n",
            "Epoch 103, Loss: 0.15838378773111364\n",
            "Epoch 104, Loss: 0.17164590491739382\n",
            "Epoch 105, Loss: 0.1916529028616903\n",
            "Epoch 106, Loss: 0.15328900646561247\n",
            "Epoch 107, Loss: 0.14653472508044194\n",
            "Epoch 108, Loss: 0.1668701269247038\n",
            "Epoch 109, Loss: 0.17845088142491972\n",
            "Epoch 110, Loss: 0.15842910123812726\n",
            "Epoch 111, Loss: 0.14506676629223786\n",
            "Epoch 112, Loss: 0.15117541881625915\n",
            "Epoch 113, Loss: 0.14779052324593067\n",
            "Epoch 114, Loss: 0.16843395592058863\n",
            "Epoch 115, Loss: 0.1619484912832756\n",
            "Epoch 116, Loss: 0.16877873133763174\n",
            "Epoch 117, Loss: 0.15936258552915147\n",
            "Epoch 118, Loss: 0.1468125959605817\n",
            "Epoch 119, Loss: 0.13543212842053312\n",
            "Epoch 120, Loss: 0.12426615291086913\n",
            "Epoch 121, Loss: 0.17604288663191106\n",
            "Epoch 122, Loss: 0.17184164979116387\n",
            "Epoch 123, Loss: 0.14954394551799122\n",
            "Epoch 124, Loss: 0.14858362510266815\n",
            "Epoch 125, Loss: 0.13490505571789144\n",
            "Epoch 126, Loss: 0.16423342492707702\n",
            "Epoch 127, Loss: 0.15547585683634213\n",
            "Epoch 128, Loss: 0.13617151953241863\n",
            "Epoch 129, Loss: 0.14268781930265373\n",
            "Epoch 130, Loss: 0.14716898982920457\n",
            "Epoch 131, Loss: 0.1393772758129041\n",
            "Epoch 132, Loss: 0.16725727506553578\n",
            "Epoch 133, Loss: 0.11500956905800896\n",
            "Epoch 134, Loss: 0.1218890494278744\n",
            "Epoch 135, Loss: 0.1718043955686071\n",
            "Epoch 136, Loss: 0.15118644254572708\n",
            "Epoch 137, Loss: 0.11819064537839741\n",
            "Epoch 138, Loss: 0.1458041279783944\n",
            "Epoch 139, Loss: 0.14099204466890192\n",
            "Epoch 140, Loss: 0.1483703651906127\n",
            "Epoch 141, Loss: 0.15250766815622444\n",
            "Epoch 142, Loss: 0.12369531623380797\n",
            "Epoch 143, Loss: 0.1466629581616434\n",
            "Epoch 144, Loss: 0.1266649144241953\n",
            "Epoch 145, Loss: 0.1286731801041023\n",
            "Epoch 146, Loss: 0.12763885616817894\n",
            "Epoch 147, Loss: 0.15445897786323068\n",
            "Epoch 148, Loss: 0.16264066588886253\n",
            "Epoch 149, Loss: 0.1482613878205533\n",
            "Epoch 150, Loss: 0.13778182066491115\n",
            "Epoch 151, Loss: 0.10459835723142528\n",
            "Epoch 152, Loss: 0.12772043341594508\n",
            "Epoch 153, Loss: 0.15716368245804097\n",
            "Epoch 154, Loss: 0.13304533657458278\n",
            "Epoch 155, Loss: 0.1316124026013343\n",
            "Epoch 156, Loss: 0.13301988252822092\n",
            "Epoch 157, Loss: 0.13339551686144926\n",
            "Epoch 158, Loss: 0.12853247953621705\n",
            "Epoch 159, Loss: 0.14980285404407231\n",
            "Epoch 160, Loss: 0.13282144889521325\n",
            "Epoch 161, Loss: 0.1261128425488577\n",
            "Epoch 162, Loss: 0.1383886209419926\n",
            "Epoch 163, Loss: 0.1459517355989236\n",
            "Epoch 164, Loss: 0.14051866765274568\n",
            "Epoch 165, Loss: 0.12013127730535272\n",
            "Epoch 166, Loss: 0.13535079086566215\n",
            "Epoch 167, Loss: 0.1266688835757125\n",
            "Epoch 168, Loss: 0.122101305337275\n",
            "Epoch 169, Loss: 0.13342961891437582\n",
            "Epoch 170, Loss: 0.1531133942610925\n",
            "Epoch 171, Loss: 0.14168334594282234\n",
            "Epoch 172, Loss: 0.10702457750821129\n",
            "Epoch 173, Loss: 0.1090897663229662\n",
            "Epoch 174, Loss: 0.10733838421542702\n",
            "Epoch 175, Loss: 0.15306388133245966\n",
            "Epoch 176, Loss: 0.1397026039045447\n",
            "Epoch 177, Loss: 0.13784773887880622\n",
            "Epoch 178, Loss: 0.1257545227932808\n",
            "Epoch 179, Loss: 0.10336513265900676\n",
            "Epoch 180, Loss: 0.13324037001437278\n",
            "Epoch 181, Loss: 0.14309463453719684\n",
            "Epoch 182, Loss: 0.1417723289640892\n",
            "Epoch 183, Loss: 0.10959142883596441\n",
            "Epoch 184, Loss: 0.13481172804584932\n",
            "Epoch 185, Loss: 0.11996061069524044\n",
            "Epoch 186, Loss: 0.1280389656777234\n",
            "Epoch 187, Loss: 0.14140160297951127\n",
            "Epoch 188, Loss: 0.11709564080452332\n",
            "Epoch 189, Loss: 0.11840355267886386\n",
            "Epoch 190, Loss: 0.12787253252418754\n",
            "Epoch 191, Loss: 0.12952731373479298\n",
            "Epoch 192, Loss: 0.14121176365315152\n",
            "Epoch 193, Loss: 0.10490326117307348\n",
            "Epoch 194, Loss: 0.13870898283341582\n",
            "Epoch 195, Loss: 0.11348194235464192\n",
            "Epoch 196, Loss: 0.12786935439185643\n",
            "Epoch 197, Loss: 0.11415652950744495\n",
            "Epoch 198, Loss: 0.1235642126663719\n",
            "Epoch 199, Loss: 0.12710048174938124\n",
            "Accuracy: 33.79%\n",
            "Training with params: {'batch_size': 128, 'lr': 0.001, 'optimizer': 'SGD'}\n",
            "Epoch 1, Loss: 4.605558596608584\n",
            "Epoch 2, Loss: 4.605234990034567\n",
            "Epoch 3, Loss: 4.604922900724289\n",
            "Epoch 4, Loss: 4.604623611625808\n",
            "Epoch 5, Loss: 4.60433013969675\n",
            "Epoch 6, Loss: 4.604035801899707\n",
            "Epoch 7, Loss: 4.603736846953097\n",
            "Epoch 8, Loss: 4.603421813691669\n",
            "Epoch 9, Loss: 4.603102713289773\n",
            "Epoch 10, Loss: 4.602763311332449\n",
            "Epoch 11, Loss: 4.60242318253383\n",
            "Epoch 12, Loss: 4.602068810816616\n",
            "Epoch 13, Loss: 4.601708618271381\n",
            "Epoch 14, Loss: 4.601318282544461\n",
            "Epoch 15, Loss: 4.600901843946608\n",
            "Epoch 16, Loss: 4.600457845136638\n",
            "Epoch 17, Loss: 4.5999863653841535\n",
            "Epoch 18, Loss: 4.599468381203654\n",
            "Epoch 19, Loss: 4.598910081721938\n",
            "Epoch 20, Loss: 4.59827421998124\n",
            "Epoch 21, Loss: 4.597596738039685\n",
            "Epoch 22, Loss: 4.596823028896166\n",
            "Epoch 23, Loss: 4.595969891609133\n",
            "Epoch 24, Loss: 4.595007551295678\n",
            "Epoch 25, Loss: 4.593941045844036\n",
            "Epoch 26, Loss: 4.592738379602847\n",
            "Epoch 27, Loss: 4.591402066028332\n",
            "Epoch 28, Loss: 4.5898756163809304\n",
            "Epoch 29, Loss: 4.588185721346179\n",
            "Epoch 30, Loss: 4.586215516795283\n",
            "Epoch 31, Loss: 4.5839698076857935\n",
            "Epoch 32, Loss: 4.581411100714408\n",
            "Epoch 33, Loss: 4.578385506749458\n",
            "Epoch 34, Loss: 4.574909121179215\n",
            "Epoch 35, Loss: 4.570858334641322\n",
            "Epoch 36, Loss: 4.566096622925585\n",
            "Epoch 37, Loss: 4.560570794907982\n",
            "Epoch 38, Loss: 4.5540407124687645\n",
            "Epoch 39, Loss: 4.54623137471621\n",
            "Epoch 40, Loss: 4.536818018959611\n",
            "Epoch 41, Loss: 4.525245629917935\n",
            "Epoch 42, Loss: 4.511057646378227\n",
            "Epoch 43, Loss: 4.493293113415809\n",
            "Epoch 44, Loss: 4.471431212656943\n",
            "Epoch 45, Loss: 4.445085269410897\n",
            "Epoch 46, Loss: 4.41533292102082\n",
            "Epoch 47, Loss: 4.383975170457455\n",
            "Epoch 48, Loss: 4.353261697627699\n",
            "Epoch 49, Loss: 4.324862081376488\n",
            "Epoch 50, Loss: 4.299325914943919\n",
            "Epoch 51, Loss: 4.276429622679415\n",
            "Epoch 52, Loss: 4.255530909809005\n",
            "Epoch 53, Loss: 4.236257210411988\n",
            "Epoch 54, Loss: 4.219080614007038\n",
            "Epoch 55, Loss: 4.20258724232159\n",
            "Epoch 56, Loss: 4.187623136793561\n",
            "Epoch 57, Loss: 4.173047792576158\n",
            "Epoch 58, Loss: 4.159195767644117\n",
            "Epoch 59, Loss: 4.145686501432258\n",
            "Epoch 60, Loss: 4.132038413411212\n",
            "Epoch 61, Loss: 4.119532769903198\n",
            "Epoch 62, Loss: 4.105540223133838\n",
            "Epoch 63, Loss: 4.091817567415554\n",
            "Epoch 64, Loss: 4.077870736036764\n",
            "Epoch 65, Loss: 4.062691440972526\n",
            "Epoch 66, Loss: 4.047578123219482\n",
            "Epoch 67, Loss: 4.029592375011395\n",
            "Epoch 68, Loss: 4.0109784206770875\n",
            "Epoch 69, Loss: 3.9896744956140933\n",
            "Epoch 70, Loss: 3.9658536789057504\n",
            "Epoch 71, Loss: 3.9394069242355463\n",
            "Epoch 72, Loss: 3.910481623066661\n",
            "Epoch 73, Loss: 3.8824981447985714\n",
            "Epoch 74, Loss: 3.855271066241252\n",
            "Epoch 75, Loss: 3.8304530616916352\n",
            "Epoch 76, Loss: 3.8085674867605612\n",
            "Epoch 77, Loss: 3.7878675412034135\n",
            "Epoch 78, Loss: 3.770353639827055\n",
            "Epoch 79, Loss: 3.7542916789384146\n",
            "Epoch 80, Loss: 3.7403957319381598\n",
            "Epoch 81, Loss: 3.7264950354690747\n",
            "Epoch 82, Loss: 3.7144746609660975\n",
            "Epoch 83, Loss: 3.7029690315656345\n",
            "Epoch 84, Loss: 3.692240569292737\n",
            "Epoch 85, Loss: 3.6816286772413327\n",
            "Epoch 86, Loss: 3.6716514639842237\n",
            "Epoch 87, Loss: 3.661583635813135\n",
            "Epoch 88, Loss: 3.6519541850175394\n",
            "Epoch 89, Loss: 3.642852841740679\n",
            "Epoch 90, Loss: 3.632614783313878\n",
            "Epoch 91, Loss: 3.623325975349797\n",
            "Epoch 92, Loss: 3.614098805600725\n",
            "Epoch 93, Loss: 3.6038349766255644\n",
            "Epoch 94, Loss: 3.594149715150409\n",
            "Epoch 95, Loss: 3.586126090315602\n",
            "Epoch 96, Loss: 3.57729761679764\n",
            "Epoch 97, Loss: 3.5666379861514588\n",
            "Epoch 98, Loss: 3.558765326619453\n",
            "Epoch 99, Loss: 3.5495913851901393\n",
            "Epoch 100, Loss: 3.5397165467976914\n",
            "Epoch 101, Loss: 3.530221005534882\n",
            "Epoch 102, Loss: 3.522416025781266\n",
            "Epoch 103, Loss: 3.5127830535859403\n",
            "Epoch 104, Loss: 3.5034615402026557\n",
            "Epoch 105, Loss: 3.494054097348772\n",
            "Epoch 106, Loss: 3.485037436570658\n",
            "Epoch 107, Loss: 3.478419844756651\n",
            "Epoch 108, Loss: 3.467992738689608\n",
            "Epoch 109, Loss: 3.4607298569301204\n",
            "Epoch 110, Loss: 3.4520210799048927\n",
            "Epoch 111, Loss: 3.4433149328012296\n",
            "Epoch 112, Loss: 3.4350817551088455\n",
            "Epoch 113, Loss: 3.4269478662544506\n",
            "Epoch 114, Loss: 3.4200126588192132\n",
            "Epoch 115, Loss: 3.411679052635837\n",
            "Epoch 116, Loss: 3.4033314821970126\n",
            "Epoch 117, Loss: 3.3963727463236855\n",
            "Epoch 118, Loss: 3.3891806559794393\n",
            "Epoch 119, Loss: 3.3808697776111494\n",
            "Epoch 120, Loss: 3.374153590873074\n",
            "Epoch 121, Loss: 3.36733543354532\n",
            "Epoch 122, Loss: 3.3601388620293657\n",
            "Epoch 123, Loss: 3.353577302849811\n",
            "Epoch 124, Loss: 3.346225269615193\n",
            "Epoch 125, Loss: 3.338028695577246\n",
            "Epoch 126, Loss: 3.3327425215250392\n",
            "Epoch 127, Loss: 3.32657560546075\n",
            "Epoch 128, Loss: 3.3199431902307377\n",
            "Epoch 129, Loss: 3.3140573275973426\n",
            "Epoch 130, Loss: 3.307713740926874\n",
            "Epoch 131, Loss: 3.300061137475016\n",
            "Epoch 132, Loss: 3.2948984922960287\n",
            "Epoch 133, Loss: 3.2882327313923163\n",
            "Epoch 134, Loss: 3.282605232180232\n",
            "Epoch 135, Loss: 3.275782011354061\n",
            "Epoch 136, Loss: 3.26993509082843\n",
            "Epoch 137, Loss: 3.264157971457752\n",
            "Epoch 138, Loss: 3.2579163706211176\n",
            "Epoch 139, Loss: 3.251516097037079\n",
            "Epoch 140, Loss: 3.244802814005586\n",
            "Epoch 141, Loss: 3.2384740007502955\n",
            "Epoch 142, Loss: 3.232965618143301\n",
            "Epoch 143, Loss: 3.2268981732370907\n",
            "Epoch 144, Loss: 3.220998232321971\n",
            "Epoch 145, Loss: 3.2137302315753438\n",
            "Epoch 146, Loss: 3.208475823902413\n",
            "Epoch 147, Loss: 3.2025622980064137\n",
            "Epoch 148, Loss: 3.1970844811490733\n",
            "Epoch 149, Loss: 3.1896320444238766\n",
            "Epoch 150, Loss: 3.1834688772021047\n",
            "Epoch 151, Loss: 3.178303775884916\n",
            "Epoch 152, Loss: 3.171891386856508\n",
            "Epoch 153, Loss: 3.166037799757155\n",
            "Epoch 154, Loss: 3.158350167676921\n",
            "Epoch 155, Loss: 3.152221493099047\n",
            "Epoch 156, Loss: 3.1465914987237253\n",
            "Epoch 157, Loss: 3.141502573057209\n",
            "Epoch 158, Loss: 3.1342281136671297\n",
            "Epoch 159, Loss: 3.127570237040215\n",
            "Epoch 160, Loss: 3.1210136925777814\n",
            "Epoch 161, Loss: 3.1155426953454763\n",
            "Epoch 162, Loss: 3.1073933391619826\n",
            "Epoch 163, Loss: 3.1009254821426118\n",
            "Epoch 164, Loss: 3.094637657370409\n",
            "Epoch 165, Loss: 3.0871631904026433\n",
            "Epoch 166, Loss: 3.0800600320177005\n",
            "Epoch 167, Loss: 3.073677522751986\n",
            "Epoch 168, Loss: 3.0671466640804126\n",
            "Epoch 169, Loss: 3.0595663258486696\n",
            "Epoch 170, Loss: 3.051922943890857\n",
            "Epoch 171, Loss: 3.0451560538748037\n",
            "Epoch 172, Loss: 3.0385546117182582\n",
            "Epoch 173, Loss: 3.0316662209113234\n",
            "Epoch 174, Loss: 3.0215985677431307\n",
            "Epoch 175, Loss: 3.0169878615747634\n",
            "Epoch 176, Loss: 3.0082962860536697\n",
            "Epoch 177, Loss: 3.0005229068229267\n",
            "Epoch 178, Loss: 2.9940707000625104\n",
            "Epoch 179, Loss: 2.9870221212391965\n",
            "Epoch 180, Loss: 2.9784400389932304\n",
            "Epoch 181, Loss: 2.9713076850032563\n",
            "Epoch 182, Loss: 2.9634844488500023\n",
            "Epoch 183, Loss: 2.956847902454074\n",
            "Epoch 184, Loss: 2.950191233164209\n",
            "Epoch 185, Loss: 2.941984055292271\n",
            "Epoch 186, Loss: 2.934533663722865\n",
            "Epoch 187, Loss: 2.9300200609904725\n",
            "Epoch 188, Loss: 2.921740046547502\n",
            "Epoch 189, Loss: 2.914848948378697\n",
            "Epoch 190, Loss: 2.908527954155222\n",
            "Epoch 191, Loss: 2.901262054662875\n",
            "Epoch 192, Loss: 2.8933557006709107\n",
            "Epoch 193, Loss: 2.88844984754577\n",
            "Epoch 194, Loss: 2.881617188148791\n",
            "Epoch 195, Loss: 2.87429108095291\n",
            "Epoch 196, Loss: 2.8674027870987993\n",
            "Epoch 197, Loss: 2.8626720350416726\n",
            "Epoch 198, Loss: 2.8556602623151695\n",
            "Epoch 199, Loss: 2.8486271947241195\n",
            "Accuracy: 25.96%\n",
            "Training with params: {'batch_size': 128, 'lr': 0.001, 'optimizer': 'RMSprop'}\n",
            "Epoch 1, Loss: 3.8285667969442696\n",
            "Epoch 2, Loss: 3.2224180667906466\n",
            "Epoch 3, Loss: 2.9447125010478223\n",
            "Epoch 4, Loss: 2.7424536611113095\n",
            "Epoch 5, Loss: 2.5833697422691015\n",
            "Epoch 6, Loss: 2.4468037557723883\n",
            "Epoch 7, Loss: 2.322647258753667\n",
            "Epoch 8, Loss: 2.210230025489007\n",
            "Epoch 9, Loss: 2.1075096145615246\n",
            "Epoch 10, Loss: 2.016667591336438\n",
            "Epoch 11, Loss: 1.925248987534467\n",
            "Epoch 12, Loss: 1.841218557504132\n",
            "Epoch 13, Loss: 1.760710101298359\n",
            "Epoch 14, Loss: 1.6871008754081434\n",
            "Epoch 15, Loss: 1.6133469821851882\n",
            "Epoch 16, Loss: 1.5427713083184285\n",
            "Epoch 17, Loss: 1.476361306122197\n",
            "Epoch 18, Loss: 1.4114406538741362\n",
            "Epoch 19, Loss: 1.3515994993926923\n",
            "Epoch 20, Loss: 1.2905311444226433\n",
            "Epoch 21, Loss: 1.2353091855793048\n",
            "Epoch 22, Loss: 1.1834384518511154\n",
            "Epoch 23, Loss: 1.1224628042077165\n",
            "Epoch 24, Loss: 1.0759298470623964\n",
            "Epoch 25, Loss: 1.0224276460954904\n",
            "Epoch 26, Loss: 0.9810115716341511\n",
            "Epoch 27, Loss: 0.9340531452537497\n",
            "Epoch 28, Loss: 0.8903168596879906\n",
            "Epoch 29, Loss: 0.8492397530304502\n",
            "Epoch 30, Loss: 0.8105396880670581\n",
            "Epoch 31, Loss: 0.7690320966951073\n",
            "Epoch 32, Loss: 0.7393672608048715\n",
            "Epoch 33, Loss: 0.7029422630586892\n",
            "Epoch 34, Loss: 0.6727677099692547\n",
            "Epoch 35, Loss: 0.6400169391004021\n",
            "Epoch 36, Loss: 0.6100595604123362\n",
            "Epoch 37, Loss: 0.5859816878500497\n",
            "Epoch 38, Loss: 0.558573428398508\n",
            "Epoch 39, Loss: 0.528852032311737\n",
            "Epoch 40, Loss: 0.5128308807111457\n",
            "Epoch 41, Loss: 0.47914332265744125\n",
            "Epoch 42, Loss: 0.4632627524607017\n",
            "Epoch 43, Loss: 0.43854717106160607\n",
            "Epoch 44, Loss: 0.42682280511502413\n",
            "Epoch 45, Loss: 0.4070079586161372\n",
            "Epoch 46, Loss: 0.3858985362760246\n",
            "Epoch 47, Loss: 0.38133949933149625\n",
            "Epoch 48, Loss: 0.35909948667601854\n",
            "Epoch 49, Loss: 0.3522642358489659\n",
            "Epoch 50, Loss: 0.33463702810085033\n",
            "Epoch 51, Loss: 0.3254849653490974\n",
            "Epoch 52, Loss: 0.3129587719964859\n",
            "Epoch 53, Loss: 0.30542634534256535\n",
            "Epoch 54, Loss: 0.29459139182591987\n",
            "Epoch 55, Loss: 0.28684781939553483\n",
            "Epoch 56, Loss: 0.2796536601336716\n",
            "Epoch 57, Loss: 0.27203380310779335\n",
            "Epoch 58, Loss: 0.2687976027807921\n",
            "Epoch 59, Loss: 0.25862551983588794\n",
            "Epoch 60, Loss: 0.25559431455476816\n",
            "Epoch 61, Loss: 0.24605932768882083\n",
            "Epoch 62, Loss: 0.24549775684962188\n",
            "Epoch 63, Loss: 0.24515242875575105\n",
            "Epoch 64, Loss: 0.23270597785253963\n",
            "Epoch 65, Loss: 0.23241795734752474\n",
            "Epoch 66, Loss: 0.2266858388167208\n",
            "Epoch 67, Loss: 0.2279176750908727\n",
            "Epoch 68, Loss: 0.22033194126680378\n",
            "Epoch 69, Loss: 0.21299732481236652\n",
            "Epoch 70, Loss: 0.21290786891146693\n",
            "Epoch 71, Loss: 0.20564933658560827\n",
            "Epoch 72, Loss: 0.20610227043290272\n",
            "Epoch 73, Loss: 0.20064602437836435\n",
            "Epoch 74, Loss: 0.1998876176888833\n",
            "Epoch 75, Loss: 0.19738217568039285\n",
            "Epoch 76, Loss: 0.2006742803146467\n",
            "Epoch 77, Loss: 0.20117887396298711\n",
            "Epoch 78, Loss: 0.1901004596439469\n",
            "Epoch 79, Loss: 0.1857366788884639\n",
            "Epoch 80, Loss: 0.20039671746170734\n",
            "Epoch 81, Loss: 0.17909687936610882\n",
            "Epoch 82, Loss: 0.19326747391287172\n",
            "Epoch 83, Loss: 0.1833021199268758\n",
            "Epoch 84, Loss: 0.1825082064379969\n",
            "Epoch 85, Loss: 0.1774234819461775\n",
            "Epoch 86, Loss: 0.18243464255405356\n",
            "Epoch 87, Loss: 0.17594337842577254\n",
            "Epoch 88, Loss: 0.17193923558553922\n",
            "Epoch 89, Loss: 0.18181661140087926\n",
            "Epoch 90, Loss: 0.16709443554282188\n",
            "Epoch 91, Loss: 0.18236689950765855\n",
            "Epoch 92, Loss: 0.1652381067995525\n",
            "Epoch 93, Loss: 0.17464549029650894\n",
            "Epoch 94, Loss: 0.1632818339625969\n",
            "Epoch 95, Loss: 0.1716396320620766\n",
            "Epoch 96, Loss: 0.16564385521956873\n",
            "Epoch 97, Loss: 0.16838994668439375\n",
            "Epoch 98, Loss: 0.16268599142446694\n",
            "Epoch 99, Loss: 0.1612907335724291\n",
            "Epoch 100, Loss: 0.16997063796386083\n",
            "Epoch 101, Loss: 0.16273501030910198\n",
            "Epoch 102, Loss: 0.16047051547528685\n",
            "Epoch 103, Loss: 0.16125341752053374\n",
            "Epoch 104, Loss: 0.16175394618640776\n",
            "Epoch 105, Loss: 0.160598561994236\n",
            "Epoch 106, Loss: 0.15415161881891207\n",
            "Epoch 107, Loss: 0.1652313943028145\n",
            "Epoch 108, Loss: 0.1559102826268243\n",
            "Epoch 109, Loss: 0.15995458742637006\n",
            "Epoch 110, Loss: 0.15872958584991106\n",
            "Epoch 111, Loss: 0.15448568527208037\n",
            "Epoch 112, Loss: 0.15991901302391\n",
            "Epoch 113, Loss: 0.15418268059430373\n",
            "Epoch 114, Loss: 0.15409482653250398\n",
            "Epoch 115, Loss: 0.15133842182300433\n",
            "Epoch 116, Loss: 0.14863572817991305\n",
            "Epoch 117, Loss: 0.14942846901219367\n",
            "Epoch 118, Loss: 0.15142309396048945\n",
            "Epoch 119, Loss: 0.15321291011312735\n",
            "Epoch 120, Loss: 0.14376966696223029\n",
            "Epoch 121, Loss: 0.146739649314843\n",
            "Epoch 122, Loss: 0.14868519741975134\n",
            "Epoch 123, Loss: 0.15006902591442056\n",
            "Epoch 124, Loss: 0.1416014426142511\n",
            "Epoch 125, Loss: 0.14267237323915105\n",
            "Epoch 126, Loss: 0.14822659335668434\n",
            "Epoch 127, Loss: 0.1441311858537252\n",
            "Epoch 128, Loss: 0.14136932287222284\n",
            "Epoch 129, Loss: 0.14449129487051987\n",
            "Epoch 130, Loss: 0.14563888298047473\n",
            "Epoch 131, Loss: 0.14462115617988208\n",
            "Epoch 132, Loss: 0.13248226712064823\n",
            "Epoch 133, Loss: 0.15109480209315143\n",
            "Epoch 134, Loss: 0.13610099803875475\n",
            "Epoch 135, Loss: 0.1415105891316329\n",
            "Epoch 136, Loss: 0.13684526682281128\n",
            "Epoch 137, Loss: 0.14421280503601713\n",
            "Epoch 138, Loss: 0.14178328158076653\n",
            "Epoch 139, Loss: 0.13954428367583496\n",
            "Epoch 140, Loss: 0.13721599753069527\n",
            "Epoch 141, Loss: 0.13927588624465267\n",
            "Epoch 142, Loss: 0.13614640118377974\n",
            "Epoch 143, Loss: 0.1352903059948131\n",
            "Epoch 144, Loss: 0.13190453227542703\n",
            "Epoch 145, Loss: 0.13990885126487831\n",
            "Epoch 146, Loss: 0.12913179925411863\n",
            "Epoch 147, Loss: 0.1318417223113234\n",
            "Epoch 148, Loss: 0.1354059903641872\n",
            "Epoch 149, Loss: 0.13452028030591548\n",
            "Epoch 150, Loss: 0.13927147794000405\n",
            "Epoch 151, Loss: 0.134102107118577\n",
            "Epoch 152, Loss: 0.13483412707191142\n",
            "Epoch 153, Loss: 0.13494997790149982\n",
            "Epoch 154, Loss: 0.13360087537799803\n",
            "Epoch 155, Loss: 0.12737000345245308\n",
            "Epoch 156, Loss: 0.12829801952109085\n",
            "Epoch 157, Loss: 0.12899193471616796\n",
            "Epoch 158, Loss: 0.1327138779901654\n",
            "Epoch 159, Loss: 0.13142034740847014\n",
            "Epoch 160, Loss: 0.12731954898408918\n",
            "Epoch 161, Loss: 0.13143222928618836\n",
            "Epoch 162, Loss: 0.1223874214460211\n",
            "Epoch 163, Loss: 0.12703252499184722\n",
            "Epoch 164, Loss: 0.13357539123872206\n",
            "Epoch 165, Loss: 0.13082524830632655\n",
            "Epoch 166, Loss: 0.1261264987866325\n",
            "Epoch 167, Loss: 0.13437751154808322\n",
            "Epoch 168, Loss: 0.13414798769385308\n",
            "Epoch 169, Loss: 0.12608716735625855\n",
            "Epoch 170, Loss: 0.12938399212982724\n",
            "Epoch 171, Loss: 0.12544042851461473\n",
            "Epoch 172, Loss: 0.12564801675793918\n",
            "Epoch 173, Loss: 0.12734844817963364\n",
            "Epoch 174, Loss: 0.13359276442300252\n",
            "Epoch 175, Loss: 0.12464354711744334\n",
            "Epoch 176, Loss: 0.12509858130436877\n",
            "Epoch 177, Loss: 0.12293952795536355\n",
            "Epoch 178, Loss: 0.12790637503942603\n",
            "Epoch 179, Loss: 0.13108193124060893\n",
            "Epoch 180, Loss: 0.12357246176020512\n",
            "Epoch 181, Loss: 0.12122296088177453\n",
            "Epoch 182, Loss: 0.12273233343758966\n",
            "Epoch 183, Loss: 0.12147019561880347\n",
            "Epoch 184, Loss: 0.12365351820393178\n",
            "Epoch 185, Loss: 0.1246821566215714\n",
            "Epoch 186, Loss: 0.12920511587072744\n",
            "Epoch 187, Loss: 0.12806223125269642\n",
            "Epoch 188, Loss: 0.11829014483939314\n",
            "Epoch 189, Loss: 0.12353381185906717\n",
            "Epoch 190, Loss: 0.12792691984035245\n",
            "Epoch 191, Loss: 0.12844235748124053\n",
            "Epoch 192, Loss: 0.12460351234916454\n",
            "Epoch 193, Loss: 0.1274012838921431\n",
            "Epoch 194, Loss: 0.12369583136475909\n",
            "Epoch 195, Loss: 0.12376271365353328\n",
            "Epoch 196, Loss: 0.12071854942728338\n",
            "Epoch 197, Loss: 0.11861752112255529\n",
            "Epoch 198, Loss: 0.12964590947808283\n",
            "Epoch 199, Loss: 0.11891903530191773\n",
            "Accuracy: 34.16%\n",
            "Training with params: {'batch_size': 128, 'lr': 0.005, 'optimizer': 'Adam'}\n",
            "Epoch 1, Loss: 3.8821330991242546\n",
            "Epoch 2, Loss: 3.2650671486964313\n",
            "Epoch 3, Loss: 3.012972035371434\n",
            "Epoch 4, Loss: 2.878359958034037\n",
            "Epoch 5, Loss: 2.768663281370002\n",
            "Epoch 6, Loss: 2.683367397474206\n",
            "Epoch 7, Loss: 2.61872964076069\n",
            "Epoch 8, Loss: 2.569755443221773\n",
            "Epoch 9, Loss: 2.5320257677141664\n",
            "Epoch 10, Loss: 2.495094590784644\n",
            "Epoch 11, Loss: 2.466505551277219\n",
            "Epoch 12, Loss: 2.4517252359853683\n",
            "Epoch 13, Loss: 2.433559863463692\n",
            "Epoch 14, Loss: 2.4058058649072867\n",
            "Epoch 15, Loss: 2.392227481393253\n",
            "Epoch 16, Loss: 2.37286228231152\n",
            "Epoch 17, Loss: 2.3596779946475994\n",
            "Epoch 18, Loss: 2.3484683863037383\n",
            "Epoch 19, Loss: 2.3402887405946737\n",
            "Epoch 20, Loss: 2.318685144719565\n",
            "Epoch 21, Loss: 2.321757109268852\n",
            "Epoch 22, Loss: 2.31193045582003\n",
            "Epoch 23, Loss: 2.304706976846661\n",
            "Epoch 24, Loss: 2.2855203005358997\n",
            "Epoch 25, Loss: 2.2871248630611487\n",
            "Epoch 26, Loss: 2.2779209778436917\n",
            "Epoch 27, Loss: 2.268873219599809\n",
            "Epoch 28, Loss: 2.268055881075847\n",
            "Epoch 29, Loss: 2.257290159344978\n",
            "Epoch 30, Loss: 2.2571509320412755\n",
            "Epoch 31, Loss: 2.2530057012577496\n",
            "Epoch 32, Loss: 2.2478872211388006\n",
            "Epoch 33, Loss: 2.2437856767488564\n",
            "Epoch 34, Loss: 2.234160170530724\n",
            "Epoch 35, Loss: 2.219272540048565\n",
            "Epoch 36, Loss: 2.226796627959327\n",
            "Epoch 37, Loss: 2.216121858952905\n",
            "Epoch 38, Loss: 2.21438259450371\n",
            "Epoch 39, Loss: 2.208581024728468\n",
            "Epoch 40, Loss: 2.2024686144440984\n",
            "Epoch 41, Loss: 2.2049041730363657\n",
            "Epoch 42, Loss: 2.1866387186757743\n",
            "Epoch 43, Loss: 2.194165891393676\n",
            "Epoch 44, Loss: 2.1920052721067464\n",
            "Epoch 45, Loss: 2.1869712711295204\n",
            "Epoch 46, Loss: 2.184432420279364\n",
            "Epoch 47, Loss: 2.172864572471365\n",
            "Epoch 48, Loss: 2.176351826819008\n",
            "Epoch 49, Loss: 2.1734445576777546\n",
            "Epoch 50, Loss: 2.175274462955992\n",
            "Epoch 51, Loss: 2.1733551665645123\n",
            "Epoch 52, Loss: 2.1675085814102837\n",
            "Epoch 53, Loss: 2.1623595120656827\n",
            "Epoch 54, Loss: 2.159183692139433\n",
            "Epoch 55, Loss: 2.1561486711892326\n",
            "Epoch 56, Loss: 2.150527570863514\n",
            "Epoch 57, Loss: 2.1564015482392764\n",
            "Epoch 58, Loss: 2.1624318778972187\n",
            "Epoch 59, Loss: 2.15724688387283\n",
            "Epoch 60, Loss: 2.1516774953783626\n",
            "Epoch 61, Loss: 2.144742579716246\n",
            "Epoch 62, Loss: 2.1384615248731333\n",
            "Epoch 63, Loss: 2.1439851379150623\n",
            "Epoch 64, Loss: 2.1418022908213192\n",
            "Epoch 65, Loss: 2.1407117962532336\n",
            "Epoch 66, Loss: 2.1417200571435795\n",
            "Epoch 67, Loss: 2.1322912861928915\n",
            "Epoch 68, Loss: 2.130244825502186\n",
            "Epoch 69, Loss: 2.139783022348838\n",
            "Epoch 70, Loss: 2.1290474797758603\n",
            "Epoch 71, Loss: 2.1372411251068115\n",
            "Epoch 72, Loss: 2.1241777565168296\n",
            "Epoch 73, Loss: 2.112894807630183\n",
            "Epoch 74, Loss: 2.1241980196569887\n",
            "Epoch 75, Loss: 2.106448788777032\n",
            "Epoch 76, Loss: 2.1051890837871814\n",
            "Epoch 77, Loss: 2.1159632913291913\n",
            "Epoch 78, Loss: 2.1225171296492866\n",
            "Epoch 79, Loss: 2.1115868786716705\n",
            "Epoch 80, Loss: 2.096310479561691\n",
            "Epoch 81, Loss: 2.1150479024023654\n",
            "Epoch 82, Loss: 2.1070946147069907\n",
            "Epoch 83, Loss: 2.1158109555768845\n",
            "Epoch 84, Loss: 2.1164570583406923\n",
            "Epoch 85, Loss: 2.0908058918345613\n",
            "Epoch 86, Loss: 2.0920899950939678\n",
            "Epoch 87, Loss: 2.0955578293032047\n",
            "Epoch 88, Loss: 2.098475341906633\n",
            "Epoch 89, Loss: 2.101744121602734\n",
            "Epoch 90, Loss: 2.0990503776408826\n",
            "Epoch 91, Loss: 2.098355603340032\n",
            "Epoch 92, Loss: 2.102025116496074\n",
            "Epoch 93, Loss: 2.0808675975141013\n",
            "Epoch 94, Loss: 2.0977155464079678\n",
            "Epoch 95, Loss: 2.0964328882944248\n",
            "Epoch 96, Loss: 2.09923472764242\n",
            "Epoch 97, Loss: 2.0957546645723037\n",
            "Epoch 98, Loss: 2.110320122650517\n",
            "Epoch 99, Loss: 2.091151277732361\n",
            "Epoch 100, Loss: 2.091862988593938\n",
            "Epoch 101, Loss: 2.089080262367073\n",
            "Epoch 102, Loss: 2.088075128662617\n",
            "Epoch 103, Loss: 2.0915865093240957\n",
            "Epoch 104, Loss: 2.0839204486373744\n",
            "Epoch 105, Loss: 2.090929111861207\n",
            "Epoch 106, Loss: 2.0840384322968895\n",
            "Epoch 107, Loss: 2.075383939706456\n",
            "Epoch 108, Loss: 2.091786727576\n",
            "Epoch 109, Loss: 2.0754564463939813\n",
            "Epoch 110, Loss: 2.081680155776041\n",
            "Epoch 111, Loss: 2.09317942196146\n",
            "Epoch 112, Loss: 2.073007610142993\n",
            "Epoch 113, Loss: 2.0861489349009132\n",
            "Epoch 114, Loss: 2.0699366398174743\n",
            "Epoch 115, Loss: 2.077495826174841\n",
            "Epoch 116, Loss: 2.074845139022983\n",
            "Epoch 117, Loss: 2.0692294037250605\n",
            "Epoch 118, Loss: 2.0716913965961816\n",
            "Epoch 119, Loss: 2.0712198162322766\n",
            "Epoch 120, Loss: 2.0781941386439917\n",
            "Epoch 121, Loss: 2.0741906251443925\n",
            "Epoch 122, Loss: 2.082690579202169\n",
            "Epoch 123, Loss: 2.070931797747112\n",
            "Epoch 124, Loss: 2.0643169953085274\n",
            "Epoch 125, Loss: 2.06896036665153\n",
            "Epoch 126, Loss: 2.085728752643556\n",
            "Epoch 127, Loss: 2.06217071131977\n",
            "Epoch 128, Loss: 2.08279825933754\n",
            "Epoch 129, Loss: 2.0576854919838476\n",
            "Epoch 130, Loss: 2.0779138141885745\n",
            "Epoch 131, Loss: 2.0729659725637997\n",
            "Epoch 132, Loss: 2.0776951005086874\n",
            "Epoch 133, Loss: 2.0653759811235513\n",
            "Epoch 134, Loss: 2.0723200542542637\n",
            "Epoch 135, Loss: 2.059231891656471\n",
            "Epoch 136, Loss: 2.0648126474121953\n",
            "Epoch 137, Loss: 2.056730807894636\n",
            "Epoch 138, Loss: 2.0737191118547678\n",
            "Epoch 139, Loss: 2.061696480912016\n",
            "Epoch 140, Loss: 2.0666336165669628\n",
            "Epoch 141, Loss: 2.0616029699135314\n",
            "Epoch 142, Loss: 2.0647802124242953\n",
            "Epoch 143, Loss: 2.058366617583253\n",
            "Epoch 144, Loss: 2.0683454595258475\n",
            "Epoch 145, Loss: 2.056579029468624\n",
            "Epoch 146, Loss: 2.0631421984309126\n",
            "Epoch 147, Loss: 2.052662684789399\n",
            "Epoch 148, Loss: 2.0541905676922223\n",
            "Epoch 149, Loss: 2.0565760361264123\n",
            "Epoch 150, Loss: 2.0649327783633376\n",
            "Epoch 151, Loss: 2.0580420414809986\n",
            "Epoch 152, Loss: 2.057374472508345\n",
            "Epoch 153, Loss: 2.054819399438551\n",
            "Epoch 154, Loss: 2.0558958925554514\n",
            "Epoch 155, Loss: 2.059747351100073\n",
            "Epoch 156, Loss: 2.070589137809051\n",
            "Epoch 157, Loss: 2.0639792549640625\n",
            "Epoch 158, Loss: 2.046589017219251\n",
            "Epoch 159, Loss: 2.0489804208126214\n",
            "Epoch 160, Loss: 2.045476174720413\n",
            "Epoch 161, Loss: 2.0586457526897224\n",
            "Epoch 162, Loss: 2.0610033499310387\n",
            "Epoch 163, Loss: 2.0435346947301682\n",
            "Epoch 164, Loss: 2.05862529503415\n",
            "Epoch 165, Loss: 2.0539487414347852\n",
            "Epoch 166, Loss: 2.0477226027442366\n",
            "Epoch 167, Loss: 2.04574647888808\n",
            "Epoch 168, Loss: 2.0529995104845833\n",
            "Epoch 169, Loss: 2.060322377992713\n",
            "Epoch 170, Loss: 2.040641707837429\n",
            "Epoch 171, Loss: 2.0530350354626354\n",
            "Epoch 172, Loss: 2.0582033346985917\n",
            "Epoch 173, Loss: 2.0584983337870644\n",
            "Epoch 174, Loss: 2.0603944127212097\n",
            "Epoch 175, Loss: 2.038036777540241\n",
            "Epoch 176, Loss: 2.043658739770465\n",
            "Epoch 177, Loss: 2.0527048476821625\n",
            "Epoch 178, Loss: 2.0555088077969565\n",
            "Epoch 179, Loss: 2.059121459646298\n",
            "Epoch 180, Loss: 2.0462251352837018\n",
            "Epoch 181, Loss: 2.0461443312027874\n",
            "Epoch 182, Loss: 2.0523928498368127\n",
            "Epoch 183, Loss: 2.049912354525398\n",
            "Epoch 184, Loss: 2.0500433993766376\n",
            "Epoch 185, Loss: 2.0458754450463883\n",
            "Epoch 186, Loss: 2.045623286605796\n",
            "Epoch 187, Loss: 2.0474529915758413\n",
            "Epoch 188, Loss: 2.048127449686875\n",
            "Epoch 189, Loss: 2.048389691221135\n",
            "Epoch 190, Loss: 2.046336120351806\n",
            "Epoch 191, Loss: 2.0416796716582746\n",
            "Epoch 192, Loss: 2.041470218497469\n",
            "Epoch 193, Loss: 2.0559658424933547\n",
            "Epoch 194, Loss: 2.0460612286082314\n",
            "Epoch 195, Loss: 2.035600475338109\n",
            "Epoch 196, Loss: 2.0454180609539647\n",
            "Epoch 197, Loss: 2.0365032982033537\n",
            "Epoch 198, Loss: 2.037575168987674\n",
            "Epoch 199, Loss: 2.0410476932135384\n",
            "Accuracy: 28.97%\n",
            "Training with params: {'batch_size': 128, 'lr': 0.005, 'optimizer': 'SGD'}\n",
            "Epoch 1, Loss: 4.605814710602431\n",
            "Epoch 2, Loss: 4.604910101732025\n",
            "Epoch 3, Loss: 4.603960968954179\n",
            "Epoch 4, Loss: 4.6028402089462865\n",
            "Epoch 5, Loss: 4.601342157329745\n",
            "Epoch 6, Loss: 4.59913113172097\n",
            "Epoch 7, Loss: 4.595642728878714\n",
            "Epoch 8, Loss: 4.589849217163632\n",
            "Epoch 9, Loss: 4.580088848348164\n",
            "Epoch 10, Loss: 4.563643554287493\n",
            "Epoch 11, Loss: 4.531558347784954\n",
            "Epoch 12, Loss: 4.4536213801644955\n",
            "Epoch 13, Loss: 4.313596374238544\n",
            "Epoch 14, Loss: 4.172308640101987\n",
            "Epoch 15, Loss: 4.070568253012264\n",
            "Epoch 16, Loss: 3.988538762187714\n",
            "Epoch 17, Loss: 3.916870185481313\n",
            "Epoch 18, Loss: 3.8576913044580716\n",
            "Epoch 19, Loss: 3.8061678738850158\n",
            "Epoch 20, Loss: 3.7573906167998645\n",
            "Epoch 21, Loss: 3.7127687784716907\n",
            "Epoch 22, Loss: 3.667753501926237\n",
            "Epoch 23, Loss: 3.6265661204257587\n",
            "Epoch 24, Loss: 3.589822044153043\n",
            "Epoch 25, Loss: 3.55163430618813\n",
            "Epoch 26, Loss: 3.5140727706577466\n",
            "Epoch 27, Loss: 3.47891184253156\n",
            "Epoch 28, Loss: 3.4448013153222514\n",
            "Epoch 29, Loss: 3.4093551519886613\n",
            "Epoch 30, Loss: 3.3751376553264727\n",
            "Epoch 31, Loss: 3.338720920445669\n",
            "Epoch 32, Loss: 3.3042771865034957\n",
            "Epoch 33, Loss: 3.267461213919208\n",
            "Epoch 34, Loss: 3.233067374705049\n",
            "Epoch 35, Loss: 3.2011486983969997\n",
            "Epoch 36, Loss: 3.166727405679805\n",
            "Epoch 37, Loss: 3.1385007282657087\n",
            "Epoch 38, Loss: 3.10773341064258\n",
            "Epoch 39, Loss: 3.0808576635082665\n",
            "Epoch 40, Loss: 3.052986137092571\n",
            "Epoch 41, Loss: 3.022580991925486\n",
            "Epoch 42, Loss: 2.996892951028731\n",
            "Epoch 43, Loss: 2.9685343897251215\n",
            "Epoch 44, Loss: 2.9415469517183426\n",
            "Epoch 45, Loss: 2.913775533056625\n",
            "Epoch 46, Loss: 2.8891141506107263\n",
            "Epoch 47, Loss: 2.8589674414271284\n",
            "Epoch 48, Loss: 2.8375979518646472\n",
            "Epoch 49, Loss: 2.8091493049241087\n",
            "Epoch 50, Loss: 2.7838802203497925\n",
            "Epoch 51, Loss: 2.7602790938618846\n",
            "Epoch 52, Loss: 2.735402337120622\n",
            "Epoch 53, Loss: 2.71318368716618\n",
            "Epoch 54, Loss: 2.6889067086417353\n",
            "Epoch 55, Loss: 2.66220479853013\n",
            "Epoch 56, Loss: 2.642972077250176\n",
            "Epoch 57, Loss: 2.6186358581113693\n",
            "Epoch 58, Loss: 2.597831433996215\n",
            "Epoch 59, Loss: 2.569926638737359\n",
            "Epoch 60, Loss: 2.5520047958549634\n",
            "Epoch 61, Loss: 2.530655049302084\n",
            "Epoch 62, Loss: 2.5068805498235367\n",
            "Epoch 63, Loss: 2.4848024137794513\n",
            "Epoch 64, Loss: 2.4639694361430604\n",
            "Epoch 65, Loss: 2.4434576022350574\n",
            "Epoch 66, Loss: 2.4163462679709315\n",
            "Epoch 67, Loss: 2.394486515723226\n",
            "Epoch 68, Loss: 2.3749493439789013\n",
            "Epoch 69, Loss: 2.3565763890590814\n",
            "Epoch 70, Loss: 2.3371128865215174\n",
            "Epoch 71, Loss: 2.314296561433836\n",
            "Epoch 72, Loss: 2.2931817449877023\n",
            "Epoch 73, Loss: 2.2716658578809263\n",
            "Epoch 74, Loss: 2.254046032800699\n",
            "Epoch 75, Loss: 2.232055077162545\n",
            "Epoch 76, Loss: 2.2104369706814855\n",
            "Epoch 77, Loss: 2.1950417433858225\n",
            "Epoch 78, Loss: 2.1755872071551545\n",
            "Epoch 79, Loss: 2.153560796357177\n",
            "Epoch 80, Loss: 2.131506039053583\n",
            "Epoch 81, Loss: 2.113875389708887\n",
            "Epoch 82, Loss: 2.093450349005287\n",
            "Epoch 83, Loss: 2.0805509328232397\n",
            "Epoch 84, Loss: 2.054943205755385\n",
            "Epoch 85, Loss: 2.037170789430818\n",
            "Epoch 86, Loss: 2.0158644554865024\n",
            "Epoch 87, Loss: 1.9986724569974348\n",
            "Epoch 88, Loss: 1.9803142907369473\n",
            "Epoch 89, Loss: 1.961846050703922\n",
            "Epoch 90, Loss: 1.9443638562546361\n",
            "Epoch 91, Loss: 1.9246111403950645\n",
            "Epoch 92, Loss: 1.907606549884962\n",
            "Epoch 93, Loss: 1.8857977283580223\n",
            "Epoch 94, Loss: 1.8715951122591257\n",
            "Epoch 95, Loss: 1.8489008544351133\n",
            "Epoch 96, Loss: 1.8321297818132678\n",
            "Epoch 97, Loss: 1.8136377907774943\n",
            "Epoch 98, Loss: 1.7971456099654097\n",
            "Epoch 99, Loss: 1.7780921050654652\n",
            "Epoch 100, Loss: 1.7542597592029425\n",
            "Epoch 101, Loss: 1.7414171869492592\n",
            "Epoch 102, Loss: 1.7224286797711306\n",
            "Epoch 103, Loss: 1.7032314648713602\n",
            "Epoch 104, Loss: 1.6873147377882467\n",
            "Epoch 105, Loss: 1.6666668720562439\n",
            "Epoch 106, Loss: 1.653692237251555\n",
            "Epoch 107, Loss: 1.6308801756490527\n",
            "Epoch 108, Loss: 1.6115778894985424\n",
            "Epoch 109, Loss: 1.593024805378731\n",
            "Epoch 110, Loss: 1.5763256915694916\n",
            "Epoch 111, Loss: 1.5570049127349463\n",
            "Epoch 112, Loss: 1.5379560005939221\n",
            "Epoch 113, Loss: 1.5261541269624326\n",
            "Epoch 114, Loss: 1.5014901383758505\n",
            "Epoch 115, Loss: 1.4887228664534782\n",
            "Epoch 116, Loss: 1.4710224638204745\n",
            "Epoch 117, Loss: 1.452360523936084\n",
            "Epoch 118, Loss: 1.4311641677261313\n",
            "Epoch 119, Loss: 1.4121523635161808\n",
            "Epoch 120, Loss: 1.3932457761386472\n",
            "Epoch 121, Loss: 1.3794454755380636\n",
            "Epoch 122, Loss: 1.357341318636599\n",
            "Epoch 123, Loss: 1.3419218729524052\n",
            "Epoch 124, Loss: 1.3209771411803068\n",
            "Epoch 125, Loss: 1.304288667486147\n",
            "Epoch 126, Loss: 1.2908866221032789\n",
            "Epoch 127, Loss: 1.2740561102357362\n",
            "Epoch 128, Loss: 1.2574463277826529\n",
            "Epoch 129, Loss: 1.2322422624244105\n",
            "Epoch 130, Loss: 1.2175957043762402\n",
            "Epoch 131, Loss: 1.198543338519533\n",
            "Epoch 132, Loss: 1.1781361371354984\n",
            "Epoch 133, Loss: 1.1654296941159632\n",
            "Epoch 134, Loss: 1.1462789901991939\n",
            "Epoch 135, Loss: 1.1299475483272388\n",
            "Epoch 136, Loss: 1.1102839205271142\n",
            "Epoch 137, Loss: 1.0900428339343546\n",
            "Epoch 138, Loss: 1.0787231466349434\n",
            "Epoch 139, Loss: 1.0601515282145546\n",
            "Epoch 140, Loss: 1.0464579466053896\n",
            "Epoch 141, Loss: 1.027340529367442\n",
            "Epoch 142, Loss: 1.007620529750424\n",
            "Epoch 143, Loss: 0.9905843472541751\n",
            "Epoch 144, Loss: 0.9748768329315478\n",
            "Epoch 145, Loss: 0.9601017639155278\n",
            "Epoch 146, Loss: 0.9395974298267413\n",
            "Epoch 147, Loss: 0.9303680536387217\n",
            "Epoch 148, Loss: 0.9070737009767986\n",
            "Epoch 149, Loss: 0.8924587794277065\n",
            "Epoch 150, Loss: 0.8772813536017142\n",
            "Epoch 151, Loss: 0.8573398722712037\n",
            "Epoch 152, Loss: 0.8450729531400344\n",
            "Epoch 153, Loss: 0.82127163965074\n",
            "Epoch 154, Loss: 0.8120952285921482\n",
            "Epoch 155, Loss: 0.7913651861955443\n",
            "Epoch 156, Loss: 0.7766052991380472\n",
            "Epoch 157, Loss: 0.7640352336037189\n",
            "Epoch 158, Loss: 0.7462085918392367\n",
            "Epoch 159, Loss: 0.7318076269553445\n",
            "Epoch 160, Loss: 0.7146128294870372\n",
            "Epoch 161, Loss: 0.7073527678199436\n",
            "Epoch 162, Loss: 0.6946298194968182\n",
            "Epoch 163, Loss: 0.6708190197987325\n",
            "Epoch 164, Loss: 0.6543426759102765\n",
            "Epoch 165, Loss: 0.6373751665015355\n",
            "Epoch 166, Loss: 0.6296955776184111\n",
            "Epoch 167, Loss: 0.614320838009305\n",
            "Epoch 168, Loss: 0.6033035607441611\n",
            "Epoch 169, Loss: 0.5820796855575289\n",
            "Epoch 170, Loss: 0.5693373536057484\n",
            "Epoch 171, Loss: 0.5542673168279936\n",
            "Epoch 172, Loss: 0.5479400638119339\n",
            "Epoch 173, Loss: 0.5346398665319623\n",
            "Epoch 174, Loss: 0.5153091215264157\n",
            "Epoch 175, Loss: 0.4928188494709142\n",
            "Epoch 176, Loss: 0.49536542925993193\n",
            "Epoch 177, Loss: 0.4790940235947709\n",
            "Epoch 178, Loss: 0.4710355623222678\n",
            "Epoch 179, Loss: 0.4612337619142459\n",
            "Epoch 180, Loss: 0.43744473254589167\n",
            "Epoch 181, Loss: 0.43108418519082276\n",
            "Epoch 182, Loss: 0.41240663186211113\n",
            "Epoch 183, Loss: 0.4084609738929802\n",
            "Epoch 184, Loss: 0.40159288448903263\n",
            "Epoch 185, Loss: 0.38553858058684315\n",
            "Epoch 186, Loss: 0.3704792479115069\n",
            "Epoch 187, Loss: 0.3616850446252262\n",
            "Epoch 188, Loss: 0.33794847287028035\n",
            "Epoch 189, Loss: 0.3365293046092743\n",
            "Epoch 190, Loss: 0.3248159132726357\n",
            "Epoch 191, Loss: 0.3223991281998432\n",
            "Epoch 192, Loss: 0.31018799718688517\n",
            "Epoch 193, Loss: 0.30263360161000813\n",
            "Epoch 194, Loss: 0.30073044771123725\n",
            "Epoch 195, Loss: 0.2602412630148861\n",
            "Epoch 196, Loss: 0.2629516704384323\n",
            "Epoch 197, Loss: 0.25542550339646963\n",
            "Epoch 198, Loss: 0.27225641142148194\n",
            "Epoch 199, Loss: 0.23307598717605976\n",
            "Accuracy: 32.56%\n",
            "Training with params: {'batch_size': 128, 'lr': 0.005, 'optimizer': 'RMSprop'}\n",
            "Epoch 1, Loss: 4.677277257070517\n",
            "Epoch 2, Loss: 3.62244955657998\n",
            "Epoch 3, Loss: 3.3376359482250555\n",
            "Epoch 4, Loss: 3.125666750056664\n",
            "Epoch 5, Loss: 2.9618646202184964\n",
            "Epoch 6, Loss: 2.827881474629083\n",
            "Epoch 7, Loss: 2.721896861825148\n",
            "Epoch 8, Loss: 2.639917143165608\n",
            "Epoch 9, Loss: 2.5638805624774044\n",
            "Epoch 10, Loss: 2.50309829821672\n",
            "Epoch 11, Loss: 2.444457753235117\n",
            "Epoch 12, Loss: 2.4040458769444615\n",
            "Epoch 13, Loss: 2.348219205046554\n",
            "Epoch 14, Loss: 2.315175948240568\n",
            "Epoch 15, Loss: 2.2622466145269096\n",
            "Epoch 16, Loss: 2.2309545113912326\n",
            "Epoch 17, Loss: 2.1905768923747266\n",
            "Epoch 18, Loss: 2.153612487456378\n",
            "Epoch 19, Loss: 2.1257706459830787\n",
            "Epoch 20, Loss: 2.090363471099483\n",
            "Epoch 21, Loss: 2.0630933305491572\n",
            "Epoch 22, Loss: 2.0295695679267043\n",
            "Epoch 23, Loss: 2.0059567482575127\n",
            "Epoch 24, Loss: 1.9807572392246606\n",
            "Epoch 25, Loss: 1.9661636565957228\n",
            "Epoch 26, Loss: 1.9406220992202954\n",
            "Epoch 27, Loss: 1.9204771311386772\n",
            "Epoch 28, Loss: 1.8905011202063402\n",
            "Epoch 29, Loss: 1.8729144011617012\n",
            "Epoch 30, Loss: 1.8514146612733222\n",
            "Epoch 31, Loss: 1.8393965108925119\n",
            "Epoch 32, Loss: 1.8215416398499629\n",
            "Epoch 33, Loss: 1.8204157681721251\n",
            "Epoch 34, Loss: 1.79279853376891\n",
            "Epoch 35, Loss: 1.7817363690232377\n",
            "Epoch 36, Loss: 1.7711649773370883\n",
            "Epoch 37, Loss: 1.7518392975068153\n",
            "Epoch 38, Loss: 1.7533840377007603\n",
            "Epoch 39, Loss: 1.729715063138996\n",
            "Epoch 40, Loss: 1.733504552365569\n",
            "Epoch 41, Loss: 1.7198084498305455\n",
            "Epoch 42, Loss: 1.7004865587824751\n",
            "Epoch 43, Loss: 1.701198873007694\n",
            "Epoch 44, Loss: 1.6919220597542766\n",
            "Epoch 45, Loss: 1.6808931652237387\n",
            "Epoch 46, Loss: 1.6692579191968875\n",
            "Epoch 47, Loss: 1.659446469353288\n",
            "Epoch 48, Loss: 1.6673455043217105\n",
            "Epoch 49, Loss: 1.6509162352213165\n",
            "Epoch 50, Loss: 1.6457055483937568\n",
            "Epoch 51, Loss: 1.641996585194717\n",
            "Epoch 52, Loss: 1.6188754212216039\n",
            "Epoch 53, Loss: 1.620700237086362\n",
            "Epoch 54, Loss: 1.6097881101891207\n",
            "Epoch 55, Loss: 1.612945331941785\n",
            "Epoch 56, Loss: 1.59963295862193\n",
            "Epoch 57, Loss: 1.6021006275016023\n",
            "Epoch 58, Loss: 1.592409314097041\n",
            "Epoch 59, Loss: 1.590506308219012\n",
            "Epoch 60, Loss: 1.5886993777111669\n",
            "Epoch 61, Loss: 1.5789742363078514\n",
            "Epoch 62, Loss: 1.576176389099082\n",
            "Epoch 63, Loss: 1.5622975589979031\n",
            "Epoch 64, Loss: 1.5627408905712235\n",
            "Epoch 65, Loss: 1.5440802833308345\n",
            "Epoch 66, Loss: 1.5651632919335914\n",
            "Epoch 67, Loss: 1.553189679484843\n",
            "Epoch 68, Loss: 1.5482234691110108\n",
            "Epoch 69, Loss: 1.552617379466591\n",
            "Epoch 70, Loss: 1.5302772900027692\n",
            "Epoch 71, Loss: 1.5355727541477173\n",
            "Epoch 72, Loss: 1.5078432665151709\n",
            "Epoch 73, Loss: 1.5437442911860277\n",
            "Epoch 74, Loss: 1.5227405927370272\n",
            "Epoch 75, Loss: 1.5227598854342994\n",
            "Epoch 76, Loss: 1.5122054043938131\n",
            "Epoch 77, Loss: 1.522814073221153\n",
            "Epoch 78, Loss: 1.5234018838619028\n",
            "Epoch 79, Loss: 1.5230832709680737\n",
            "Epoch 80, Loss: 1.5130550812577348\n",
            "Epoch 81, Loss: 1.5082817152333077\n",
            "Epoch 82, Loss: 1.521344296310259\n",
            "Epoch 83, Loss: 1.5097320829815877\n",
            "Epoch 84, Loss: 1.512120742627117\n",
            "Epoch 85, Loss: 1.4998766833254138\n",
            "Epoch 86, Loss: 1.495837583871144\n",
            "Epoch 87, Loss: 1.5014152662528446\n",
            "Epoch 88, Loss: 1.518813153819355\n",
            "Epoch 89, Loss: 1.511430416875483\n",
            "Epoch 90, Loss: 1.5158013687719165\n",
            "Epoch 91, Loss: 1.4937493084641673\n",
            "Epoch 92, Loss: 1.5022663925309925\n",
            "Epoch 93, Loss: 1.4941905904608919\n",
            "Epoch 94, Loss: 1.5094935817791677\n",
            "Epoch 95, Loss: 1.4849184184428066\n",
            "Epoch 96, Loss: 1.4914917189751744\n",
            "Epoch 97, Loss: 1.4934405785082552\n",
            "Epoch 98, Loss: 1.5044587640201343\n",
            "Epoch 99, Loss: 1.4991357727428836\n",
            "Epoch 100, Loss: 1.4946724950809918\n",
            "Epoch 101, Loss: 1.5097445575782404\n",
            "Epoch 102, Loss: 1.5013286174105867\n",
            "Epoch 103, Loss: 1.4973905638355733\n",
            "Epoch 104, Loss: 1.4889719580750331\n",
            "Epoch 105, Loss: 1.4925277130987944\n",
            "Epoch 106, Loss: 1.5017511176941034\n",
            "Epoch 107, Loss: 1.5126324744175768\n",
            "Epoch 108, Loss: 1.4956045222404364\n",
            "Epoch 109, Loss: 1.492509631549611\n",
            "Epoch 110, Loss: 1.493775059804892\n",
            "Epoch 111, Loss: 1.4921734746154922\n",
            "Epoch 112, Loss: 1.4974645722247755\n",
            "Epoch 113, Loss: 1.483451772528841\n",
            "Epoch 114, Loss: 1.5050734219038884\n",
            "Epoch 115, Loss: 1.483033865461569\n",
            "Epoch 116, Loss: 1.5024350027903877\n",
            "Epoch 117, Loss: 1.4953012560944423\n",
            "Epoch 118, Loss: 1.4866882786726403\n",
            "Epoch 119, Loss: 1.505143876575753\n",
            "Epoch 120, Loss: 1.5026414591028256\n",
            "Epoch 121, Loss: 1.5234887819460896\n",
            "Epoch 122, Loss: 1.4896415432395838\n",
            "Epoch 123, Loss: 1.508762372271789\n",
            "Epoch 124, Loss: 1.509364162412141\n",
            "Epoch 125, Loss: 1.5098364045248007\n",
            "Epoch 126, Loss: 1.5096156040725806\n",
            "Epoch 127, Loss: 1.5131184536477793\n",
            "Epoch 128, Loss: 1.5291296908312746\n",
            "Epoch 129, Loss: 1.4953991528362265\n",
            "Epoch 130, Loss: 1.5094579807327837\n",
            "Epoch 131, Loss: 1.5210890693737722\n",
            "Epoch 132, Loss: 1.5281080440487094\n",
            "Epoch 133, Loss: 1.515266893464891\n",
            "Epoch 134, Loss: 1.5158526902003666\n",
            "Epoch 135, Loss: 1.5308394125660363\n",
            "Epoch 136, Loss: 1.5129300860492774\n",
            "Epoch 137, Loss: 1.519596226532441\n",
            "Epoch 138, Loss: 1.5220107896553585\n",
            "Epoch 139, Loss: 1.5177500674791653\n",
            "Epoch 140, Loss: 1.536773538345571\n",
            "Epoch 141, Loss: 1.5225310343915543\n",
            "Epoch 142, Loss: 1.525856040322872\n",
            "Epoch 143, Loss: 1.4888703794125706\n",
            "Epoch 144, Loss: 1.5304469882374834\n",
            "Epoch 145, Loss: 1.5334645332887655\n",
            "Epoch 146, Loss: 1.550428567640007\n",
            "Epoch 147, Loss: 1.5170895821603059\n",
            "Epoch 148, Loss: 1.541247448500465\n",
            "Epoch 149, Loss: 1.5320931737075376\n",
            "Epoch 150, Loss: 1.5645498607469641\n",
            "Epoch 151, Loss: 1.5110809813679942\n",
            "Epoch 152, Loss: 1.5471007607477096\n",
            "Epoch 153, Loss: 1.5248652582278337\n",
            "Epoch 154, Loss: 1.5417956275403346\n",
            "Epoch 155, Loss: 1.5456687475714233\n",
            "Epoch 156, Loss: 1.5589588254004183\n",
            "Epoch 157, Loss: 1.5669134469593273\n",
            "Epoch 158, Loss: 1.5517979598106326\n",
            "Epoch 159, Loss: 1.5550932292743107\n",
            "Epoch 160, Loss: 1.5685991157046364\n",
            "Epoch 161, Loss: 1.5388377805804963\n",
            "Epoch 162, Loss: 1.5508240058903804\n",
            "Epoch 163, Loss: 1.562506108942544\n",
            "Epoch 164, Loss: 1.5314974129352423\n",
            "Epoch 165, Loss: 1.557575877670132\n",
            "Epoch 166, Loss: 1.5493675857553701\n",
            "Epoch 167, Loss: 1.5953556643727491\n",
            "Epoch 168, Loss: 1.5434705912304656\n",
            "Epoch 169, Loss: 1.5860216163308418\n",
            "Epoch 170, Loss: 1.5356908483273537\n",
            "Epoch 171, Loss: 1.5838137648599533\n",
            "Epoch 172, Loss: 1.5611316958046935\n",
            "Epoch 173, Loss: 1.5781431358183742\n",
            "Epoch 174, Loss: 1.5880512332977237\n",
            "Epoch 175, Loss: 1.5849958360957368\n",
            "Epoch 176, Loss: 1.5681875553887215\n",
            "Epoch 177, Loss: 1.5905930832828707\n",
            "Epoch 178, Loss: 1.5836617823147103\n",
            "Epoch 179, Loss: 1.6144926211108332\n",
            "Epoch 180, Loss: 1.5995438269642004\n",
            "Epoch 181, Loss: 1.5523827228399798\n",
            "Epoch 182, Loss: 1.6046507617701655\n",
            "Epoch 183, Loss: 1.647519516822932\n",
            "Epoch 184, Loss: 1.6198692812639124\n",
            "Epoch 185, Loss: 1.6237293051941621\n",
            "Epoch 186, Loss: 1.5922202606640203\n",
            "Epoch 187, Loss: 1.6123241643466608\n",
            "Epoch 188, Loss: 1.5981064833643492\n",
            "Epoch 189, Loss: 1.5771790343477292\n",
            "Epoch 190, Loss: 1.629610804188282\n",
            "Epoch 191, Loss: 1.6041526568820104\n",
            "Epoch 192, Loss: 1.6114586690807586\n",
            "Epoch 193, Loss: 1.6134995716002287\n",
            "Epoch 194, Loss: 1.5863367076724997\n",
            "Epoch 195, Loss: 1.6860325778536784\n",
            "Epoch 196, Loss: 1.6480261395349527\n",
            "Epoch 197, Loss: 1.6318937765667811\n",
            "Epoch 198, Loss: 1.6325391253546986\n",
            "Epoch 199, Loss: 1.6554581524466006\n",
            "Accuracy: 23.86%\n",
            "Training with params: {'batch_size': 256, 'lr': 0.1, 'optimizer': 'Adam'}\n",
            "Epoch 1, Loss: 71.6280186006001\n",
            "Epoch 2, Loss: 4.617594380767978\n",
            "Epoch 3, Loss: 4.61911941547783\n",
            "Epoch 4, Loss: 4.61963918501017\n",
            "Epoch 5, Loss: 4.618931660846788\n",
            "Epoch 6, Loss: 4.620425472454149\n",
            "Epoch 7, Loss: 4.620002252714975\n",
            "Epoch 8, Loss: 4.619482814049234\n",
            "Epoch 9, Loss: 4.620091535607163\n",
            "Epoch 10, Loss: 4.619802302243758\n",
            "Epoch 11, Loss: 4.619892740736202\n",
            "Epoch 12, Loss: 4.620852399845512\n",
            "Epoch 13, Loss: 4.621151804924011\n",
            "Epoch 14, Loss: 4.621045686760727\n",
            "Epoch 15, Loss: 4.620508412925565\n",
            "Epoch 16, Loss: 4.621655276843479\n",
            "Epoch 17, Loss: 4.620359654329261\n",
            "Epoch 18, Loss: 4.621048438305757\n",
            "Epoch 19, Loss: 4.620679709376121\n",
            "Epoch 20, Loss: 4.620650889922161\n",
            "Epoch 21, Loss: 4.620134450951401\n",
            "Epoch 22, Loss: 4.620995000917084\n",
            "Epoch 23, Loss: 4.620014811048702\n",
            "Epoch 24, Loss: 4.621373108455113\n",
            "Epoch 25, Loss: 4.620434400986652\n",
            "Epoch 26, Loss: 4.622114998953683\n",
            "Epoch 27, Loss: 4.620566733029424\n",
            "Epoch 28, Loss: 4.620712864155672\n",
            "Epoch 29, Loss: 4.6221376535843826\n",
            "Epoch 30, Loss: 4.621241615743053\n",
            "Epoch 31, Loss: 4.621169846885058\n",
            "Epoch 32, Loss: 4.6207133750526275\n",
            "Epoch 33, Loss: 4.621639609336853\n",
            "Epoch 34, Loss: 4.6213399974667295\n",
            "Epoch 35, Loss: 4.622494624585522\n",
            "Epoch 36, Loss: 4.621487549373081\n",
            "Epoch 37, Loss: 4.622421936113007\n",
            "Epoch 38, Loss: 4.621428689178155\n",
            "Epoch 39, Loss: 4.62170761702012\n",
            "Epoch 40, Loss: 4.621260861961209\n",
            "Epoch 41, Loss: 4.621682493054137\n",
            "Epoch 42, Loss: 4.621300427281127\n",
            "Epoch 43, Loss: 4.622283456276874\n",
            "Epoch 44, Loss: 4.62117676832238\n",
            "Epoch 45, Loss: 4.621456817704804\n",
            "Epoch 46, Loss: 4.621278030531747\n",
            "Epoch 47, Loss: 4.620547114586343\n",
            "Epoch 48, Loss: 4.621658298434044\n",
            "Epoch 49, Loss: 4.621537775409465\n",
            "Epoch 50, Loss: 4.621854838059873\n",
            "Epoch 51, Loss: 4.620580631859449\n",
            "Epoch 52, Loss: 4.6206993156549885\n",
            "Epoch 53, Loss: 4.621758967029805\n",
            "Epoch 54, Loss: 4.620892729078021\n",
            "Epoch 55, Loss: 4.62182613538236\n",
            "Epoch 56, Loss: 4.62075302795488\n",
            "Epoch 57, Loss: 4.621306358551492\n",
            "Epoch 58, Loss: 4.621455058759572\n",
            "Epoch 59, Loss: 4.620637949632139\n",
            "Epoch 60, Loss: 4.621262535756948\n",
            "Epoch 61, Loss: 4.62133226832565\n",
            "Epoch 62, Loss: 4.621009018956398\n",
            "Epoch 63, Loss: 4.621447271230269\n",
            "Epoch 64, Loss: 4.6210350236114195\n",
            "Epoch 65, Loss: 4.621126649331074\n",
            "Epoch 66, Loss: 4.6203075063471895\n",
            "Epoch 67, Loss: 4.621845016674119\n",
            "Epoch 68, Loss: 4.620205419404166\n",
            "Epoch 69, Loss: 4.6208668703935585\n",
            "Epoch 70, Loss: 4.621148518153599\n",
            "Epoch 71, Loss: 4.620402182851519\n",
            "Epoch 72, Loss: 4.620181283172296\n",
            "Epoch 73, Loss: 4.622068295673448\n",
            "Epoch 74, Loss: 4.620944122878873\n",
            "Epoch 75, Loss: 4.620261948935839\n",
            "Epoch 76, Loss: 4.622814774513245\n",
            "Epoch 77, Loss: 4.6214384716384265\n",
            "Epoch 78, Loss: 4.6210286057725245\n",
            "Epoch 79, Loss: 4.621278517100276\n",
            "Epoch 80, Loss: 4.621658018657139\n",
            "Epoch 81, Loss: 4.6215307809868635\n",
            "Epoch 82, Loss: 4.6212478754471755\n",
            "Epoch 83, Loss: 4.621183823566048\n",
            "Epoch 84, Loss: 4.621527766694828\n",
            "Epoch 85, Loss: 4.620347665280712\n",
            "Epoch 86, Loss: 4.621060702265526\n",
            "Epoch 87, Loss: 4.621170820022116\n",
            "Epoch 88, Loss: 4.621421200888498\n",
            "Epoch 89, Loss: 4.621998477955254\n",
            "Epoch 90, Loss: 4.620963845934186\n",
            "Epoch 91, Loss: 4.620881002776477\n",
            "Epoch 92, Loss: 4.621244435407678\n",
            "Epoch 93, Loss: 4.621755315332996\n",
            "Epoch 94, Loss: 4.620921733428021\n",
            "Epoch 95, Loss: 4.620909732215258\n",
            "Epoch 96, Loss: 4.620557792332708\n",
            "Epoch 97, Loss: 4.622097093231824\n",
            "Epoch 98, Loss: 4.621249841184032\n",
            "Epoch 99, Loss: 4.622522052453489\n",
            "Epoch 100, Loss: 4.620798745933844\n",
            "Epoch 101, Loss: 4.619951180049351\n",
            "Epoch 102, Loss: 4.61985926238858\n",
            "Epoch 103, Loss: 4.620923081222846\n",
            "Epoch 104, Loss: 4.620998496911963\n",
            "Epoch 105, Loss: 4.622422695159912\n",
            "Epoch 106, Loss: 4.620334304108912\n",
            "Epoch 107, Loss: 4.621061821373141\n",
            "Epoch 108, Loss: 4.621045713521997\n",
            "Epoch 109, Loss: 4.620342991789993\n",
            "Epoch 110, Loss: 4.621138818409978\n",
            "Epoch 111, Loss: 4.621423891612461\n",
            "Epoch 112, Loss: 4.621556642104168\n",
            "Epoch 113, Loss: 4.621083164701656\n",
            "Epoch 114, Loss: 4.619533037652775\n",
            "Epoch 115, Loss: 4.6215358510309335\n",
            "Epoch 116, Loss: 4.62106280424157\n",
            "Epoch 117, Loss: 4.620630366461618\n",
            "Epoch 118, Loss: 4.620869094011735\n",
            "Epoch 119, Loss: 4.621617438841839\n",
            "Epoch 120, Loss: 4.621026861424348\n",
            "Epoch 121, Loss: 4.622554260857251\n",
            "Epoch 122, Loss: 4.620741119190138\n",
            "Epoch 123, Loss: 4.6213648611185505\n",
            "Epoch 124, Loss: 4.620676804562004\n",
            "Epoch 125, Loss: 4.621489057735521\n",
            "Epoch 126, Loss: 4.621471083894068\n",
            "Epoch 127, Loss: 4.621610237627613\n",
            "Epoch 128, Loss: 4.621198931518866\n",
            "Epoch 129, Loss: 4.622161164575694\n",
            "Epoch 130, Loss: 4.620715854119282\n",
            "Epoch 131, Loss: 4.622114485623885\n",
            "Epoch 132, Loss: 4.6208650335973624\n",
            "Epoch 133, Loss: 4.621687008410084\n",
            "Epoch 134, Loss: 4.620867585649296\n",
            "Epoch 135, Loss: 4.620636176089851\n",
            "Epoch 136, Loss: 4.620684915659379\n",
            "Epoch 137, Loss: 4.620825242023079\n",
            "Epoch 138, Loss: 4.621041689600263\n",
            "Epoch 139, Loss: 4.621139390128\n",
            "Epoch 140, Loss: 4.621460766208415\n",
            "Epoch 141, Loss: 4.621401903580646\n",
            "Epoch 142, Loss: 4.621555304040714\n",
            "Epoch 143, Loss: 4.620987522358797\n",
            "Epoch 144, Loss: 4.621408204643094\n",
            "Epoch 145, Loss: 4.62043558578102\n",
            "Epoch 146, Loss: 4.621414809810872\n",
            "Epoch 147, Loss: 4.621497728386704\n",
            "Epoch 148, Loss: 4.620323383078283\n",
            "Epoch 149, Loss: 4.620915519947908\n",
            "Epoch 150, Loss: 4.621654855961702\n",
            "Epoch 151, Loss: 4.620015426557892\n",
            "Epoch 152, Loss: 4.620517360920808\n",
            "Epoch 153, Loss: 4.620921911025534\n",
            "Epoch 154, Loss: 4.621075039007226\n",
            "Epoch 155, Loss: 4.622366620569813\n",
            "Epoch 156, Loss: 4.621664760064106\n",
            "Epoch 157, Loss: 4.620719659085176\n",
            "Epoch 158, Loss: 4.621796807464288\n",
            "Epoch 159, Loss: 4.621765645182863\n",
            "Epoch 160, Loss: 4.621627861139726\n",
            "Epoch 161, Loss: 4.6209895099912375\n",
            "Epoch 162, Loss: 4.621362839426313\n",
            "Epoch 163, Loss: 4.61982308115278\n",
            "Epoch 164, Loss: 4.622078484418441\n",
            "Epoch 165, Loss: 4.621364771103372\n",
            "Epoch 166, Loss: 4.620279338895058\n",
            "Epoch 167, Loss: 4.621112699411353\n",
            "Epoch 168, Loss: 4.621221934046064\n",
            "Epoch 169, Loss: 4.621057055434402\n",
            "Epoch 170, Loss: 4.621787251258383\n",
            "Epoch 171, Loss: 4.6217072958848915\n",
            "Epoch 172, Loss: 4.620869797103259\n",
            "Epoch 173, Loss: 4.620507902028609\n",
            "Epoch 174, Loss: 4.621146435640296\n",
            "Epoch 175, Loss: 4.622077873774937\n",
            "Epoch 176, Loss: 4.620647123881748\n",
            "Epoch 177, Loss: 4.621289569504407\n",
            "Epoch 178, Loss: 4.620970295400036\n",
            "Epoch 179, Loss: 4.6214524020954055\n",
            "Epoch 180, Loss: 4.621874064815287\n",
            "Epoch 181, Loss: 4.62145999743014\n",
            "Epoch 182, Loss: 4.621868449814466\n",
            "Epoch 183, Loss: 4.621551628015479\n",
            "Epoch 184, Loss: 4.621391709969968\n",
            "Epoch 185, Loss: 4.620423543209932\n",
            "Epoch 186, Loss: 4.6219108493960634\n",
            "Epoch 187, Loss: 4.621104503164486\n",
            "Epoch 188, Loss: 4.621640635996449\n",
            "Epoch 189, Loss: 4.6214088542120795\n",
            "Epoch 190, Loss: 4.6217294225887375\n",
            "Epoch 191, Loss: 4.621643910602647\n",
            "Epoch 192, Loss: 4.619914911231216\n",
            "Epoch 193, Loss: 4.620328521241947\n",
            "Epoch 194, Loss: 4.621289963624915\n",
            "Epoch 195, Loss: 4.621163482568702\n",
            "Epoch 196, Loss: 4.621380144236039\n",
            "Epoch 197, Loss: 4.620645768788396\n",
            "Epoch 198, Loss: 4.620477510958302\n",
            "Epoch 199, Loss: 4.621382727914927\n",
            "Accuracy: 1.00%\n",
            "Training with params: {'batch_size': 256, 'lr': 0.1, 'optimizer': 'SGD'}\n",
            "Epoch 1, Loss: 4.5899088796304195\n",
            "Epoch 2, Loss: 4.27679231702065\n",
            "Epoch 3, Loss: 3.9693243187301013\n",
            "Epoch 4, Loss: 3.6792547520326107\n",
            "Epoch 5, Loss: 3.478540075068571\n",
            "Epoch 6, Loss: 3.2960488625935147\n",
            "Epoch 7, Loss: 3.1177360050532283\n",
            "Epoch 8, Loss: 2.960754039336224\n",
            "Epoch 9, Loss: 2.827840736934117\n",
            "Epoch 10, Loss: 2.7039183001128997\n",
            "Epoch 11, Loss: 2.581379866113468\n",
            "Epoch 12, Loss: 2.4785312839916775\n",
            "Epoch 13, Loss: 2.370147413137008\n",
            "Epoch 14, Loss: 2.267411637062929\n",
            "Epoch 15, Loss: 2.1719225420027364\n",
            "Epoch 16, Loss: 2.0716166557097924\n",
            "Epoch 17, Loss: 1.9847746290722672\n",
            "Epoch 18, Loss: 1.8918064090670372\n",
            "Epoch 19, Loss: 1.7966546112177324\n",
            "Epoch 20, Loss: 1.7087812569676613\n",
            "Epoch 21, Loss: 1.617525078812424\n",
            "Epoch 22, Loss: 1.5339085496201808\n",
            "Epoch 23, Loss: 1.4451366754210726\n",
            "Epoch 24, Loss: 1.373555722893501\n",
            "Epoch 25, Loss: 1.2929323720080512\n",
            "Epoch 26, Loss: 1.2016968927821334\n",
            "Epoch 27, Loss: 1.125424185577704\n",
            "Epoch 28, Loss: 1.0645385369354365\n",
            "Epoch 29, Loss: 0.9722786107841803\n",
            "Epoch 30, Loss: 0.8947143439127474\n",
            "Epoch 31, Loss: 0.8369644796665834\n",
            "Epoch 32, Loss: 0.764504537442509\n",
            "Epoch 33, Loss: 0.6972808139968891\n",
            "Epoch 34, Loss: 0.6464876451966713\n",
            "Epoch 35, Loss: 0.5725839883089066\n",
            "Epoch 36, Loss: 0.5735766778947139\n",
            "Epoch 37, Loss: 0.5291784401146733\n",
            "Epoch 38, Loss: 0.4482007057082896\n",
            "Epoch 39, Loss: 0.4279174183552362\n",
            "Epoch 40, Loss: 0.4239059408404389\n",
            "Epoch 41, Loss: 0.3414220235937712\n",
            "Epoch 42, Loss: 0.34899256446835947\n",
            "Epoch 43, Loss: 0.2848121758018221\n",
            "Epoch 44, Loss: 0.3094758362110172\n",
            "Epoch 45, Loss: 0.265457583819421\n",
            "Epoch 46, Loss: 0.23322262516131206\n",
            "Epoch 47, Loss: 0.24676415601707236\n",
            "Epoch 48, Loss: 0.1901699637956157\n",
            "Epoch 49, Loss: 0.2488211958992238\n",
            "Epoch 50, Loss: 0.17517115304968794\n",
            "Epoch 51, Loss: 0.09276947888488672\n",
            "Epoch 52, Loss: 0.07765277638575252\n",
            "Epoch 53, Loss: 0.02975622886715799\n",
            "Epoch 54, Loss: 0.01678582109815004\n",
            "Epoch 55, Loss: 0.01097976297320684\n",
            "Epoch 56, Loss: 0.010132479672058846\n",
            "Epoch 57, Loss: 0.008325100340642872\n",
            "Epoch 58, Loss: 0.006554847269566084\n",
            "Epoch 59, Loss: 0.00786742300973559\n",
            "Epoch 60, Loss: 0.006852059021036198\n",
            "Epoch 61, Loss: 0.00691096023989043\n",
            "Epoch 62, Loss: 0.00588479091333729\n",
            "Epoch 63, Loss: 0.0066164140984515795\n",
            "Epoch 64, Loss: 0.004888127867384262\n",
            "Epoch 65, Loss: 0.0056949444100313\n",
            "Epoch 66, Loss: 0.006822847859216474\n",
            "Epoch 67, Loss: 0.005718210250571636\n",
            "Epoch 68, Loss: 0.006521237051596732\n",
            "Epoch 69, Loss: 0.006007740685979987\n",
            "Epoch 70, Loss: 0.0056305007194762825\n",
            "Epoch 71, Loss: 0.0053902361749870965\n",
            "Epoch 72, Loss: 0.005597366409718382\n",
            "Epoch 73, Loss: 0.004989756932612794\n",
            "Epoch 74, Loss: 0.005207110028856491\n",
            "Epoch 75, Loss: 0.004838422895884331\n",
            "Epoch 76, Loss: 0.0049418492613322275\n",
            "Epoch 77, Loss: 0.004866672134827537\n",
            "Epoch 78, Loss: 0.005257505093753452\n",
            "Epoch 79, Loss: 0.004612547380502849\n",
            "Epoch 80, Loss: 0.004526499979262126\n",
            "Epoch 81, Loss: 0.004161005142549937\n",
            "Epoch 82, Loss: 0.004617224347050663\n",
            "Epoch 83, Loss: 0.004566676771697084\n",
            "Epoch 84, Loss: 0.003382299545165199\n",
            "Epoch 85, Loss: 0.004034781858814424\n",
            "Epoch 86, Loss: 0.004170632023098213\n",
            "Epoch 87, Loss: 0.0036898514779156303\n",
            "Epoch 88, Loss: 0.004542627858000389\n",
            "Epoch 89, Loss: 0.003170589422533878\n",
            "Epoch 90, Loss: 0.00321548410595694\n",
            "Epoch 91, Loss: 0.003878874518630589\n",
            "Epoch 92, Loss: 0.003620148395106187\n",
            "Epoch 93, Loss: 0.00314888363105378\n",
            "Epoch 94, Loss: 0.0035563895104058583\n",
            "Epoch 95, Loss: 0.003177335650787204\n",
            "Epoch 96, Loss: 0.0032610487852457017\n",
            "Epoch 97, Loss: 0.0028361542066036513\n",
            "Epoch 98, Loss: 0.0031075858074770197\n",
            "Epoch 99, Loss: 0.002788439522674355\n",
            "Epoch 100, Loss: 0.0033929579772770274\n",
            "Epoch 101, Loss: 0.003103108180785428\n",
            "Epoch 102, Loss: 0.0031233669400609534\n",
            "Epoch 103, Loss: 0.0024712651692406864\n",
            "Epoch 104, Loss: 0.002685267740904534\n",
            "Epoch 105, Loss: 0.0025225953887661025\n",
            "Epoch 106, Loss: 0.002785521261548452\n",
            "Epoch 107, Loss: 0.0024405304708979\n",
            "Epoch 108, Loss: 0.0021467537913775567\n",
            "Epoch 109, Loss: 0.002347870018453176\n",
            "Epoch 110, Loss: 0.0022721057949285973\n",
            "Epoch 111, Loss: 0.0025665978179432035\n",
            "Epoch 112, Loss: 0.0024294773314495056\n",
            "Epoch 113, Loss: 0.002031211104548099\n",
            "Epoch 114, Loss: 0.0022308986651240754\n",
            "Epoch 115, Loss: 0.002097151461366222\n",
            "Epoch 116, Loss: 0.002140345012980551\n",
            "Epoch 117, Loss: 0.0021796914685294224\n",
            "Epoch 118, Loss: 0.0018847678757269097\n",
            "Epoch 119, Loss: 0.00207259671596872\n",
            "Epoch 120, Loss: 0.0022579849362361474\n",
            "Epoch 121, Loss: 0.002128451130541792\n",
            "Epoch 122, Loss: 0.001973785029822837\n",
            "Epoch 123, Loss: 0.0020773377892714497\n",
            "Epoch 124, Loss: 0.002319710757194339\n",
            "Epoch 125, Loss: 0.0019797281193135933\n",
            "Epoch 126, Loss: 0.0019499734371493105\n",
            "Epoch 127, Loss: 0.001791595852558444\n",
            "Epoch 128, Loss: 0.00191787825924064\n",
            "Epoch 129, Loss: 0.0017665153659900356\n",
            "Epoch 130, Loss: 0.0016765868154203529\n",
            "Epoch 131, Loss: 0.001566646273389853\n",
            "Epoch 132, Loss: 0.0016826502505864482\n",
            "Epoch 133, Loss: 0.0015352704120937697\n",
            "Epoch 134, Loss: 0.0012933116704516578\n",
            "Epoch 135, Loss: 0.0013340956115547558\n",
            "Epoch 136, Loss: 0.0014846330664206144\n",
            "Epoch 137, Loss: 0.0013201847587526795\n",
            "Epoch 138, Loss: 0.0014981921032107938\n",
            "Epoch 139, Loss: 0.0015995856847439427\n",
            "Epoch 140, Loss: 0.0017194220857404895\n",
            "Epoch 141, Loss: 0.0015218948499342352\n",
            "Epoch 142, Loss: 0.0011994830456387717\n",
            "Epoch 143, Loss: 0.0012216460875476645\n",
            "Epoch 144, Loss: 0.0012422645866499065\n",
            "Epoch 145, Loss: 0.0010682028873909113\n",
            "Epoch 146, Loss: 0.0013681843535165654\n",
            "Epoch 147, Loss: 0.0011710367615872814\n",
            "Epoch 148, Loss: 0.001255489050853779\n",
            "Epoch 149, Loss: 0.001233338704080278\n",
            "Epoch 150, Loss: 0.0012139423950949485\n",
            "Epoch 151, Loss: 0.0011196360319445375\n",
            "Epoch 152, Loss: 0.0013685328673573723\n",
            "Epoch 153, Loss: 0.0014270140826719937\n",
            "Epoch 154, Loss: 0.001039622655409811\n",
            "Epoch 155, Loss: 0.0010529814807137437\n",
            "Epoch 156, Loss: 0.0010719251292116217\n",
            "Epoch 157, Loss: 0.0010481599647948537\n",
            "Epoch 158, Loss: 0.0008427822700765802\n",
            "Epoch 159, Loss: 0.001042827696400476\n",
            "Epoch 160, Loss: 0.0011731239021025189\n",
            "Epoch 161, Loss: 0.0009107012340290548\n",
            "Epoch 162, Loss: 0.0011618300484361518\n",
            "Epoch 163, Loss: 0.0009539337737880153\n",
            "Epoch 164, Loss: 0.001390390651735919\n",
            "Epoch 165, Loss: 0.0011801333988545111\n",
            "Epoch 166, Loss: 0.0010032232461600536\n",
            "Epoch 167, Loss: 0.0009676102023632314\n",
            "Epoch 168, Loss: 0.0009318764778629197\n",
            "Epoch 169, Loss: 0.0013132572886225178\n",
            "Epoch 170, Loss: 0.0010564976596191516\n",
            "Epoch 171, Loss: 0.0009749801570344099\n",
            "Epoch 172, Loss: 0.0010474498521201598\n",
            "Epoch 173, Loss: 0.0012802235360883357\n",
            "Epoch 174, Loss: 0.0010508238372457695\n",
            "Epoch 175, Loss: 0.0009232279373129726\n",
            "Epoch 176, Loss: 0.0009642984744515365\n",
            "Epoch 177, Loss: 0.0008920770445799964\n",
            "Epoch 178, Loss: 0.0007865392518786914\n",
            "Epoch 179, Loss: 0.000714288346768756\n",
            "Epoch 180, Loss: 0.0009111856756766317\n",
            "Epoch 181, Loss: 0.0008868892822350489\n",
            "Epoch 182, Loss: 0.0009953707467873904\n",
            "Epoch 183, Loss: 0.0007598655820587811\n",
            "Epoch 184, Loss: 0.0008675139456778308\n",
            "Epoch 185, Loss: 0.0009309253766369526\n",
            "Epoch 186, Loss: 0.0008397039930096813\n",
            "Epoch 187, Loss: 0.0007371574340549141\n",
            "Epoch 188, Loss: 0.0009208932725804127\n",
            "Epoch 189, Loss: 0.0009086621667680744\n",
            "Epoch 190, Loss: 0.000836776882537453\n",
            "Epoch 191, Loss: 0.0008670317888294337\n",
            "Epoch 192, Loss: 0.0006357621322775008\n",
            "Epoch 193, Loss: 0.0007747699133799368\n",
            "Epoch 194, Loss: 0.0008892611016080434\n",
            "Epoch 195, Loss: 0.0008877294385827702\n",
            "Epoch 196, Loss: 0.0008892922033199432\n",
            "Epoch 197, Loss: 0.0008168545573425945\n",
            "Epoch 198, Loss: 0.0008427850158967681\n",
            "Epoch 199, Loss: 0.0008789439148780989\n",
            "Accuracy: 36.68%\n",
            "Training with params: {'batch_size': 256, 'lr': 0.1, 'optimizer': 'RMSprop'}\n",
            "Epoch 1, Loss: 3969334.684009564\n",
            "Epoch 2, Loss: 4.643931279377061\n",
            "Epoch 3, Loss: 4.634501946215727\n",
            "Epoch 4, Loss: 4.61643753003101\n",
            "Epoch 5, Loss: 4.62138096410401\n",
            "Epoch 6, Loss: 4.632185371554628\n",
            "Epoch 7, Loss: 4.67036375220941\n",
            "Epoch 8, Loss: 4.622204325637039\n",
            "Epoch 9, Loss: 4.622649674512902\n",
            "Epoch 10, Loss: 4.6223437859087575\n",
            "Epoch 11, Loss: 4.621901857609651\n",
            "Epoch 12, Loss: 4.62288083835524\n",
            "Epoch 13, Loss: 4.6225044873296\n",
            "Epoch 14, Loss: 4.621767703367739\n",
            "Epoch 15, Loss: 4.622365635268542\n",
            "Epoch 16, Loss: 4.6221756326908965\n",
            "Epoch 17, Loss: 4.6220318760190695\n",
            "Epoch 18, Loss: 4.621613855264624\n",
            "Epoch 19, Loss: 4.622447629364169\n",
            "Epoch 20, Loss: 4.622329011255381\n",
            "Epoch 21, Loss: 4.6226203246992466\n",
            "Epoch 22, Loss: 4.622113247306979\n",
            "Epoch 23, Loss: 4.622389662022493\n",
            "Epoch 24, Loss: 4.6221646703019434\n",
            "Epoch 25, Loss: 4.621745204438969\n",
            "Epoch 26, Loss: 4.622419646808079\n",
            "Epoch 27, Loss: 4.622449860280874\n",
            "Epoch 28, Loss: 4.621995709380325\n",
            "Epoch 29, Loss: 4.622109126071541\n",
            "Epoch 30, Loss: 4.622598215025299\n",
            "Epoch 31, Loss: 4.622924787657602\n",
            "Epoch 32, Loss: 4.622492247698259\n",
            "Epoch 33, Loss: 4.622596495005549\n",
            "Epoch 34, Loss: 4.621933426175799\n",
            "Epoch 35, Loss: 4.621882820615963\n",
            "Epoch 36, Loss: 4.623086666574284\n",
            "Epoch 37, Loss: 4.621992768073569\n",
            "Epoch 38, Loss: 4.622742954565554\n",
            "Epoch 39, Loss: 4.6221408016827645\n",
            "Epoch 40, Loss: 4.621906409458238\n",
            "Epoch 41, Loss: 4.621807672539536\n",
            "Epoch 42, Loss: 4.622593989177626\n",
            "Epoch 43, Loss: 4.622876400850257\n",
            "Epoch 44, Loss: 4.6223565777953795\n",
            "Epoch 45, Loss: 4.623188349665428\n",
            "Epoch 46, Loss: 4.622437766620091\n",
            "Epoch 47, Loss: 4.621952159064157\n",
            "Epoch 48, Loss: 4.622195786359359\n",
            "Epoch 49, Loss: 4.622955572848418\n",
            "Epoch 50, Loss: 4.621751413053396\n",
            "Epoch 51, Loss: 4.6216844563581505\n",
            "Epoch 52, Loss: 4.621941534840331\n",
            "Epoch 53, Loss: 4.622311387743268\n",
            "Epoch 54, Loss: 4.621857679620081\n",
            "Epoch 55, Loss: 4.621950383089026\n",
            "Epoch 56, Loss: 4.6227142397238286\n",
            "Epoch 57, Loss: 4.622642470865833\n",
            "Epoch 58, Loss: 4.622514345207993\n",
            "Epoch 59, Loss: 4.621590981678087\n",
            "Epoch 60, Loss: 4.6220207044056485\n",
            "Epoch 61, Loss: 4.621520400047302\n",
            "Epoch 62, Loss: 4.622030654732062\n",
            "Epoch 63, Loss: 4.621780142492177\n",
            "Epoch 64, Loss: 4.622352259499686\n",
            "Epoch 65, Loss: 4.6220681545685744\n",
            "Epoch 66, Loss: 4.6224304340323625\n",
            "Epoch 67, Loss: 4.622832509936119\n",
            "Epoch 68, Loss: 4.621549088127759\n",
            "Epoch 69, Loss: 4.622050168562908\n",
            "Epoch 70, Loss: 4.6219377225759075\n",
            "Epoch 71, Loss: 4.622589232970257\n",
            "Epoch 72, Loss: 4.622113395710381\n",
            "Epoch 73, Loss: 4.621644024946252\n",
            "Epoch 74, Loss: 4.623099799058875\n",
            "Epoch 75, Loss: 4.621354017938886\n",
            "Epoch 76, Loss: 4.622368817426721\n",
            "Epoch 77, Loss: 4.623107177870614\n",
            "Epoch 78, Loss: 4.622207176928618\n",
            "Epoch 79, Loss: 4.622149834827501\n",
            "Epoch 80, Loss: 4.621728464048736\n",
            "Epoch 81, Loss: 4.622471356878475\n",
            "Epoch 82, Loss: 4.62251108519885\n",
            "Epoch 83, Loss: 4.622476760221987\n",
            "Epoch 84, Loss: 4.621944179340285\n",
            "Epoch 85, Loss: 4.621848342370014\n",
            "Epoch 86, Loss: 4.622173027116425\n",
            "Epoch 87, Loss: 4.621589962317019\n",
            "Epoch 88, Loss: 4.6222904920578\n",
            "Epoch 89, Loss: 4.621297593019446\n",
            "Epoch 90, Loss: 4.6222134609611665\n",
            "Epoch 91, Loss: 4.622022071663214\n",
            "Epoch 92, Loss: 4.622550319652168\n",
            "Epoch 93, Loss: 4.621971821298405\n",
            "Epoch 94, Loss: 4.622978889212316\n",
            "Epoch 95, Loss: 4.622441321003194\n",
            "Epoch 96, Loss: 4.621964824442961\n",
            "Epoch 97, Loss: 4.621899349348886\n",
            "Epoch 98, Loss: 4.621848751087578\n",
            "Epoch 99, Loss: 4.622021076630573\n",
            "Epoch 100, Loss: 4.622670747795883\n",
            "Epoch 101, Loss: 4.622505363152952\n",
            "Epoch 102, Loss: 4.622448008887622\n",
            "Epoch 103, Loss: 4.622484474766011\n",
            "Epoch 104, Loss: 4.622289316994803\n",
            "Epoch 105, Loss: 4.62306729384831\n",
            "Epoch 106, Loss: 4.622504000761071\n",
            "Epoch 107, Loss: 4.622714110783169\n",
            "Epoch 108, Loss: 4.622961173252183\n",
            "Epoch 109, Loss: 4.622865438461304\n",
            "Epoch 110, Loss: 4.621674540091534\n",
            "Epoch 111, Loss: 4.622562593343306\n",
            "Epoch 112, Loss: 4.621746131352016\n",
            "Epoch 113, Loss: 4.622473955154419\n",
            "Epoch 114, Loss: 4.622945612790633\n",
            "Epoch 115, Loss: 4.622169200254946\n",
            "Epoch 116, Loss: 4.622669983883293\n",
            "Epoch 117, Loss: 4.622467323225372\n",
            "Epoch 118, Loss: 4.62186971489264\n",
            "Epoch 119, Loss: 4.622006647440852\n",
            "Epoch 120, Loss: 4.621526744900917\n",
            "Epoch 121, Loss: 4.622380536429736\n",
            "Epoch 122, Loss: 4.621961260328487\n",
            "Epoch 123, Loss: 4.6222379718508035\n",
            "Epoch 124, Loss: 4.623023702173817\n",
            "Epoch 125, Loss: 4.623094103774246\n",
            "Epoch 126, Loss: 4.622045278549194\n",
            "Epoch 127, Loss: 4.6222595219709435\n",
            "Epoch 128, Loss: 4.622140546234286\n",
            "Epoch 129, Loss: 4.62241282025162\n",
            "Epoch 130, Loss: 4.6218458243778775\n",
            "Epoch 131, Loss: 4.62235909092183\n",
            "Epoch 132, Loss: 4.623222874135387\n",
            "Epoch 133, Loss: 4.621903910928843\n",
            "Epoch 134, Loss: 4.622475220232594\n",
            "Epoch 135, Loss: 4.622293241169988\n",
            "Epoch 136, Loss: 4.622061174743029\n",
            "Epoch 137, Loss: 4.6233090186605645\n",
            "Epoch 138, Loss: 4.622391255534425\n",
            "Epoch 139, Loss: 4.6226361916989696\n",
            "Epoch 140, Loss: 4.6222849403108865\n",
            "Epoch 141, Loss: 4.621983647346497\n",
            "Epoch 142, Loss: 4.622265431345726\n",
            "Epoch 143, Loss: 4.622722720613285\n",
            "Epoch 144, Loss: 4.622048939977374\n",
            "Epoch 145, Loss: 4.621756334694064\n",
            "Epoch 146, Loss: 4.622484141466569\n",
            "Epoch 147, Loss: 4.623179258132468\n",
            "Epoch 148, Loss: 4.623023320217522\n",
            "Epoch 149, Loss: 4.623301613087556\n",
            "Epoch 150, Loss: 4.622140665443576\n",
            "Epoch 151, Loss: 4.622409847317909\n",
            "Epoch 152, Loss: 4.622770744927076\n",
            "Epoch 153, Loss: 4.622344491433124\n",
            "Epoch 154, Loss: 4.6223932820923475\n",
            "Epoch 155, Loss: 4.622500867259745\n",
            "Epoch 156, Loss: 4.622722555179985\n",
            "Epoch 157, Loss: 4.62232244987877\n",
            "Epoch 158, Loss: 4.6221676116087\n",
            "Epoch 159, Loss: 4.622134831486916\n",
            "Epoch 160, Loss: 4.623410317362572\n",
            "Epoch 161, Loss: 4.622651825145799\n",
            "Epoch 162, Loss: 4.621847352203058\n",
            "Epoch 163, Loss: 4.621482819926982\n",
            "Epoch 164, Loss: 4.622642393014869\n",
            "Epoch 165, Loss: 4.6225618099679755\n",
            "Epoch 166, Loss: 4.621962493779708\n",
            "Epoch 167, Loss: 4.6219071660723\n",
            "Epoch 168, Loss: 4.622238317314459\n",
            "Epoch 169, Loss: 4.621827402893378\n",
            "Epoch 170, Loss: 4.622011583678576\n",
            "Epoch 171, Loss: 4.622812991239587\n",
            "Epoch 172, Loss: 4.62228355359058\n",
            "Epoch 173, Loss: 4.621994726511897\n",
            "Epoch 174, Loss: 4.622658705224796\n",
            "Epoch 175, Loss: 4.621956793629393\n",
            "Epoch 176, Loss: 4.622060442457394\n",
            "Epoch 177, Loss: 4.622205580983843\n",
            "Epoch 178, Loss: 4.622126270313652\n",
            "Epoch 179, Loss: 4.622968442586004\n",
            "Epoch 180, Loss: 4.622540624774232\n",
            "Epoch 181, Loss: 4.62244053519502\n",
            "Epoch 182, Loss: 4.62260653534714\n",
            "Epoch 183, Loss: 4.622492593161914\n",
            "Epoch 184, Loss: 4.62188040723606\n",
            "Epoch 185, Loss: 4.621868024067003\n",
            "Epoch 186, Loss: 4.622546794463177\n",
            "Epoch 187, Loss: 4.622633347705919\n",
            "Epoch 188, Loss: 4.621927935249952\n",
            "Epoch 189, Loss: 4.622499341867408\n",
            "Epoch 190, Loss: 4.622621473000974\n",
            "Epoch 191, Loss: 4.622472371373858\n",
            "Epoch 192, Loss: 4.621873507694322\n",
            "Epoch 193, Loss: 4.622347138365921\n",
            "Epoch 194, Loss: 4.622209887115323\n",
            "Epoch 195, Loss: 4.62181083280213\n",
            "Epoch 196, Loss: 4.622032749409578\n",
            "Epoch 197, Loss: 4.622626010252505\n",
            "Epoch 198, Loss: 4.6220932347433905\n",
            "Epoch 199, Loss: 4.622035330655623\n",
            "Accuracy: 1.00%\n",
            "Training with params: {'batch_size': 256, 'lr': 0.01, 'optimizer': 'Adam'}\n",
            "Epoch 1, Loss: 3.9910541079482256\n",
            "Epoch 2, Loss: 3.446050373875365\n",
            "Epoch 3, Loss: 3.237650623126906\n",
            "Epoch 4, Loss: 3.1183597932056504\n",
            "Epoch 5, Loss: 3.0458566242334792\n",
            "Epoch 6, Loss: 2.9800000908423443\n",
            "Epoch 7, Loss: 2.9269161005409394\n",
            "Epoch 8, Loss: 2.8881263720745944\n",
            "Epoch 9, Loss: 2.8533563978817997\n",
            "Epoch 10, Loss: 2.8359327669046364\n",
            "Epoch 11, Loss: 2.8103267623453725\n",
            "Epoch 12, Loss: 2.795729527668077\n",
            "Epoch 13, Loss: 2.7733094740887076\n",
            "Epoch 14, Loss: 2.7402989329123986\n",
            "Epoch 15, Loss: 2.7434245506111457\n",
            "Epoch 16, Loss: 2.7197458172331053\n",
            "Epoch 17, Loss: 2.7210312643829657\n",
            "Epoch 18, Loss: 2.6950163366843243\n",
            "Epoch 19, Loss: 2.6821806661936702\n",
            "Epoch 20, Loss: 2.6812988008771623\n",
            "Epoch 21, Loss: 2.6706455714848576\n",
            "Epoch 22, Loss: 2.655834819589342\n",
            "Epoch 23, Loss: 2.653352390746681\n",
            "Epoch 24, Loss: 2.65756734901545\n",
            "Epoch 25, Loss: 2.6426919139161402\n",
            "Epoch 26, Loss: 2.626034031108934\n",
            "Epoch 27, Loss: 2.634668290615082\n",
            "Epoch 28, Loss: 2.6178111312340717\n",
            "Epoch 29, Loss: 2.617487046183372\n",
            "Epoch 30, Loss: 2.6128705764303404\n",
            "Epoch 31, Loss: 2.5980650101389204\n",
            "Epoch 32, Loss: 2.623337995032875\n",
            "Epoch 33, Loss: 2.597182123028502\n",
            "Epoch 34, Loss: 2.6016091229964275\n",
            "Epoch 35, Loss: 2.594916902026352\n",
            "Epoch 36, Loss: 2.593681944876301\n",
            "Epoch 37, Loss: 2.6017797808257903\n",
            "Epoch 38, Loss: 2.5772049329718767\n",
            "Epoch 39, Loss: 2.574459943236137\n",
            "Epoch 40, Loss: 2.5811363495126063\n",
            "Epoch 41, Loss: 2.571777528646041\n",
            "Epoch 42, Loss: 2.5739302878477135\n",
            "Epoch 43, Loss: 2.576183629279234\n",
            "Epoch 44, Loss: 2.572952681658219\n",
            "Epoch 45, Loss: 2.5755049634952933\n",
            "Epoch 46, Loss: 2.5707661095930607\n",
            "Epoch 47, Loss: 2.5584247355558434\n",
            "Epoch 48, Loss: 2.5634962721746795\n",
            "Epoch 49, Loss: 2.5551791531699046\n",
            "Epoch 50, Loss: 2.55727675982884\n",
            "Epoch 51, Loss: 2.5415100820210514\n",
            "Epoch 52, Loss: 2.5511956908264937\n",
            "Epoch 53, Loss: 2.5520892654146468\n",
            "Epoch 54, Loss: 2.548175424945598\n",
            "Epoch 55, Loss: 2.543485849487538\n",
            "Epoch 56, Loss: 2.5481729458789437\n",
            "Epoch 57, Loss: 2.5384368300437927\n",
            "Epoch 58, Loss: 2.5368128005339177\n",
            "Epoch 59, Loss: 2.5372598633474235\n",
            "Epoch 60, Loss: 2.5440750012592392\n",
            "Epoch 61, Loss: 2.533416373389108\n",
            "Epoch 62, Loss: 2.531975806975851\n",
            "Epoch 63, Loss: 2.5319208770382162\n",
            "Epoch 64, Loss: 2.53068521558022\n",
            "Epoch 65, Loss: 2.517003676112817\n",
            "Epoch 66, Loss: 2.5443665750172673\n",
            "Epoch 67, Loss: 2.542667866969595\n",
            "Epoch 68, Loss: 2.522204430735841\n",
            "Epoch 69, Loss: 2.527369990640757\n",
            "Epoch 70, Loss: 2.5212700184510677\n",
            "Epoch 71, Loss: 2.5072127836091176\n",
            "Epoch 72, Loss: 2.5145445453877353\n",
            "Epoch 73, Loss: 2.5133994051388333\n",
            "Epoch 74, Loss: 2.5026920389155953\n",
            "Epoch 75, Loss: 2.5272227951458524\n",
            "Epoch 76, Loss: 2.516806428529778\n",
            "Epoch 77, Loss: 2.5277648580317593\n",
            "Epoch 78, Loss: 2.5119421798355726\n",
            "Epoch 79, Loss: 2.4992582931810494\n",
            "Epoch 80, Loss: 2.519899988661007\n",
            "Epoch 81, Loss: 2.5190392501500187\n",
            "Epoch 82, Loss: 2.5227959873724957\n",
            "Epoch 83, Loss: 2.5039047209583982\n",
            "Epoch 84, Loss: 2.5009457797420267\n",
            "Epoch 85, Loss: 2.5088684157449372\n",
            "Epoch 86, Loss: 2.5171680572081585\n",
            "Epoch 87, Loss: 2.506515234100575\n",
            "Epoch 88, Loss: 2.5130119056117777\n",
            "Epoch 89, Loss: 2.501945361799123\n",
            "Epoch 90, Loss: 2.50207573783641\n",
            "Epoch 91, Loss: 2.5047963590038065\n",
            "Epoch 92, Loss: 2.5259998148801377\n",
            "Epoch 93, Loss: 2.496808938834132\n",
            "Epoch 94, Loss: 2.50371543363649\n",
            "Epoch 95, Loss: 2.503952610249422\n",
            "Epoch 96, Loss: 2.5041446576313096\n",
            "Epoch 97, Loss: 2.50134399715735\n",
            "Epoch 98, Loss: 2.515294180840862\n",
            "Epoch 99, Loss: 2.485062528629692\n",
            "Epoch 100, Loss: 2.4953527122127768\n",
            "Epoch 101, Loss: 2.4876419476100375\n",
            "Epoch 102, Loss: 2.4979381451801377\n",
            "Epoch 103, Loss: 2.4995139910250295\n",
            "Epoch 104, Loss: 2.480239453364392\n",
            "Epoch 105, Loss: 2.4859472671333624\n",
            "Epoch 106, Loss: 2.4750492755247624\n",
            "Epoch 107, Loss: 2.4947466254234314\n",
            "Epoch 108, Loss: 2.480394241761188\n",
            "Epoch 109, Loss: 2.4834651217168693\n",
            "Epoch 110, Loss: 2.4893205044220905\n",
            "Epoch 111, Loss: 2.46603001137169\n",
            "Epoch 112, Loss: 2.475019246948009\n",
            "Epoch 113, Loss: 2.4706350710927225\n",
            "Epoch 114, Loss: 2.483584757970304\n",
            "Epoch 115, Loss: 2.4841650213514055\n",
            "Epoch 116, Loss: 2.4661080192546456\n",
            "Epoch 117, Loss: 2.4671319759621912\n",
            "Epoch 118, Loss: 2.479485242950673\n",
            "Epoch 119, Loss: 2.4625864673633964\n",
            "Epoch 120, Loss: 2.4713570244458256\n",
            "Epoch 121, Loss: 2.477731923667752\n",
            "Epoch 122, Loss: 2.5059159994125366\n",
            "Epoch 123, Loss: 2.4755765126675975\n",
            "Epoch 124, Loss: 2.4760608916379967\n",
            "Epoch 125, Loss: 2.4721236520883987\n",
            "Epoch 126, Loss: 2.4800918637489784\n",
            "Epoch 127, Loss: 2.471100379009636\n",
            "Epoch 128, Loss: 2.477822247816592\n",
            "Epoch 129, Loss: 2.4718539191752065\n",
            "Epoch 130, Loss: 2.470418307245994\n",
            "Epoch 131, Loss: 2.4926110007324995\n",
            "Epoch 132, Loss: 2.459646071706499\n",
            "Epoch 133, Loss: 2.460799163701583\n",
            "Epoch 134, Loss: 2.455467761779318\n",
            "Epoch 135, Loss: 2.4647097295644333\n",
            "Epoch 136, Loss: 2.4601192243245182\n",
            "Epoch 137, Loss: 2.4614818558400993\n",
            "Epoch 138, Loss: 2.474174303667886\n",
            "Epoch 139, Loss: 2.467187557901655\n",
            "Epoch 140, Loss: 2.481133279751758\n",
            "Epoch 141, Loss: 2.4758861344687793\n",
            "Epoch 142, Loss: 2.459561833313533\n",
            "Epoch 143, Loss: 2.46501326439332\n",
            "Epoch 144, Loss: 2.462922891791986\n",
            "Epoch 145, Loss: 2.4725333926629047\n",
            "Epoch 146, Loss: 2.47090598636744\n",
            "Epoch 147, Loss: 2.457747009335732\n",
            "Epoch 148, Loss: 2.462837097596149\n",
            "Epoch 149, Loss: 2.458091301577432\n",
            "Epoch 150, Loss: 2.4703630890165056\n",
            "Epoch 151, Loss: 2.4512075769657993\n",
            "Epoch 152, Loss: 2.4697498229085184\n",
            "Epoch 153, Loss: 2.459268509125223\n",
            "Epoch 154, Loss: 2.457858299722477\n",
            "Epoch 155, Loss: 2.470589747234267\n",
            "Epoch 156, Loss: 2.4461307562127406\n",
            "Epoch 157, Loss: 2.4588254750991356\n",
            "Epoch 158, Loss: 2.4600664517100976\n",
            "Epoch 159, Loss: 2.473402754384644\n",
            "Epoch 160, Loss: 2.4474835456634056\n",
            "Epoch 161, Loss: 2.4473230850939847\n",
            "Epoch 162, Loss: 2.4663878715768153\n",
            "Epoch 163, Loss: 2.465510016801406\n",
            "Epoch 164, Loss: 2.4566227830186183\n",
            "Epoch 165, Loss: 2.443035572159047\n",
            "Epoch 166, Loss: 2.4516168334046187\n",
            "Epoch 167, Loss: 2.453272281860819\n",
            "Epoch 168, Loss: 2.440341124729234\n",
            "Epoch 169, Loss: 2.465696312943283\n",
            "Epoch 170, Loss: 2.469843244066044\n",
            "Epoch 171, Loss: 2.439907254004965\n",
            "Epoch 172, Loss: 2.4620218447276523\n",
            "Epoch 173, Loss: 2.4517279376789016\n",
            "Epoch 174, Loss: 2.449828160052397\n",
            "Epoch 175, Loss: 2.459696337884786\n",
            "Epoch 176, Loss: 2.439869147174212\n",
            "Epoch 177, Loss: 2.4509533716707814\n",
            "Epoch 178, Loss: 2.437542684224187\n",
            "Epoch 179, Loss: 2.454158808503832\n",
            "Epoch 180, Loss: 2.44668470961707\n",
            "Epoch 181, Loss: 2.433905616098521\n",
            "Epoch 182, Loss: 2.4428635203108495\n",
            "Epoch 183, Loss: 2.439978909735777\n",
            "Epoch 184, Loss: 2.4522281994625015\n",
            "Epoch 185, Loss: 2.4552708718241476\n",
            "Epoch 186, Loss: 2.434484767670534\n",
            "Epoch 187, Loss: 2.435703630350074\n",
            "Epoch 188, Loss: 2.4591826516754773\n",
            "Epoch 189, Loss: 2.4536330785070146\n",
            "Epoch 190, Loss: 2.446797938979402\n",
            "Epoch 191, Loss: 2.443481963507983\n",
            "Epoch 192, Loss: 2.431153045625103\n",
            "Epoch 193, Loss: 2.4473415515860735\n",
            "Epoch 194, Loss: 2.4383068559121113\n",
            "Epoch 195, Loss: 2.431545792793741\n",
            "Epoch 196, Loss: 2.4374706805968773\n",
            "Epoch 197, Loss: 2.426284387403605\n",
            "Epoch 198, Loss: 2.454818225636774\n",
            "Epoch 199, Loss: 2.4510759103054904\n",
            "Accuracy: 26.88%\n",
            "Training with params: {'batch_size': 256, 'lr': 0.01, 'optimizer': 'SGD'}\n",
            "Epoch 1, Loss: 4.605632052129629\n",
            "Epoch 2, Loss: 4.605072121230924\n",
            "Epoch 3, Loss: 4.604424070338814\n",
            "Epoch 4, Loss: 4.603720365738382\n",
            "Epoch 5, Loss: 4.602748530251639\n",
            "Epoch 6, Loss: 4.601501039096287\n",
            "Epoch 7, Loss: 4.599767653309569\n",
            "Epoch 8, Loss: 4.597099440438407\n",
            "Epoch 9, Loss: 4.592461432729449\n",
            "Epoch 10, Loss: 4.583220554857838\n",
            "Epoch 11, Loss: 4.560769611475419\n",
            "Epoch 12, Loss: 4.501834304965272\n",
            "Epoch 13, Loss: 4.399056733870993\n",
            "Epoch 14, Loss: 4.290982594295424\n",
            "Epoch 15, Loss: 4.208548528807504\n",
            "Epoch 16, Loss: 4.1379552264602815\n",
            "Epoch 17, Loss: 4.056774630838511\n",
            "Epoch 18, Loss: 3.941246293029007\n",
            "Epoch 19, Loss: 3.84537907644194\n",
            "Epoch 20, Loss: 3.787938586303166\n",
            "Epoch 21, Loss: 3.741952031242604\n",
            "Epoch 22, Loss: 3.6990080646106174\n",
            "Epoch 23, Loss: 3.65849456251884\n",
            "Epoch 24, Loss: 3.6174341586171366\n",
            "Epoch 25, Loss: 3.5757922615323747\n",
            "Epoch 26, Loss: 3.5383782581407197\n",
            "Epoch 27, Loss: 3.504961904214353\n",
            "Epoch 28, Loss: 3.472223475271342\n",
            "Epoch 29, Loss: 3.4401089305780372\n",
            "Epoch 30, Loss: 3.410496810261084\n",
            "Epoch 31, Loss: 3.381103609289442\n",
            "Epoch 32, Loss: 3.354566058334039\n",
            "Epoch 33, Loss: 3.3263552541635475\n",
            "Epoch 34, Loss: 3.298395034001798\n",
            "Epoch 35, Loss: 3.270468902831175\n",
            "Epoch 36, Loss: 3.2419609962677467\n",
            "Epoch 37, Loss: 3.2158556373751894\n",
            "Epoch 38, Loss: 3.187762492773484\n",
            "Epoch 39, Loss: 3.162180703513476\n",
            "Epoch 40, Loss: 3.135311011148959\n",
            "Epoch 41, Loss: 3.1114775441130815\n",
            "Epoch 42, Loss: 3.0821591238586272\n",
            "Epoch 43, Loss: 3.0571665447585437\n",
            "Epoch 44, Loss: 3.030270138565375\n",
            "Epoch 45, Loss: 3.004453024085687\n",
            "Epoch 46, Loss: 2.9762142118142574\n",
            "Epoch 47, Loss: 2.9503575247161242\n",
            "Epoch 48, Loss: 2.927272606869133\n",
            "Epoch 49, Loss: 2.899765823568617\n",
            "Epoch 50, Loss: 2.872481281660041\n",
            "Epoch 51, Loss: 2.852233366090424\n",
            "Epoch 52, Loss: 2.82791974593182\n",
            "Epoch 53, Loss: 2.798973021458606\n",
            "Epoch 54, Loss: 2.7699385504333343\n",
            "Epoch 55, Loss: 2.742153007156995\n",
            "Epoch 56, Loss: 2.724163130838044\n",
            "Epoch 57, Loss: 2.7002923792722275\n",
            "Epoch 58, Loss: 2.6702417743449307\n",
            "Epoch 59, Loss: 2.650336829983458\n",
            "Epoch 60, Loss: 2.622449233823893\n",
            "Epoch 61, Loss: 2.600680346391639\n",
            "Epoch 62, Loss: 2.576767478670393\n",
            "Epoch 63, Loss: 2.5506895731906503\n",
            "Epoch 64, Loss: 2.5327542314724045\n",
            "Epoch 65, Loss: 2.510597137772307\n",
            "Epoch 66, Loss: 2.49061580580108\n",
            "Epoch 67, Loss: 2.463719911721288\n",
            "Epoch 68, Loss: 2.444193090711321\n",
            "Epoch 69, Loss: 2.419650295559241\n",
            "Epoch 70, Loss: 2.4028623992083022\n",
            "Epoch 71, Loss: 2.3797052782409045\n",
            "Epoch 72, Loss: 2.357908689245886\n",
            "Epoch 73, Loss: 2.3398600658591913\n",
            "Epoch 74, Loss: 2.3216825091108984\n",
            "Epoch 75, Loss: 2.2934544183770003\n",
            "Epoch 76, Loss: 2.277003919591709\n",
            "Epoch 77, Loss: 2.2564782828700785\n",
            "Epoch 78, Loss: 2.2299477664791807\n",
            "Epoch 79, Loss: 2.219323669769326\n",
            "Epoch 80, Loss: 2.199500080274076\n",
            "Epoch 81, Loss: 2.1735885940035997\n",
            "Epoch 82, Loss: 2.1539289531659107\n",
            "Epoch 83, Loss: 2.1360469588211606\n",
            "Epoch 84, Loss: 2.1141386865353096\n",
            "Epoch 85, Loss: 2.094022423028946\n",
            "Epoch 86, Loss: 2.0821247484002794\n",
            "Epoch 87, Loss: 2.0514210474734402\n",
            "Epoch 88, Loss: 2.0332551178883533\n",
            "Epoch 89, Loss: 2.016221051313439\n",
            "Epoch 90, Loss: 1.996133872440883\n",
            "Epoch 91, Loss: 1.970762737551514\n",
            "Epoch 92, Loss: 1.9599417782559687\n",
            "Epoch 93, Loss: 1.93342201807061\n",
            "Epoch 94, Loss: 1.924079036226078\n",
            "Epoch 95, Loss: 1.896557020897768\n",
            "Epoch 96, Loss: 1.8758887885784616\n",
            "Epoch 97, Loss: 1.8643723920899995\n",
            "Epoch 98, Loss: 1.8363364229396897\n",
            "Epoch 99, Loss: 1.8245937009246982\n",
            "Epoch 100, Loss: 1.8019981986405897\n",
            "Epoch 101, Loss: 1.7867470924951592\n",
            "Epoch 102, Loss: 1.7655053467166668\n",
            "Epoch 103, Loss: 1.7383630622406394\n",
            "Epoch 104, Loss: 1.7306514923669853\n",
            "Epoch 105, Loss: 1.7107382276836707\n",
            "Epoch 106, Loss: 1.6922083533540064\n",
            "Epoch 107, Loss: 1.6667619049549103\n",
            "Epoch 108, Loss: 1.64947144474302\n",
            "Epoch 109, Loss: 1.6343951675356652\n",
            "Epoch 110, Loss: 1.6104150341481578\n",
            "Epoch 111, Loss: 1.597793856445624\n",
            "Epoch 112, Loss: 1.5720118716055034\n",
            "Epoch 113, Loss: 1.558648218913954\n",
            "Epoch 114, Loss: 1.5391588059006904\n",
            "Epoch 115, Loss: 1.5161734959300683\n",
            "Epoch 116, Loss: 1.5071411607216816\n",
            "Epoch 117, Loss: 1.4900430751090148\n",
            "Epoch 118, Loss: 1.4629528254878765\n",
            "Epoch 119, Loss: 1.4510837860253392\n",
            "Epoch 120, Loss: 1.4331078930776946\n",
            "Epoch 121, Loss: 1.413272876520546\n",
            "Epoch 122, Loss: 1.3925453619081147\n",
            "Epoch 123, Loss: 1.3806408065922406\n",
            "Epoch 124, Loss: 1.3467989728158833\n",
            "Epoch 125, Loss: 1.3383396705802606\n",
            "Epoch 126, Loss: 1.3215211538635954\n",
            "Epoch 127, Loss: 1.302464523789834\n",
            "Epoch 128, Loss: 1.284517946291943\n",
            "Epoch 129, Loss: 1.2724087524170777\n",
            "Epoch 130, Loss: 1.2561040885594426\n",
            "Epoch 131, Loss: 1.2353436627558299\n",
            "Epoch 132, Loss: 1.211505839106988\n",
            "Epoch 133, Loss: 1.19631462498587\n",
            "Epoch 134, Loss: 1.1756324105116787\n",
            "Epoch 135, Loss: 1.163588486155685\n",
            "Epoch 136, Loss: 1.143525601649771\n",
            "Epoch 137, Loss: 1.1274062632297983\n",
            "Epoch 138, Loss: 1.1147359977571332\n",
            "Epoch 139, Loss: 1.097249446474776\n",
            "Epoch 140, Loss: 1.083375202149761\n",
            "Epoch 141, Loss: 1.0634604655966466\n",
            "Epoch 142, Loss: 1.0356002030323963\n",
            "Epoch 143, Loss: 1.0308412231352864\n",
            "Epoch 144, Loss: 1.0022012226435604\n",
            "Epoch 145, Loss: 0.9896411792356141\n",
            "Epoch 146, Loss: 0.969560694633698\n",
            "Epoch 147, Loss: 0.9606221403394427\n",
            "Epoch 148, Loss: 0.9374033197456476\n",
            "Epoch 149, Loss: 0.9307845237911964\n",
            "Epoch 150, Loss: 0.9127847999334335\n",
            "Epoch 151, Loss: 0.8846855513295349\n",
            "Epoch 152, Loss: 0.8847436175054434\n",
            "Epoch 153, Loss: 0.8602463210723839\n",
            "Epoch 154, Loss: 0.8486624341838214\n",
            "Epoch 155, Loss: 0.8227843593577949\n",
            "Epoch 156, Loss: 0.814687695734355\n",
            "Epoch 157, Loss: 0.8063364299584408\n",
            "Epoch 158, Loss: 0.7837542712080235\n",
            "Epoch 159, Loss: 0.7830543019333664\n",
            "Epoch 160, Loss: 0.7476951023753808\n",
            "Epoch 161, Loss: 0.7356611538906487\n",
            "Epoch 162, Loss: 0.7219651840171035\n",
            "Epoch 163, Loss: 0.693268038484515\n",
            "Epoch 164, Loss: 0.693347226448205\n",
            "Epoch 165, Loss: 0.6810053821120944\n",
            "Epoch 166, Loss: 0.6579014515998413\n",
            "Epoch 167, Loss: 0.6442012272927226\n",
            "Epoch 168, Loss: 0.652077428540405\n",
            "Epoch 169, Loss: 0.603789430643831\n",
            "Epoch 170, Loss: 0.6113698374556036\n",
            "Epoch 171, Loss: 0.5871013212873011\n",
            "Epoch 172, Loss: 0.5946864256141137\n",
            "Epoch 173, Loss: 0.5555665059661379\n",
            "Epoch 174, Loss: 0.5625216083563104\n",
            "Epoch 175, Loss: 0.5540227894576228\n",
            "Epoch 176, Loss: 0.5191071255474674\n",
            "Epoch 177, Loss: 0.49342234341465696\n",
            "Epoch 178, Loss: 0.5055661005329113\n",
            "Epoch 179, Loss: 0.488643049129418\n",
            "Epoch 180, Loss: 0.4661297326793476\n",
            "Epoch 181, Loss: 0.4437816956517648\n",
            "Epoch 182, Loss: 0.4331645833290353\n",
            "Epoch 183, Loss: 0.47208864941280715\n",
            "Epoch 184, Loss: 0.4496779888868332\n",
            "Epoch 185, Loss: 0.407657046935388\n",
            "Epoch 186, Loss: 0.3908447666587878\n",
            "Epoch 187, Loss: 0.40456778951445405\n",
            "Epoch 188, Loss: 0.402774202428302\n",
            "Epoch 189, Loss: 0.35794042271314835\n",
            "Epoch 190, Loss: 0.3598627696688078\n",
            "Epoch 191, Loss: 0.3645053484610149\n",
            "Epoch 192, Loss: 0.2941812011508309\n",
            "Epoch 193, Loss: 0.3270688072151067\n",
            "Epoch 194, Loss: 0.2834197886440219\n",
            "Epoch 195, Loss: 0.26660532344664845\n",
            "Epoch 196, Loss: 0.381832855278436\n",
            "Epoch 197, Loss: 0.26603721401521135\n",
            "Epoch 198, Loss: 0.27503200346718026\n",
            "Epoch 199, Loss: 0.2728411005999969\n",
            "Accuracy: 27.53%\n",
            "Training with params: {'batch_size': 256, 'lr': 0.01, 'optimizer': 'RMSprop'}\n",
            "Epoch 1, Loss: 42.16357316046345\n",
            "Epoch 2, Loss: 3.840606773386196\n",
            "Epoch 3, Loss: 3.69292543007403\n",
            "Epoch 4, Loss: 3.561361236231668\n",
            "Epoch 5, Loss: 3.496438885221676\n",
            "Epoch 6, Loss: 3.422314298396208\n",
            "Epoch 7, Loss: 3.331221486840929\n",
            "Epoch 8, Loss: 3.2663245857978356\n",
            "Epoch 9, Loss: 3.223319547516959\n",
            "Epoch 10, Loss: 3.1140544353699195\n",
            "Epoch 11, Loss: 3.0488939784011064\n",
            "Epoch 12, Loss: 2.9983252688330047\n",
            "Epoch 13, Loss: 2.954544288771493\n",
            "Epoch 14, Loss: 2.9114052884432735\n",
            "Epoch 15, Loss: 2.8621428341281656\n",
            "Epoch 16, Loss: 2.843009897640773\n",
            "Epoch 17, Loss: 2.7898741705077037\n",
            "Epoch 18, Loss: 2.7689301578366026\n",
            "Epoch 19, Loss: 2.7244168519973755\n",
            "Epoch 20, Loss: 2.7168160859419377\n",
            "Epoch 21, Loss: 2.678334717847863\n",
            "Epoch 22, Loss: 2.6482480265656294\n",
            "Epoch 23, Loss: 2.6225341716591193\n",
            "Epoch 24, Loss: 2.6036200292256413\n",
            "Epoch 25, Loss: 2.578382526125227\n",
            "Epoch 26, Loss: 2.554715568922004\n",
            "Epoch 27, Loss: 2.5425346919468472\n",
            "Epoch 28, Loss: 2.4911409044752317\n",
            "Epoch 29, Loss: 2.492878941857085\n",
            "Epoch 30, Loss: 2.463383515270389\n",
            "Epoch 31, Loss: 2.461544231492646\n",
            "Epoch 32, Loss: 2.4378951204066373\n",
            "Epoch 33, Loss: 2.425396189397695\n",
            "Epoch 34, Loss: 2.4009650568572845\n",
            "Epoch 35, Loss: 2.3914573350731207\n",
            "Epoch 36, Loss: 2.38320531285539\n",
            "Epoch 37, Loss: 2.352440500137757\n",
            "Epoch 38, Loss: 2.35133879646963\n",
            "Epoch 39, Loss: 2.3398397595298532\n",
            "Epoch 40, Loss: 2.3334433156616834\n",
            "Epoch 41, Loss: 2.343290709719366\n",
            "Epoch 42, Loss: 2.287866995650895\n",
            "Epoch 43, Loss: 2.3118967499051775\n",
            "Epoch 44, Loss: 2.277086397214812\n",
            "Epoch 45, Loss: 2.2745645240861543\n",
            "Epoch 46, Loss: 2.258007196747527\n",
            "Epoch 47, Loss: 2.2563750026177387\n",
            "Epoch 48, Loss: 2.254430055618286\n",
            "Epoch 49, Loss: 2.228796210824227\n",
            "Epoch 50, Loss: 2.239629694393703\n",
            "Epoch 51, Loss: 2.2297444611179587\n",
            "Epoch 52, Loss: 2.226102759035266\n",
            "Epoch 53, Loss: 2.2063770494898973\n",
            "Epoch 54, Loss: 2.2153587681906566\n",
            "Epoch 55, Loss: 2.2034369725353864\n",
            "Epoch 56, Loss: 2.189081361707376\n",
            "Epoch 57, Loss: 2.1923909977990754\n",
            "Epoch 58, Loss: 2.173601500842036\n",
            "Epoch 59, Loss: 2.1689969124842663\n",
            "Epoch 60, Loss: 2.162267818134658\n",
            "Epoch 61, Loss: 2.1760447080038032\n",
            "Epoch 62, Loss: 2.158580090318407\n",
            "Epoch 63, Loss: 2.1552183804463367\n",
            "Epoch 64, Loss: 2.1492950405393327\n",
            "Epoch 65, Loss: 2.14228978813911\n",
            "Epoch 66, Loss: 2.1446836450878455\n",
            "Epoch 67, Loss: 2.1322816567761556\n",
            "Epoch 68, Loss: 2.1292514332703183\n",
            "Epoch 69, Loss: 2.122605321358661\n",
            "Epoch 70, Loss: 2.119338731376492\n",
            "Epoch 71, Loss: 2.116380959141011\n",
            "Epoch 72, Loss: 2.099552627120699\n",
            "Epoch 73, Loss: 2.110440207379205\n",
            "Epoch 74, Loss: 2.1062080525622076\n",
            "Epoch 75, Loss: 2.1310805635792867\n",
            "Epoch 76, Loss: 2.087916698991036\n",
            "Epoch 77, Loss: 2.113654857387348\n",
            "Epoch 78, Loss: 2.0776216880399354\n",
            "Epoch 79, Loss: 2.0998055271956386\n",
            "Epoch 80, Loss: 2.083213428453523\n",
            "Epoch 81, Loss: 2.0840504449241015\n",
            "Epoch 82, Loss: 2.106006279891851\n",
            "Epoch 83, Loss: 2.0833192406868446\n",
            "Epoch 84, Loss: 2.057500633658195\n",
            "Epoch 85, Loss: 2.0743929165966657\n",
            "Epoch 86, Loss: 2.0591208144110076\n",
            "Epoch 87, Loss: 2.0748033681694342\n",
            "Epoch 88, Loss: 2.0667846804978898\n",
            "Epoch 89, Loss: 2.0732342193321305\n",
            "Epoch 90, Loss: 2.0644137251133823\n",
            "Epoch 91, Loss: 2.08317200809109\n",
            "Epoch 92, Loss: 2.0637338112811654\n",
            "Epoch 93, Loss: 2.064948673759188\n",
            "Epoch 94, Loss: 2.0529236167061087\n",
            "Epoch 95, Loss: 2.062127703914837\n",
            "Epoch 96, Loss: 2.0609121997745667\n",
            "Epoch 97, Loss: 2.0433949749080504\n",
            "Epoch 98, Loss: 2.0598997923792624\n",
            "Epoch 99, Loss: 2.061836519411632\n",
            "Epoch 100, Loss: 2.043970061808216\n",
            "Epoch 101, Loss: 2.0594164984566823\n",
            "Epoch 102, Loss: 2.0701176305206452\n",
            "Epoch 103, Loss: 2.062577501851685\n",
            "Epoch 104, Loss: 2.055937813252819\n",
            "Epoch 105, Loss: 2.058248296684148\n",
            "Epoch 106, Loss: 2.0416848081715253\n",
            "Epoch 107, Loss: 2.0956616486821855\n",
            "Epoch 108, Loss: 2.03483452967235\n",
            "Epoch 109, Loss: 2.0565266755162455\n",
            "Epoch 110, Loss: 2.056168491134838\n",
            "Epoch 111, Loss: 2.0713606458537432\n",
            "Epoch 112, Loss: 2.02435061639669\n",
            "Epoch 113, Loss: 2.0719556419216856\n",
            "Epoch 114, Loss: 2.04466569849423\n",
            "Epoch 115, Loss: 2.0451949469897213\n",
            "Epoch 116, Loss: 2.0722704359463284\n",
            "Epoch 117, Loss: 2.054955768341921\n",
            "Epoch 118, Loss: 2.0252868107386996\n",
            "Epoch 119, Loss: 2.065951557791963\n",
            "Epoch 120, Loss: 2.0393873422729727\n",
            "Epoch 121, Loss: 2.0618776818927453\n",
            "Epoch 122, Loss: 2.0447957692097645\n",
            "Epoch 123, Loss: 2.063758261957947\n",
            "Epoch 124, Loss: 2.0384750262815126\n",
            "Epoch 125, Loss: 2.0755110437772712\n",
            "Epoch 126, Loss: 2.0619361911501204\n",
            "Epoch 127, Loss: 2.0392871140217292\n",
            "Epoch 128, Loss: 2.0600599026193422\n",
            "Epoch 129, Loss: 2.0870505443641116\n",
            "Epoch 130, Loss: 2.072845587925035\n",
            "Epoch 131, Loss: 2.0798772214626777\n",
            "Epoch 132, Loss: 2.0167723358893883\n",
            "Epoch 133, Loss: 2.0581684179452\n",
            "Epoch 134, Loss: 2.043771648893551\n",
            "Epoch 135, Loss: 2.094136714327092\n",
            "Epoch 136, Loss: 2.08512806892395\n",
            "Epoch 137, Loss: 2.117693521538559\n",
            "Epoch 138, Loss: 2.0354945988071207\n",
            "Epoch 139, Loss: 2.090518894852424\n",
            "Epoch 140, Loss: 2.064819644908516\n",
            "Epoch 141, Loss: 2.062568652386568\n",
            "Epoch 142, Loss: 2.0425246643776798\n",
            "Epoch 143, Loss: 2.0647738746234348\n",
            "Epoch 144, Loss: 2.0950391450706793\n",
            "Epoch 145, Loss: 2.0838336664803174\n",
            "Epoch 146, Loss: 2.2140858325423025\n",
            "Epoch 147, Loss: 2.1001637492861067\n",
            "Epoch 148, Loss: 2.088738292455673\n",
            "Epoch 149, Loss: 2.1131886201245442\n",
            "Epoch 150, Loss: 2.0587298541652914\n",
            "Epoch 151, Loss: 2.099106576977944\n",
            "Epoch 152, Loss: 2.07764133810997\n",
            "Epoch 153, Loss: 2.102359726112716\n",
            "Epoch 154, Loss: 2.1226533803404593\n",
            "Epoch 155, Loss: 2.0880119374820163\n",
            "Epoch 156, Loss: 2.153917614902769\n",
            "Epoch 157, Loss: 2.099607627610771\n",
            "Epoch 158, Loss: 2.1348478058163\n",
            "Epoch 159, Loss: 2.089969424568877\n",
            "Epoch 160, Loss: 2.0696840268008563\n",
            "Epoch 161, Loss: 2.1317227750408407\n",
            "Epoch 162, Loss: 2.229213693312236\n",
            "Epoch 163, Loss: 2.1129723866375123\n",
            "Epoch 164, Loss: 2.085153493346\n",
            "Epoch 165, Loss: 2.1149024501138802\n",
            "Epoch 166, Loss: 2.120700433546183\n",
            "Epoch 167, Loss: 2.083183002715208\n",
            "Epoch 168, Loss: 2.0972886657228273\n",
            "Epoch 169, Loss: 2.168937655735989\n",
            "Epoch 170, Loss: 2.135239362716675\n",
            "Epoch 171, Loss: 2.1311310700007846\n",
            "Epoch 172, Loss: 2.1394739716636892\n",
            "Epoch 173, Loss: 2.1383073798247745\n",
            "Epoch 174, Loss: 2.181864992088201\n",
            "Epoch 175, Loss: 2.1167448619190528\n",
            "Epoch 176, Loss: 2.183189104406201\n",
            "Epoch 177, Loss: 2.118640584605081\n",
            "Epoch 178, Loss: 2.137810921182438\n",
            "Epoch 179, Loss: 2.1821510651890113\n",
            "Epoch 180, Loss: 2.1106461438597464\n",
            "Epoch 181, Loss: 2.1730271468357163\n",
            "Epoch 182, Loss: 2.1545718890063617\n",
            "Epoch 183, Loss: 2.172563146571724\n",
            "Epoch 184, Loss: 2.186715785337954\n",
            "Epoch 185, Loss: 2.149117246574285\n",
            "Epoch 186, Loss: 2.204614350990373\n",
            "Epoch 187, Loss: 2.207894806959191\n",
            "Epoch 188, Loss: 2.278692570876102\n",
            "Epoch 189, Loss: 2.1888720259374503\n",
            "Epoch 190, Loss: 2.207109362495189\n",
            "Epoch 191, Loss: 2.2293860760270334\n",
            "Epoch 192, Loss: 2.2225014640360463\n",
            "Epoch 193, Loss: 2.1607136659476223\n",
            "Epoch 194, Loss: 2.1621560180673796\n",
            "Epoch 195, Loss: 2.2384312694170037\n",
            "Epoch 196, Loss: 2.2062130758956986\n",
            "Epoch 197, Loss: 2.2571677474343046\n",
            "Epoch 198, Loss: 2.214225872438781\n",
            "Epoch 199, Loss: 2.186469320131808\n",
            "Accuracy: 18.80%\n",
            "Training with params: {'batch_size': 256, 'lr': 0.001, 'optimizer': 'Adam'}\n",
            "Epoch 1, Loss: 3.953956594272536\n",
            "Epoch 2, Loss: 3.2981475530838478\n",
            "Epoch 3, Loss: 2.9764482330302804\n",
            "Epoch 4, Loss: 2.760095253282664\n",
            "Epoch 5, Loss: 2.601190478217845\n",
            "Epoch 6, Loss: 2.469483425422591\n",
            "Epoch 7, Loss: 2.354359974666518\n",
            "Epoch 8, Loss: 2.2459762570809345\n",
            "Epoch 9, Loss: 2.15282098918545\n",
            "Epoch 10, Loss: 2.0610418088582096\n",
            "Epoch 11, Loss: 1.9719612020619062\n",
            "Epoch 12, Loss: 1.8966769642975865\n",
            "Epoch 13, Loss: 1.8203096499248428\n",
            "Epoch 14, Loss: 1.7454136537045848\n",
            "Epoch 15, Loss: 1.6790515044514014\n",
            "Epoch 16, Loss: 1.6087059761796678\n",
            "Epoch 17, Loss: 1.5503864111948986\n",
            "Epoch 18, Loss: 1.4824077730276146\n",
            "Epoch 19, Loss: 1.4349217968327659\n",
            "Epoch 20, Loss: 1.3671095724008522\n",
            "Epoch 21, Loss: 1.3106263249504322\n",
            "Epoch 22, Loss: 1.2540734203494326\n",
            "Epoch 23, Loss: 1.2079909246186822\n",
            "Epoch 24, Loss: 1.1572768159058628\n",
            "Epoch 25, Loss: 1.1072688759589682\n",
            "Epoch 26, Loss: 1.0602743954074627\n",
            "Epoch 27, Loss: 1.0127157289154676\n",
            "Epoch 28, Loss: 0.9755741233120159\n",
            "Epoch 29, Loss: 0.9380760776753329\n",
            "Epoch 30, Loss: 0.8852436530346773\n",
            "Epoch 31, Loss: 0.853487201795286\n",
            "Epoch 32, Loss: 0.8165164927438814\n",
            "Epoch 33, Loss: 0.7707438940296367\n",
            "Epoch 34, Loss: 0.7434562532877436\n",
            "Epoch 35, Loss: 0.7051581484930856\n",
            "Epoch 36, Loss: 0.6637949432645526\n",
            "Epoch 37, Loss: 0.6454180939769258\n",
            "Epoch 38, Loss: 0.6067356137292725\n",
            "Epoch 39, Loss: 0.5913496134536607\n",
            "Epoch 40, Loss: 0.5490196263607667\n",
            "Epoch 41, Loss: 0.5235214943484384\n",
            "Epoch 42, Loss: 0.5030345795105915\n",
            "Epoch 43, Loss: 0.4713626849103947\n",
            "Epoch 44, Loss: 0.4662520620895892\n",
            "Epoch 45, Loss: 0.43723784645601194\n",
            "Epoch 46, Loss: 0.4224209797625639\n",
            "Epoch 47, Loss: 0.4049302234637494\n",
            "Epoch 48, Loss: 0.39389384325061527\n",
            "Epoch 49, Loss: 0.3683215550774214\n",
            "Epoch 50, Loss: 0.33590948346013927\n",
            "Epoch 51, Loss: 0.32546286049241924\n",
            "Epoch 52, Loss: 0.314789675708328\n",
            "Epoch 53, Loss: 0.31096244572984927\n",
            "Epoch 54, Loss: 0.2947249073459178\n",
            "Epoch 55, Loss: 0.3117016214801341\n",
            "Epoch 56, Loss: 0.29395116858032283\n",
            "Epoch 57, Loss: 0.262612773523647\n",
            "Epoch 58, Loss: 0.25567524883972137\n",
            "Epoch 59, Loss: 0.2561548413062582\n",
            "Epoch 60, Loss: 0.23706703754712125\n",
            "Epoch 61, Loss: 0.24990417481381066\n",
            "Epoch 62, Loss: 0.2721875366203639\n",
            "Epoch 63, Loss: 0.2169652440946321\n",
            "Epoch 64, Loss: 0.19862054482254446\n",
            "Epoch 65, Loss: 0.1850887216931703\n",
            "Epoch 66, Loss: 0.25530144572257996\n",
            "Epoch 67, Loss: 0.23356170978929316\n",
            "Epoch 68, Loss: 0.20650108637554304\n",
            "Epoch 69, Loss: 0.21025754184443124\n",
            "Epoch 70, Loss: 0.16926391757264428\n",
            "Epoch 71, Loss: 0.17772547782835912\n",
            "Epoch 72, Loss: 0.204369488723424\n",
            "Epoch 73, Loss: 0.2132817420120142\n",
            "Epoch 74, Loss: 0.20077372593234996\n",
            "Epoch 75, Loss: 0.1654630295019977\n",
            "Epoch 76, Loss: 0.11550540378203197\n",
            "Epoch 77, Loss: 0.13929616291151972\n",
            "Epoch 78, Loss: 0.223793927770184\n",
            "Epoch 79, Loss: 0.2365564962430876\n",
            "Epoch 80, Loss: 0.15984885715784466\n",
            "Epoch 81, Loss: 0.13545617060165624\n",
            "Epoch 82, Loss: 0.1618825527554264\n",
            "Epoch 83, Loss: 0.19621889931815012\n",
            "Epoch 84, Loss: 0.1556142908045832\n",
            "Epoch 85, Loss: 0.1337809161644201\n",
            "Epoch 86, Loss: 0.1834525699746244\n",
            "Epoch 87, Loss: 0.16184694555645085\n",
            "Epoch 88, Loss: 0.17054227420261928\n",
            "Epoch 89, Loss: 0.15769454726607215\n",
            "Epoch 90, Loss: 0.12649897242687186\n",
            "Epoch 91, Loss: 0.15653410605249965\n",
            "Epoch 92, Loss: 0.16228468320807632\n",
            "Epoch 93, Loss: 0.21075876760391557\n",
            "Epoch 94, Loss: 0.18016558079695216\n",
            "Epoch 95, Loss: 0.11514244679057477\n",
            "Epoch 96, Loss: 0.11251697183719703\n",
            "Epoch 97, Loss: 0.09519710781394827\n",
            "Epoch 98, Loss: 0.16425517191920352\n",
            "Epoch 99, Loss: 0.21750244939205599\n",
            "Epoch 100, Loss: 0.15139836913012727\n",
            "Epoch 101, Loss: 0.12974963557659364\n",
            "Epoch 102, Loss: 0.09757223664497842\n",
            "Epoch 103, Loss: 0.13140774119113172\n",
            "Epoch 104, Loss: 0.14158782847605797\n",
            "Epoch 105, Loss: 0.15632136481605013\n",
            "Epoch 106, Loss: 0.13757809844552255\n",
            "Epoch 107, Loss: 0.14200515758094132\n",
            "Epoch 108, Loss: 0.13086494210423255\n",
            "Epoch 109, Loss: 0.14052982715775772\n",
            "Epoch 110, Loss: 0.11732511642408006\n",
            "Epoch 111, Loss: 0.15075269818534048\n",
            "Epoch 112, Loss: 0.14812482975195257\n",
            "Epoch 113, Loss: 0.11879254847156759\n",
            "Epoch 114, Loss: 0.11274559345400455\n",
            "Epoch 115, Loss: 0.10356432779178935\n",
            "Epoch 116, Loss: 0.16786377242177117\n",
            "Epoch 117, Loss: 0.14668085159999983\n",
            "Epoch 118, Loss: 0.1420084809200192\n",
            "Epoch 119, Loss: 0.1000020634573029\n",
            "Epoch 120, Loss: 0.08644788692306195\n",
            "Epoch 121, Loss: 0.12914963851549796\n",
            "Epoch 122, Loss: 0.15259831653413725\n",
            "Epoch 123, Loss: 0.1338584552309951\n",
            "Epoch 124, Loss: 0.12115740072818434\n",
            "Epoch 125, Loss: 0.11433771642267096\n",
            "Epoch 126, Loss: 0.13307772844801752\n",
            "Epoch 127, Loss: 0.14793548079169527\n",
            "Epoch 128, Loss: 0.12844765224322982\n",
            "Epoch 129, Loss: 0.0903640823647836\n",
            "Epoch 130, Loss: 0.10834654164975699\n",
            "Epoch 131, Loss: 0.14831390165324723\n",
            "Epoch 132, Loss: 0.10805512151243735\n",
            "Epoch 133, Loss: 0.11257421225309372\n",
            "Epoch 134, Loss: 0.10904585891308224\n",
            "Epoch 135, Loss: 0.11643897595682315\n",
            "Epoch 136, Loss: 0.11666988290618269\n",
            "Epoch 137, Loss: 0.09421149533888211\n",
            "Epoch 138, Loss: 0.15008342014245538\n",
            "Epoch 139, Loss: 0.15355225985071488\n",
            "Epoch 140, Loss: 0.09894102338549433\n",
            "Epoch 141, Loss: 0.09036523389763065\n",
            "Epoch 142, Loss: 0.08568872530393455\n",
            "Epoch 143, Loss: 0.09962117330798385\n",
            "Epoch 144, Loss: 0.11369207754198994\n",
            "Epoch 145, Loss: 0.1494172464638036\n",
            "Epoch 146, Loss: 0.1409970673029216\n",
            "Epoch 147, Loss: 0.11023087596178663\n",
            "Epoch 148, Loss: 0.10417247078932669\n",
            "Epoch 149, Loss: 0.10965995340398987\n",
            "Epoch 150, Loss: 0.10263301859780842\n",
            "Epoch 151, Loss: 0.08658178683788496\n",
            "Epoch 152, Loss: 0.10896305888130957\n",
            "Epoch 153, Loss: 0.13893421502707867\n",
            "Epoch 154, Loss: 0.10650940286945931\n",
            "Epoch 155, Loss: 0.09923304758053653\n",
            "Epoch 156, Loss: 0.08208701614176436\n",
            "Epoch 157, Loss: 0.08114720890488551\n",
            "Epoch 158, Loss: 0.13389154529313044\n",
            "Epoch 159, Loss: 0.1204595805563945\n",
            "Epoch 160, Loss: 0.13361232457872557\n",
            "Epoch 161, Loss: 0.10966557710982706\n",
            "Epoch 162, Loss: 0.0832136692289187\n",
            "Epoch 163, Loss: 0.09049292457080921\n",
            "Epoch 164, Loss: 0.13587691986515205\n",
            "Epoch 165, Loss: 0.10920673468131192\n",
            "Epoch 166, Loss: 0.09467159419757676\n",
            "Epoch 167, Loss: 0.11227289224233554\n",
            "Epoch 168, Loss: 0.09465257943208728\n",
            "Epoch 169, Loss: 0.07899960617496803\n",
            "Epoch 170, Loss: 0.10362089144978293\n",
            "Epoch 171, Loss: 0.10614695622376641\n",
            "Epoch 172, Loss: 0.09250745721807589\n",
            "Epoch 173, Loss: 0.11634960662269471\n",
            "Epoch 174, Loss: 0.10908891458292397\n",
            "Epoch 175, Loss: 0.09862389004960352\n",
            "Epoch 176, Loss: 0.08998257977583883\n",
            "Epoch 177, Loss: 0.0758639886899262\n",
            "Epoch 178, Loss: 0.07587932594766726\n",
            "Epoch 179, Loss: 0.0979085154783893\n",
            "Epoch 180, Loss: 0.12554602220426408\n",
            "Epoch 181, Loss: 0.12726726669019886\n",
            "Epoch 182, Loss: 0.0908742626292669\n",
            "Epoch 183, Loss: 0.10084842977931305\n",
            "Epoch 184, Loss: 0.08724879328047438\n",
            "Epoch 185, Loss: 0.1018414010331795\n",
            "Epoch 186, Loss: 0.10901734470484817\n",
            "Epoch 187, Loss: 0.09843700329716108\n",
            "Epoch 188, Loss: 0.10476016810125842\n",
            "Epoch 189, Loss: 0.08052161963162374\n",
            "Epoch 190, Loss: 0.07813041108869473\n",
            "Epoch 191, Loss: 0.09225592650092986\n",
            "Epoch 192, Loss: 0.1179125760967026\n",
            "Epoch 193, Loss: 0.11130792203796458\n",
            "Epoch 194, Loss: 0.11604308994600968\n",
            "Epoch 195, Loss: 0.08323244159870154\n",
            "Epoch 196, Loss: 0.06433936807193927\n",
            "Epoch 197, Loss: 0.0657358506922515\n",
            "Epoch 198, Loss: 0.07928267803651337\n",
            "Epoch 199, Loss: 0.11066347615299177\n",
            "Accuracy: 34.47%\n",
            "Training with params: {'batch_size': 256, 'lr': 0.001, 'optimizer': 'SGD'}\n",
            "Epoch 1, Loss: 4.605869903856394\n",
            "Epoch 2, Loss: 4.605797045084895\n",
            "Epoch 3, Loss: 4.605681845120022\n",
            "Epoch 4, Loss: 4.605614287512643\n",
            "Epoch 5, Loss: 4.605551841307659\n",
            "Epoch 6, Loss: 4.605415030401581\n",
            "Epoch 7, Loss: 4.60532490817868\n",
            "Epoch 8, Loss: 4.6052479208732136\n",
            "Epoch 9, Loss: 4.605164289474487\n",
            "Epoch 10, Loss: 4.605070396345489\n",
            "Epoch 11, Loss: 4.604964523899312\n",
            "Epoch 12, Loss: 4.604877197012609\n",
            "Epoch 13, Loss: 4.604795190752769\n",
            "Epoch 14, Loss: 4.604699847649555\n",
            "Epoch 15, Loss: 4.6046135620195034\n",
            "Epoch 16, Loss: 4.604553740851733\n",
            "Epoch 17, Loss: 4.604441440835291\n",
            "Epoch 18, Loss: 4.604350564431171\n",
            "Epoch 19, Loss: 4.604266188582596\n",
            "Epoch 20, Loss: 4.604161880454239\n",
            "Epoch 21, Loss: 4.604088391576495\n",
            "Epoch 22, Loss: 4.603982433980825\n",
            "Epoch 23, Loss: 4.603907650830794\n",
            "Epoch 24, Loss: 4.603771017522228\n",
            "Epoch 25, Loss: 4.603710194023288\n",
            "Epoch 26, Loss: 4.603622237030341\n",
            "Epoch 27, Loss: 4.603519240204169\n",
            "Epoch 28, Loss: 4.603391571920746\n",
            "Epoch 29, Loss: 4.603269657310174\n",
            "Epoch 30, Loss: 4.603208074764329\n",
            "Epoch 31, Loss: 4.603075273182927\n",
            "Epoch 32, Loss: 4.602969853245482\n",
            "Epoch 33, Loss: 4.602839421252815\n",
            "Epoch 34, Loss: 4.602727160161855\n",
            "Epoch 35, Loss: 4.6026221197478625\n",
            "Epoch 36, Loss: 4.602480241230556\n",
            "Epoch 37, Loss: 4.602365328341114\n",
            "Epoch 38, Loss: 4.602217664524001\n",
            "Epoch 39, Loss: 4.602097917576225\n",
            "Epoch 40, Loss: 4.601954392024449\n",
            "Epoch 41, Loss: 4.601835097585406\n",
            "Epoch 42, Loss: 4.60166579849866\n",
            "Epoch 43, Loss: 4.601503294341418\n",
            "Epoch 44, Loss: 4.601367383587117\n",
            "Epoch 45, Loss: 4.601185358300501\n",
            "Epoch 46, Loss: 4.601032926111805\n",
            "Epoch 47, Loss: 4.600871312374971\n",
            "Epoch 48, Loss: 4.600665267632932\n",
            "Epoch 49, Loss: 4.600485813860991\n",
            "Epoch 50, Loss: 4.600293166783391\n",
            "Epoch 51, Loss: 4.600106523961437\n",
            "Epoch 52, Loss: 4.599906060160423\n",
            "Epoch 53, Loss: 4.599681418769213\n",
            "Epoch 54, Loss: 4.599466727704418\n",
            "Epoch 55, Loss: 4.5991932907883\n",
            "Epoch 56, Loss: 4.598940119451406\n",
            "Epoch 57, Loss: 4.598700136554484\n",
            "Epoch 58, Loss: 4.598462321320358\n",
            "Epoch 59, Loss: 4.598173226628985\n",
            "Epoch 60, Loss: 4.597893980084633\n",
            "Epoch 61, Loss: 4.597558478919828\n",
            "Epoch 62, Loss: 4.597250972475324\n",
            "Epoch 63, Loss: 4.596891142884079\n",
            "Epoch 64, Loss: 4.596558259457958\n",
            "Epoch 65, Loss: 4.596171717254483\n",
            "Epoch 66, Loss: 4.595810335509631\n",
            "Epoch 67, Loss: 4.595375048870943\n",
            "Epoch 68, Loss: 4.594936670089255\n",
            "Epoch 69, Loss: 4.594470048437313\n",
            "Epoch 70, Loss: 4.593957353611382\n",
            "Epoch 71, Loss: 4.593436829897822\n",
            "Epoch 72, Loss: 4.5928613701645205\n",
            "Epoch 73, Loss: 4.592271916720332\n",
            "Epoch 74, Loss: 4.591634110528595\n",
            "Epoch 75, Loss: 4.59098173890795\n",
            "Epoch 76, Loss: 4.590276474855384\n",
            "Epoch 77, Loss: 4.589552706601668\n",
            "Epoch 78, Loss: 4.588714261444247\n",
            "Epoch 79, Loss: 4.587839834544123\n",
            "Epoch 80, Loss: 4.586917028135183\n",
            "Epoch 81, Loss: 4.5858703535430285\n",
            "Epoch 82, Loss: 4.584810203435469\n",
            "Epoch 83, Loss: 4.583692107881818\n",
            "Epoch 84, Loss: 4.582433773546803\n",
            "Epoch 85, Loss: 4.581003517520671\n",
            "Epoch 86, Loss: 4.579582029459428\n",
            "Epoch 87, Loss: 4.577995864712462\n",
            "Epoch 88, Loss: 4.576195008900701\n",
            "Epoch 89, Loss: 4.574334448697615\n",
            "Epoch 90, Loss: 4.572300945009504\n",
            "Epoch 91, Loss: 4.570065082335959\n",
            "Epoch 92, Loss: 4.567526123961624\n",
            "Epoch 93, Loss: 4.56492801831693\n",
            "Epoch 94, Loss: 4.56195883604945\n",
            "Epoch 95, Loss: 4.558813980647495\n",
            "Epoch 96, Loss: 4.555190349111752\n",
            "Epoch 97, Loss: 4.551597943111342\n",
            "Epoch 98, Loss: 4.547472238540649\n",
            "Epoch 99, Loss: 4.542977398755599\n",
            "Epoch 100, Loss: 4.538089382405183\n",
            "Epoch 101, Loss: 4.532791081739932\n",
            "Epoch 102, Loss: 4.527117074752341\n",
            "Epoch 103, Loss: 4.520806475561493\n",
            "Epoch 104, Loss: 4.514217707575584\n",
            "Epoch 105, Loss: 4.506863907891876\n",
            "Epoch 106, Loss: 4.498748947163017\n",
            "Epoch 107, Loss: 4.490442278433819\n",
            "Epoch 108, Loss: 4.48119914774992\n",
            "Epoch 109, Loss: 4.471644871088923\n",
            "Epoch 110, Loss: 4.4610912799835205\n",
            "Epoch 111, Loss: 4.4498862003793525\n",
            "Epoch 112, Loss: 4.438055349856007\n",
            "Epoch 113, Loss: 4.425283249543638\n",
            "Epoch 114, Loss: 4.412221799091417\n",
            "Epoch 115, Loss: 4.399058429562316\n",
            "Epoch 116, Loss: 4.384828440997065\n",
            "Epoch 117, Loss: 4.369640014609512\n",
            "Epoch 118, Loss: 4.355268166989696\n",
            "Epoch 119, Loss: 4.340235576337697\n",
            "Epoch 120, Loss: 4.325525115947334\n",
            "Epoch 121, Loss: 4.310438355621026\n",
            "Epoch 122, Loss: 4.295752303940909\n",
            "Epoch 123, Loss: 4.280575107555\n",
            "Epoch 124, Loss: 4.266281741006034\n",
            "Epoch 125, Loss: 4.251568920758306\n",
            "Epoch 126, Loss: 4.237009573955925\n",
            "Epoch 127, Loss: 4.223204357283456\n",
            "Epoch 128, Loss: 4.209266071416894\n",
            "Epoch 129, Loss: 4.195539503681417\n",
            "Epoch 130, Loss: 4.18056376369632\n",
            "Epoch 131, Loss: 4.166510581970215\n",
            "Epoch 132, Loss: 4.153249910899571\n",
            "Epoch 133, Loss: 4.139374375343323\n",
            "Epoch 134, Loss: 4.125550000035033\n",
            "Epoch 135, Loss: 4.112432562575048\n",
            "Epoch 136, Loss: 4.098719936244342\n",
            "Epoch 137, Loss: 4.085793956201904\n",
            "Epoch 138, Loss: 4.072811451493477\n",
            "Epoch 139, Loss: 4.059657557886474\n",
            "Epoch 140, Loss: 4.046758081231799\n",
            "Epoch 141, Loss: 4.034883966251296\n",
            "Epoch 142, Loss: 4.023649353153852\n",
            "Epoch 143, Loss: 4.011697119596053\n",
            "Epoch 144, Loss: 3.99971221539439\n",
            "Epoch 145, Loss: 3.98845811644379\n",
            "Epoch 146, Loss: 3.977898838568707\n",
            "Epoch 147, Loss: 3.966593181600376\n",
            "Epoch 148, Loss: 3.956207268092097\n",
            "Epoch 149, Loss: 3.9445435696718643\n",
            "Epoch 150, Loss: 3.936014334766232\n",
            "Epoch 151, Loss: 3.927326892103468\n",
            "Epoch 152, Loss: 3.917506585315782\n",
            "Epoch 153, Loss: 3.907530656882695\n",
            "Epoch 154, Loss: 3.899253632341112\n",
            "Epoch 155, Loss: 3.89081956172476\n",
            "Epoch 156, Loss: 3.88167007601991\n",
            "Epoch 157, Loss: 3.8733479538742377\n",
            "Epoch 158, Loss: 3.865485422465266\n",
            "Epoch 159, Loss: 3.8566124573045846\n",
            "Epoch 160, Loss: 3.8489275647669423\n",
            "Epoch 161, Loss: 3.8414213134317983\n",
            "Epoch 162, Loss: 3.833641821024369\n",
            "Epoch 163, Loss: 3.82635975613886\n",
            "Epoch 164, Loss: 3.8186764425160935\n",
            "Epoch 165, Loss: 3.811189225741795\n",
            "Epoch 166, Loss: 3.802920801298959\n",
            "Epoch 167, Loss: 3.7952401066312986\n",
            "Epoch 168, Loss: 3.788318923541478\n",
            "Epoch 169, Loss: 3.781314346255088\n",
            "Epoch 170, Loss: 3.773495709409519\n",
            "Epoch 171, Loss: 3.766784418602379\n",
            "Epoch 172, Loss: 3.7586839138245094\n",
            "Epoch 173, Loss: 3.751331794018648\n",
            "Epoch 174, Loss: 3.7444385825371254\n",
            "Epoch 175, Loss: 3.736286064799951\n",
            "Epoch 176, Loss: 3.730173197327828\n",
            "Epoch 177, Loss: 3.7225814291409085\n",
            "Epoch 178, Loss: 3.715151706520392\n",
            "Epoch 179, Loss: 3.7084995800135085\n",
            "Epoch 180, Loss: 3.700572843454322\n",
            "Epoch 181, Loss: 3.6938520767250838\n",
            "Epoch 182, Loss: 3.6873662861026064\n",
            "Epoch 183, Loss: 3.6806980931029027\n",
            "Epoch 184, Loss: 3.6742324269547755\n",
            "Epoch 185, Loss: 3.668072441402747\n",
            "Epoch 186, Loss: 3.661409303850057\n",
            "Epoch 187, Loss: 3.6547665425709317\n",
            "Epoch 188, Loss: 3.648354773618737\n",
            "Epoch 189, Loss: 3.642289909781242\n",
            "Epoch 190, Loss: 3.6364783973109964\n",
            "Epoch 191, Loss: 3.630979412672471\n",
            "Epoch 192, Loss: 3.624153654186093\n",
            "Epoch 193, Loss: 3.6190148300054124\n",
            "Epoch 194, Loss: 3.611979601334552\n",
            "Epoch 195, Loss: 3.6071464297722797\n",
            "Epoch 196, Loss: 3.600086045508482\n",
            "Epoch 197, Loss: 3.595795916051281\n",
            "Epoch 198, Loss: 3.589509035859789\n",
            "Epoch 199, Loss: 3.5844939229439716\n",
            "Accuracy: 16.17%\n",
            "Training with params: {'batch_size': 256, 'lr': 0.001, 'optimizer': 'RMSprop'}\n",
            "Epoch 1, Loss: 3.9137179973174114\n",
            "Epoch 2, Loss: 3.2821148208209445\n",
            "Epoch 3, Loss: 2.990856711961785\n",
            "Epoch 4, Loss: 2.7868013369793796\n",
            "Epoch 5, Loss: 2.627134469090676\n",
            "Epoch 6, Loss: 2.5041290533785916\n",
            "Epoch 7, Loss: 2.3804726016764737\n",
            "Epoch 8, Loss: 2.28423888221079\n",
            "Epoch 9, Loss: 2.1894923691846886\n",
            "Epoch 10, Loss: 2.104779454518338\n",
            "Epoch 11, Loss: 2.026493098662824\n",
            "Epoch 12, Loss: 1.9426296608788627\n",
            "Epoch 13, Loss: 1.871653020990138\n",
            "Epoch 14, Loss: 1.8056781894090224\n",
            "Epoch 15, Loss: 1.7231214186366723\n",
            "Epoch 16, Loss: 1.6644159379054089\n",
            "Epoch 17, Loss: 1.6011838468970085\n",
            "Epoch 18, Loss: 1.5354137402407977\n",
            "Epoch 19, Loss: 1.4618811424897642\n",
            "Epoch 20, Loss: 1.4178053110229725\n",
            "Epoch 21, Loss: 1.3543007629258292\n",
            "Epoch 22, Loss: 1.2973950146412363\n",
            "Epoch 23, Loss: 1.2427739744283715\n",
            "Epoch 24, Loss: 1.194302681453374\n",
            "Epoch 25, Loss: 1.1354800100837434\n",
            "Epoch 26, Loss: 1.0859259357865976\n",
            "Epoch 27, Loss: 1.044966312087312\n",
            "Epoch 28, Loss: 0.9847950792434265\n",
            "Epoch 29, Loss: 0.9420157011066165\n",
            "Epoch 30, Loss: 0.8940327173593093\n",
            "Epoch 31, Loss: 0.8624360619150863\n",
            "Epoch 32, Loss: 0.8024413999246092\n",
            "Epoch 33, Loss: 0.77364595629731\n",
            "Epoch 34, Loss: 0.7284654342398351\n",
            "Epoch 35, Loss: 0.6931611214365277\n",
            "Epoch 36, Loss: 0.6584814107235597\n",
            "Epoch 37, Loss: 0.6284352478628256\n",
            "Epoch 38, Loss: 0.5859040382261179\n",
            "Epoch 39, Loss: 0.5666253638206696\n",
            "Epoch 40, Loss: 0.5296118016145668\n",
            "Epoch 41, Loss: 0.501485003348516\n",
            "Epoch 42, Loss: 0.47113528406741667\n",
            "Epoch 43, Loss: 0.45660116477888457\n",
            "Epoch 44, Loss: 0.42464569461893065\n",
            "Epoch 45, Loss: 0.3942917355013137\n",
            "Epoch 46, Loss: 0.3926643154450825\n",
            "Epoch 47, Loss: 0.36350526789925536\n",
            "Epoch 48, Loss: 0.34424773938193615\n",
            "Epoch 49, Loss: 0.3233746120668187\n",
            "Epoch 50, Loss: 0.31301122227189493\n",
            "Epoch 51, Loss: 0.3070563861758125\n",
            "Epoch 52, Loss: 0.28378563549141494\n",
            "Epoch 53, Loss: 0.2658117641599811\n",
            "Epoch 54, Loss: 0.2617790463323496\n",
            "Epoch 55, Loss: 0.2579577310885094\n",
            "Epoch 56, Loss: 0.22282299536223316\n",
            "Epoch 57, Loss: 0.25859845844002405\n",
            "Epoch 58, Loss: 0.2426297093684576\n",
            "Epoch 59, Loss: 0.21290641192498863\n",
            "Epoch 60, Loss: 0.2128268264005987\n",
            "Epoch 61, Loss: 0.2025257660722246\n",
            "Epoch 62, Loss: 0.21896092312372462\n",
            "Epoch 63, Loss: 0.19262277618126603\n",
            "Epoch 64, Loss: 0.19174684462498645\n",
            "Epoch 65, Loss: 0.18694919233723561\n",
            "Epoch 66, Loss: 0.1901254816627016\n",
            "Epoch 67, Loss: 0.18385223365787948\n",
            "Epoch 68, Loss: 0.1975888909239854\n",
            "Epoch 69, Loss: 0.17626751197159898\n",
            "Epoch 70, Loss: 0.1600877025107644\n",
            "Epoch 71, Loss: 0.18033873142522513\n",
            "Epoch 72, Loss: 0.15292837523988315\n",
            "Epoch 73, Loss: 0.16481127213610677\n",
            "Epoch 74, Loss: 0.16055170916096898\n",
            "Epoch 75, Loss: 0.15833999699323761\n",
            "Epoch 76, Loss: 0.18103982981446445\n",
            "Epoch 77, Loss: 0.15079024826575602\n",
            "Epoch 78, Loss: 0.13159089166747065\n",
            "Epoch 79, Loss: 0.1502860824824596\n",
            "Epoch 80, Loss: 0.12813556960270722\n",
            "Epoch 81, Loss: 0.17449025039998245\n",
            "Epoch 82, Loss: 0.13404822893136617\n",
            "Epoch 83, Loss: 0.14182279405317136\n",
            "Epoch 84, Loss: 0.1251360576751889\n",
            "Epoch 85, Loss: 0.14364924946115637\n",
            "Epoch 86, Loss: 0.13369587134113725\n",
            "Epoch 87, Loss: 0.14168842178674376\n",
            "Epoch 88, Loss: 0.10784595278186762\n",
            "Epoch 89, Loss: 0.14349456443166247\n",
            "Epoch 90, Loss: 0.1314373947965095\n",
            "Epoch 91, Loss: 0.16502474354846136\n",
            "Epoch 92, Loss: 0.10091126990998733\n",
            "Epoch 93, Loss: 0.13322762839914282\n",
            "Epoch 94, Loss: 0.15746443097631693\n",
            "Epoch 95, Loss: 0.11784335164524311\n",
            "Epoch 96, Loss: 0.1276610642148904\n",
            "Epoch 97, Loss: 0.1308666632358669\n",
            "Epoch 98, Loss: 0.10546421083830754\n",
            "Epoch 99, Loss: 0.1384310786878424\n",
            "Epoch 100, Loss: 0.11132692417833118\n",
            "Epoch 101, Loss: 0.11071442814106691\n",
            "Epoch 102, Loss: 0.10099932392679002\n",
            "Epoch 103, Loss: 0.11987549311728501\n",
            "Epoch 104, Loss: 0.10258158397081556\n",
            "Epoch 105, Loss: 0.10548863243501709\n",
            "Epoch 106, Loss: 0.12201890643990161\n",
            "Epoch 107, Loss: 0.09413921932822891\n",
            "Epoch 108, Loss: 0.10516188274688866\n",
            "Epoch 109, Loss: 0.12169700139677342\n",
            "Epoch 110, Loss: 0.14071784442651789\n",
            "Epoch 111, Loss: 0.11672941485790497\n",
            "Epoch 112, Loss: 0.09527449267061085\n",
            "Epoch 113, Loss: 0.09980407974929834\n",
            "Epoch 114, Loss: 0.09804693559108645\n",
            "Epoch 115, Loss: 0.11592890880526785\n",
            "Epoch 116, Loss: 0.10475228853732804\n",
            "Epoch 117, Loss: 0.09559114371920574\n",
            "Epoch 118, Loss: 0.08690578935249728\n",
            "Epoch 119, Loss: 0.10744255667609372\n",
            "Epoch 120, Loss: 0.08680761445845876\n",
            "Epoch 121, Loss: 0.10759001410547264\n",
            "Epoch 122, Loss: 0.13137143419589847\n",
            "Epoch 123, Loss: 0.1091615093672382\n",
            "Epoch 124, Loss: 0.09294239749979911\n",
            "Epoch 125, Loss: 0.16068907805756494\n",
            "Epoch 126, Loss: 0.08360753564297088\n",
            "Epoch 127, Loss: 0.08038293010061037\n",
            "Epoch 128, Loss: 0.0921496844669918\n",
            "Epoch 129, Loss: 0.10554253249619232\n",
            "Epoch 130, Loss: 0.08934877223183153\n",
            "Epoch 131, Loss: 0.08565363562570846\n",
            "Epoch 132, Loss: 0.09166189079762113\n",
            "Epoch 133, Loss: 0.07774222513413703\n",
            "Epoch 134, Loss: 0.10100479123220608\n",
            "Epoch 135, Loss: 0.09145596049897069\n",
            "Epoch 136, Loss: 0.08214773660601705\n",
            "Epoch 137, Loss: 0.07501243765237836\n",
            "Epoch 138, Loss: 0.09280962281271207\n",
            "Epoch 139, Loss: 0.12163668586539902\n",
            "Epoch 140, Loss: 0.07694588326231329\n",
            "Epoch 141, Loss: 0.08425690668958183\n",
            "Epoch 142, Loss: 0.09880662237160973\n",
            "Epoch 143, Loss: 0.09219868044482962\n",
            "Epoch 144, Loss: 0.11593848201731334\n",
            "Epoch 145, Loss: 0.08440005428357315\n",
            "Epoch 146, Loss: 0.08007377735695478\n",
            "Epoch 147, Loss: 0.07339550823220337\n",
            "Epoch 148, Loss: 0.08443215145843522\n",
            "Epoch 149, Loss: 0.10350809069065263\n",
            "Epoch 150, Loss: 0.0745451673627736\n",
            "Epoch 151, Loss: 0.10401920800819536\n",
            "Epoch 152, Loss: 0.0782591260955384\n",
            "Epoch 153, Loss: 0.08118226414318291\n",
            "Epoch 154, Loss: 0.08002291160293531\n",
            "Epoch 155, Loss: 0.07959099685089016\n",
            "Epoch 156, Loss: 0.08236124099004177\n",
            "Epoch 157, Loss: 0.09025470968050768\n",
            "Epoch 158, Loss: 0.12043026970148239\n",
            "Epoch 159, Loss: 0.0858985392343519\n",
            "Epoch 160, Loss: 0.0824223344180999\n",
            "Epoch 161, Loss: 0.07843834133248548\n",
            "Epoch 162, Loss: 0.07525860465948983\n",
            "Epoch 163, Loss: 0.07825002927637222\n",
            "Epoch 164, Loss: 0.07792078535908795\n",
            "Epoch 165, Loss: 0.07902622051841142\n",
            "Epoch 166, Loss: 0.07335064848124677\n",
            "Epoch 167, Loss: 0.08196197564200479\n",
            "Epoch 168, Loss: 0.07039415037582572\n",
            "Epoch 169, Loss: 0.07766173236376168\n",
            "Epoch 170, Loss: 0.0752929299452095\n",
            "Epoch 171, Loss: 0.0713605722935148\n",
            "Epoch 172, Loss: 0.1022395018122292\n",
            "Epoch 173, Loss: 0.06732945054726751\n",
            "Epoch 174, Loss: 0.07508461836402361\n",
            "Epoch 175, Loss: 0.09930858020765745\n",
            "Epoch 176, Loss: 0.06565536910249871\n",
            "Epoch 177, Loss: 0.07831172207940598\n",
            "Epoch 178, Loss: 0.06784377100506836\n",
            "Epoch 179, Loss: 0.0783835798668276\n",
            "Epoch 180, Loss: 0.08915121489613108\n",
            "Epoch 181, Loss: 0.06889690563073192\n",
            "Epoch 182, Loss: 0.06946893593258395\n",
            "Epoch 183, Loss: 0.07210983248422759\n",
            "Epoch 184, Loss: 0.06814344181698195\n",
            "Epoch 185, Loss: 0.08933202344781663\n",
            "Epoch 186, Loss: 0.0678291555199468\n",
            "Epoch 187, Loss: 0.07364284735153981\n",
            "Epoch 188, Loss: 0.06882236771789209\n",
            "Epoch 189, Loss: 0.06771324488942568\n",
            "Epoch 190, Loss: 0.07612519583674338\n",
            "Epoch 191, Loss: 0.06250914687062709\n",
            "Epoch 192, Loss: 0.07319955066872799\n",
            "Epoch 193, Loss: 0.0705937036522189\n",
            "Epoch 194, Loss: 0.07883597892347952\n",
            "Epoch 195, Loss: 0.06365205992573911\n",
            "Epoch 196, Loss: 0.08437648035908993\n",
            "Epoch 197, Loss: 0.065133303033226\n",
            "Epoch 198, Loss: 0.06893248574770226\n",
            "Epoch 199, Loss: 0.06966161547342734\n",
            "Accuracy: 34.48%\n",
            "Training with params: {'batch_size': 256, 'lr': 0.005, 'optimizer': 'Adam'}\n",
            "Epoch 1, Loss: 3.7911226822405446\n",
            "Epoch 2, Loss: 3.07898482011289\n",
            "Epoch 3, Loss: 2.775600968574991\n",
            "Epoch 4, Loss: 2.5719143821268666\n",
            "Epoch 5, Loss: 2.405092718649884\n",
            "Epoch 6, Loss: 2.282044399149564\n",
            "Epoch 7, Loss: 2.1713888134275163\n",
            "Epoch 8, Loss: 2.0862548813527946\n",
            "Epoch 9, Loss: 2.017234503614659\n",
            "Epoch 10, Loss: 1.9327904630680472\n",
            "Epoch 11, Loss: 1.8667662484305245\n",
            "Epoch 12, Loss: 1.8105074313222145\n",
            "Epoch 13, Loss: 1.7544150577515971\n",
            "Epoch 14, Loss: 1.7177162121753304\n",
            "Epoch 15, Loss: 1.6678365791330532\n",
            "Epoch 16, Loss: 1.6505921160688206\n",
            "Epoch 17, Loss: 1.5958442900862013\n",
            "Epoch 18, Loss: 1.5745117208179162\n",
            "Epoch 19, Loss: 1.5376755716849346\n",
            "Epoch 20, Loss: 1.5138841307893092\n",
            "Epoch 21, Loss: 1.48302104643413\n",
            "Epoch 22, Loss: 1.4557748132822466\n",
            "Epoch 23, Loss: 1.425071976014546\n",
            "Epoch 24, Loss: 1.3985393460916014\n",
            "Epoch 25, Loss: 1.3889359661511012\n",
            "Epoch 26, Loss: 1.357908594669128\n",
            "Epoch 27, Loss: 1.35588459062333\n",
            "Epoch 28, Loss: 1.3256453667976418\n",
            "Epoch 29, Loss: 1.3232786296581736\n",
            "Epoch 30, Loss: 1.284446810581246\n",
            "Epoch 31, Loss: 1.269703329825888\n",
            "Epoch 32, Loss: 1.2530508363733486\n",
            "Epoch 33, Loss: 1.2193945615875477\n",
            "Epoch 34, Loss: 1.2132904146398817\n",
            "Epoch 35, Loss: 1.2171131746501338\n",
            "Epoch 36, Loss: 1.187723706267318\n",
            "Epoch 37, Loss: 1.1887961939281346\n",
            "Epoch 38, Loss: 1.1761141124428536\n",
            "Epoch 39, Loss: 1.1705541917864157\n",
            "Epoch 40, Loss: 1.153575785914246\n",
            "Epoch 41, Loss: 1.1633834026905956\n",
            "Epoch 42, Loss: 1.120556852343131\n",
            "Epoch 43, Loss: 1.104002704425734\n",
            "Epoch 44, Loss: 1.1073066671283878\n",
            "Epoch 45, Loss: 1.0882599274722897\n",
            "Epoch 46, Loss: 1.0982704573139852\n",
            "Epoch 47, Loss: 1.0612717307343775\n",
            "Epoch 48, Loss: 1.0886199623346329\n",
            "Epoch 49, Loss: 1.0570947987084487\n",
            "Epoch 50, Loss: 1.0543607722739785\n",
            "Epoch 51, Loss: 1.0411693213545545\n",
            "Epoch 52, Loss: 1.0352005432454907\n",
            "Epoch 53, Loss: 1.0460077627580993\n",
            "Epoch 54, Loss: 1.036823289126766\n",
            "Epoch 55, Loss: 1.0261219116498013\n",
            "Epoch 56, Loss: 1.0085850257654578\n",
            "Epoch 57, Loss: 1.0014861819087242\n",
            "Epoch 58, Loss: 1.016155718844764\n",
            "Epoch 59, Loss: 1.0033470894000969\n",
            "Epoch 60, Loss: 0.9925211269636544\n",
            "Epoch 61, Loss: 0.9794934635259667\n",
            "Epoch 62, Loss: 0.9777158845444115\n",
            "Epoch 63, Loss: 0.9602059770603569\n",
            "Epoch 64, Loss: 0.9818476873392962\n",
            "Epoch 65, Loss: 0.9759482978558054\n",
            "Epoch 66, Loss: 0.9512546518627478\n",
            "Epoch 67, Loss: 0.9641405240613588\n",
            "Epoch 68, Loss: 0.934007563457197\n",
            "Epoch 69, Loss: 0.9188157286570997\n",
            "Epoch 70, Loss: 0.9278812229025121\n",
            "Epoch 71, Loss: 0.9284007065758413\n",
            "Epoch 72, Loss: 0.9827745714966132\n",
            "Epoch 73, Loss: 0.9465696282532751\n",
            "Epoch 74, Loss: 0.9362186360724118\n",
            "Epoch 75, Loss: 0.8955194974432186\n",
            "Epoch 76, Loss: 0.9341187267279139\n",
            "Epoch 77, Loss: 0.902897738984653\n",
            "Epoch 78, Loss: 0.906387804418194\n",
            "Epoch 79, Loss: 0.9161035509742036\n",
            "Epoch 80, Loss: 0.9236668159767073\n",
            "Epoch 81, Loss: 0.919514497323912\n",
            "Epoch 82, Loss: 0.9064407306058067\n",
            "Epoch 83, Loss: 0.8939921224907953\n",
            "Epoch 84, Loss: 0.887319666086411\n",
            "Epoch 85, Loss: 0.8587974045349627\n",
            "Epoch 86, Loss: 0.8934973627328873\n",
            "Epoch 87, Loss: 0.8781890209232058\n",
            "Epoch 88, Loss: 0.8482398682711075\n",
            "Epoch 89, Loss: 0.8911080640189502\n",
            "Epoch 90, Loss: 0.871194486107145\n",
            "Epoch 91, Loss: 0.8724425714843127\n",
            "Epoch 92, Loss: 0.9153413471518731\n",
            "Epoch 93, Loss: 0.9014789103245249\n",
            "Epoch 94, Loss: 0.8872121964790383\n",
            "Epoch 95, Loss: 0.8539803949545841\n",
            "Epoch 96, Loss: 0.8276702731239552\n",
            "Epoch 97, Loss: 0.8356721133601909\n",
            "Epoch 98, Loss: 0.8630157003597337\n",
            "Epoch 99, Loss: 0.8700188656850737\n",
            "Epoch 100, Loss: 0.8490318111619171\n",
            "Epoch 101, Loss: 0.8605767689189132\n",
            "Epoch 102, Loss: 0.8579422630825821\n",
            "Epoch 103, Loss: 0.8032250983678565\n",
            "Epoch 104, Loss: 0.8157971486145136\n",
            "Epoch 105, Loss: 0.8241127224601045\n",
            "Epoch 106, Loss: 0.8220809235864756\n",
            "Epoch 107, Loss: 0.8144540342749381\n",
            "Epoch 108, Loss: 0.820681672011103\n",
            "Epoch 109, Loss: 0.8147305009924636\n",
            "Epoch 110, Loss: 0.8000149285914947\n",
            "Epoch 111, Loss: 0.8548352149676304\n",
            "Epoch 112, Loss: 0.8380847591526654\n",
            "Epoch 113, Loss: 0.7980447606164582\n",
            "Epoch 114, Loss: 0.7791246835674558\n",
            "Epoch 115, Loss: 0.8081200814672879\n",
            "Epoch 116, Loss: 0.8657904531882734\n",
            "Epoch 117, Loss: 0.844498794601888\n",
            "Epoch 118, Loss: 0.7720449953054895\n",
            "Epoch 119, Loss: 0.806758736773413\n",
            "Epoch 120, Loss: 0.782012225869967\n",
            "Epoch 121, Loss: 0.8330685289538636\n",
            "Epoch 122, Loss: 0.8193448094689116\n",
            "Epoch 123, Loss: 0.7911875220585842\n",
            "Epoch 124, Loss: 0.8462551315220035\n",
            "Epoch 125, Loss: 0.7656750773288765\n",
            "Epoch 126, Loss: 0.7956877223082951\n",
            "Epoch 127, Loss: 0.7666821002351994\n",
            "Epoch 128, Loss: 0.7825556780610766\n",
            "Epoch 129, Loss: 0.8497131588507671\n",
            "Epoch 130, Loss: 0.8432324425298341\n",
            "Epoch 131, Loss: 0.7565564215183258\n",
            "Epoch 132, Loss: 0.7545255754066973\n",
            "Epoch 133, Loss: 0.813633816582816\n",
            "Epoch 134, Loss: 0.7975179683797213\n",
            "Epoch 135, Loss: 0.8020481765270233\n",
            "Epoch 136, Loss: 0.8116765539256894\n",
            "Epoch 137, Loss: 0.7919504639445519\n",
            "Epoch 138, Loss: 0.7585446164011955\n",
            "Epoch 139, Loss: 0.7962164513918818\n",
            "Epoch 140, Loss: 0.7864753074791967\n",
            "Epoch 141, Loss: 0.7992172822052118\n",
            "Epoch 142, Loss: 0.7848840690388972\n",
            "Epoch 143, Loss: 0.7719802362274151\n",
            "Epoch 144, Loss: 0.7764304838314349\n",
            "Epoch 145, Loss: 0.77100207261285\n",
            "Epoch 146, Loss: 0.7609682352263101\n",
            "Epoch 147, Loss: 0.7732738785597743\n",
            "Epoch 148, Loss: 0.7611742244691265\n",
            "Epoch 149, Loss: 0.7811265399261397\n",
            "Epoch 150, Loss: 0.782111334101278\n",
            "Epoch 151, Loss: 0.7393937241666171\n",
            "Epoch 152, Loss: 0.7567900881475332\n",
            "Epoch 153, Loss: 0.7722711105431829\n",
            "Epoch 154, Loss: 0.8066525161266327\n",
            "Epoch 155, Loss: 0.809351249312868\n",
            "Epoch 156, Loss: 0.766159712355964\n",
            "Epoch 157, Loss: 0.7510441383536981\n",
            "Epoch 158, Loss: 0.7605482459980615\n",
            "Epoch 159, Loss: 0.7661927751436526\n",
            "Epoch 160, Loss: 0.8049338937413936\n",
            "Epoch 161, Loss: 0.747817122662554\n",
            "Epoch 162, Loss: 0.7458593113994112\n",
            "Epoch 163, Loss: 0.7378063904387611\n",
            "Epoch 164, Loss: 0.7638740156378064\n",
            "Epoch 165, Loss: 0.7715078532999876\n",
            "Epoch 166, Loss: 0.7596359832250342\n",
            "Epoch 167, Loss: 0.7676690707401354\n",
            "Epoch 168, Loss: 0.7507183240080366\n",
            "Epoch 169, Loss: 0.7711938794474212\n",
            "Epoch 170, Loss: 0.7705974642719541\n",
            "Epoch 171, Loss: 0.7629554514982262\n",
            "Epoch 172, Loss: 0.6759849433996239\n",
            "Epoch 173, Loss: 0.7610815582530839\n",
            "Epoch 174, Loss: 0.7217206038078483\n",
            "Epoch 175, Loss: 0.8058058718333438\n",
            "Epoch 176, Loss: 0.8145034626430395\n",
            "Epoch 177, Loss: 0.7253789897171818\n",
            "Epoch 178, Loss: 0.7170646493228114\n",
            "Epoch 179, Loss: 0.7168725055395341\n",
            "Epoch 180, Loss: 0.7247477435335821\n",
            "Epoch 181, Loss: 0.7519779643233941\n",
            "Epoch 182, Loss: 0.7137961489509563\n",
            "Epoch 183, Loss: 0.7341755573846855\n",
            "Epoch 184, Loss: 0.7761156473840986\n",
            "Epoch 185, Loss: 0.8170320897990343\n",
            "Epoch 186, Loss: 0.6833399049177462\n",
            "Epoch 187, Loss: 0.729975585426603\n",
            "Epoch 188, Loss: 0.7296768089338225\n",
            "Epoch 189, Loss: 0.7356739396951637\n",
            "Epoch 190, Loss: 0.7344545788910924\n",
            "Epoch 191, Loss: 0.7858110850258749\n",
            "Epoch 192, Loss: 0.7300452588468181\n",
            "Epoch 193, Loss: 0.7403949972014038\n",
            "Epoch 194, Loss: 0.7523544924903889\n",
            "Epoch 195, Loss: 0.7065179902984171\n",
            "Epoch 196, Loss: 0.759494418392376\n",
            "Epoch 197, Loss: 0.6991560541245402\n",
            "Epoch 198, Loss: 0.7773675044276276\n",
            "Epoch 199, Loss: 0.7277745394682398\n",
            "Accuracy: 27.53%\n",
            "Training with params: {'batch_size': 256, 'lr': 0.005, 'optimizer': 'SGD'}\n",
            "Epoch 1, Loss: 4.605898842519643\n",
            "Epoch 2, Loss: 4.605441803834876\n",
            "Epoch 3, Loss: 4.604900090061888\n",
            "Epoch 4, Loss: 4.604425929030594\n",
            "Epoch 5, Loss: 4.603905699690994\n",
            "Epoch 6, Loss: 4.603332835800794\n",
            "Epoch 7, Loss: 4.602630240576608\n",
            "Epoch 8, Loss: 4.6018050811728655\n",
            "Epoch 9, Loss: 4.600863877607852\n",
            "Epoch 10, Loss: 4.599582119863861\n",
            "Epoch 11, Loss: 4.597914805217665\n",
            "Epoch 12, Loss: 4.595547157890943\n",
            "Epoch 13, Loss: 4.5921531064169745\n",
            "Epoch 14, Loss: 4.586983001962\n",
            "Epoch 15, Loss: 4.579117256767896\n",
            "Epoch 16, Loss: 4.567985432488578\n",
            "Epoch 17, Loss: 4.553437792524999\n",
            "Epoch 18, Loss: 4.53409642102767\n",
            "Epoch 19, Loss: 4.5067474282517725\n",
            "Epoch 20, Loss: 4.467319121166152\n",
            "Epoch 21, Loss: 4.423570800800713\n",
            "Epoch 22, Loss: 4.384876667236795\n",
            "Epoch 23, Loss: 4.3522898542637725\n",
            "Epoch 24, Loss: 4.321936714405916\n",
            "Epoch 25, Loss: 4.292495155821041\n",
            "Epoch 26, Loss: 4.2626160723822455\n",
            "Epoch 27, Loss: 4.232558702935978\n",
            "Epoch 28, Loss: 4.203509437794588\n",
            "Epoch 29, Loss: 4.173126824048101\n",
            "Epoch 30, Loss: 4.140186643113895\n",
            "Epoch 31, Loss: 4.101471291512859\n",
            "Epoch 32, Loss: 4.054735580269171\n",
            "Epoch 33, Loss: 4.006679148090129\n",
            "Epoch 34, Loss: 3.962433465889522\n",
            "Epoch 35, Loss: 3.9258585401943753\n",
            "Epoch 36, Loss: 3.89290745525944\n",
            "Epoch 37, Loss: 3.8645304283317254\n",
            "Epoch 38, Loss: 3.837668303324252\n",
            "Epoch 39, Loss: 3.8112008583788968\n",
            "Epoch 40, Loss: 3.7846867259667842\n",
            "Epoch 41, Loss: 3.756973073190572\n",
            "Epoch 42, Loss: 3.7299266153452346\n",
            "Epoch 43, Loss: 3.702022849297037\n",
            "Epoch 44, Loss: 3.6728569938212026\n",
            "Epoch 45, Loss: 3.6466290987267787\n",
            "Epoch 46, Loss: 3.618451997941854\n",
            "Epoch 47, Loss: 3.5906751326152255\n",
            "Epoch 48, Loss: 3.56268388762766\n",
            "Epoch 49, Loss: 3.5379119053178902\n",
            "Epoch 50, Loss: 3.513681321727986\n",
            "Epoch 51, Loss: 3.49241348553677\n",
            "Epoch 52, Loss: 3.4706905374721604\n",
            "Epoch 53, Loss: 3.450031286599685\n",
            "Epoch 54, Loss: 3.4286982940167796\n",
            "Epoch 55, Loss: 3.4117191470399195\n",
            "Epoch 56, Loss: 3.394961016518729\n",
            "Epoch 57, Loss: 3.3791863735841243\n",
            "Epoch 58, Loss: 3.3612977874522305\n",
            "Epoch 59, Loss: 3.34411128808041\n",
            "Epoch 60, Loss: 3.330363850204312\n",
            "Epoch 61, Loss: 3.3157332101646735\n",
            "Epoch 62, Loss: 3.298859880895031\n",
            "Epoch 63, Loss: 3.2811999953522974\n",
            "Epoch 64, Loss: 3.2677140637319915\n",
            "Epoch 65, Loss: 3.2514119780793482\n",
            "Epoch 66, Loss: 3.2356526194786537\n",
            "Epoch 67, Loss: 3.219631466330314\n",
            "Epoch 68, Loss: 3.20462235382625\n",
            "Epoch 69, Loss: 3.189465207712991\n",
            "Epoch 70, Loss: 3.173690030769426\n",
            "Epoch 71, Loss: 3.1591808747272103\n",
            "Epoch 72, Loss: 3.140810859446623\n",
            "Epoch 73, Loss: 3.1240785146246153\n",
            "Epoch 74, Loss: 3.1047352126666476\n",
            "Epoch 75, Loss: 3.0866864846677196\n",
            "Epoch 76, Loss: 3.0661506251412995\n",
            "Epoch 77, Loss: 3.047772976816917\n",
            "Epoch 78, Loss: 3.033775733441723\n",
            "Epoch 79, Loss: 3.0131053717768923\n",
            "Epoch 80, Loss: 2.9959619702125084\n",
            "Epoch 81, Loss: 2.980217389914454\n",
            "Epoch 82, Loss: 2.963758421187498\n",
            "Epoch 83, Loss: 2.9499965516888365\n",
            "Epoch 84, Loss: 2.9344564189716262\n",
            "Epoch 85, Loss: 2.921185261132766\n",
            "Epoch 86, Loss: 2.905593201822164\n",
            "Epoch 87, Loss: 2.8924075778649776\n",
            "Epoch 88, Loss: 2.87880618109995\n",
            "Epoch 89, Loss: 2.8683802607108135\n",
            "Epoch 90, Loss: 2.852959254566504\n",
            "Epoch 91, Loss: 2.8418446341339423\n",
            "Epoch 92, Loss: 2.827993637445022\n",
            "Epoch 93, Loss: 2.8167153900983384\n",
            "Epoch 94, Loss: 2.7978096859795705\n",
            "Epoch 95, Loss: 2.7894003002011045\n",
            "Epoch 96, Loss: 2.77915703642125\n",
            "Epoch 97, Loss: 2.7621175099392326\n",
            "Epoch 98, Loss: 2.7517682885637087\n",
            "Epoch 99, Loss: 2.742576852136729\n",
            "Epoch 100, Loss: 2.729573740034687\n",
            "Epoch 101, Loss: 2.7156128579256484\n",
            "Epoch 102, Loss: 2.703938834521235\n",
            "Epoch 103, Loss: 2.6922123553801556\n",
            "Epoch 104, Loss: 2.6762010199683055\n",
            "Epoch 105, Loss: 2.669085424773547\n",
            "Epoch 106, Loss: 2.6529842931397107\n",
            "Epoch 107, Loss: 2.6410953816102474\n",
            "Epoch 108, Loss: 2.6305033418597006\n",
            "Epoch 109, Loss: 2.6160865900467853\n",
            "Epoch 110, Loss: 2.6043052880131468\n",
            "Epoch 111, Loss: 2.597255208054367\n",
            "Epoch 112, Loss: 2.5815326449822407\n",
            "Epoch 113, Loss: 2.565725106365827\n",
            "Epoch 114, Loss: 2.556902005964396\n",
            "Epoch 115, Loss: 2.546087172566628\n",
            "Epoch 116, Loss: 2.5360166807563935\n",
            "Epoch 117, Loss: 2.5247520986868413\n",
            "Epoch 118, Loss: 2.508321701263895\n",
            "Epoch 119, Loss: 2.4995290418060456\n",
            "Epoch 120, Loss: 2.486159679841022\n",
            "Epoch 121, Loss: 2.4731294001851762\n",
            "Epoch 122, Loss: 2.469290917017022\n",
            "Epoch 123, Loss: 2.455030453448393\n",
            "Epoch 124, Loss: 2.4379083903468386\n",
            "Epoch 125, Loss: 2.429189055549855\n",
            "Epoch 126, Loss: 2.4167590554879634\n",
            "Epoch 127, Loss: 2.4048647479135163\n",
            "Epoch 128, Loss: 2.392542685781206\n",
            "Epoch 129, Loss: 2.3791749258430634\n",
            "Epoch 130, Loss: 2.367175582720309\n",
            "Epoch 131, Loss: 2.3568995947740516\n",
            "Epoch 132, Loss: 2.34488013934116\n",
            "Epoch 133, Loss: 2.331133782863617\n",
            "Epoch 134, Loss: 2.32004368061922\n",
            "Epoch 135, Loss: 2.3100514004425126\n",
            "Epoch 136, Loss: 2.2967015237224344\n",
            "Epoch 137, Loss: 2.285610391777389\n",
            "Epoch 138, Loss: 2.27290203194229\n",
            "Epoch 139, Loss: 2.266712407676541\n",
            "Epoch 140, Loss: 2.253282380347349\n",
            "Epoch 141, Loss: 2.2412576535526587\n",
            "Epoch 142, Loss: 2.226947693192229\n",
            "Epoch 143, Loss: 2.2182657292910983\n",
            "Epoch 144, Loss: 2.2041831162511087\n",
            "Epoch 145, Loss: 2.193583467785193\n",
            "Epoch 146, Loss: 2.1843122998062445\n",
            "Epoch 147, Loss: 2.1740794449436422\n",
            "Epoch 148, Loss: 2.1618986087185994\n",
            "Epoch 149, Loss: 2.1505335715352274\n",
            "Epoch 150, Loss: 2.135259082122725\n",
            "Epoch 151, Loss: 2.1304023369234435\n",
            "Epoch 152, Loss: 2.115387680579205\n",
            "Epoch 153, Loss: 2.105081244999049\n",
            "Epoch 154, Loss: 2.095780150014527\n",
            "Epoch 155, Loss: 2.0829297401467146\n",
            "Epoch 156, Loss: 2.0759913611168765\n",
            "Epoch 157, Loss: 2.057358347031535\n",
            "Epoch 158, Loss: 2.0521817639165993\n",
            "Epoch 159, Loss: 2.0377578096730367\n",
            "Epoch 160, Loss: 2.0246410029275075\n",
            "Epoch 161, Loss: 2.013196118632141\n",
            "Epoch 162, Loss: 2.0047261915644823\n",
            "Epoch 163, Loss: 1.9948898632915653\n",
            "Epoch 164, Loss: 1.9820621488045673\n",
            "Epoch 165, Loss: 1.9721790363593978\n",
            "Epoch 166, Loss: 1.9626538485896832\n",
            "Epoch 167, Loss: 1.952049269359939\n",
            "Epoch 168, Loss: 1.9387168920769984\n",
            "Epoch 169, Loss: 1.9253657819056997\n",
            "Epoch 170, Loss: 1.911945518790459\n",
            "Epoch 171, Loss: 1.9036470767186613\n",
            "Epoch 172, Loss: 1.8914123439058965\n",
            "Epoch 173, Loss: 1.8854301760391312\n",
            "Epoch 174, Loss: 1.8789833327945398\n",
            "Epoch 175, Loss: 1.8621664174965449\n",
            "Epoch 176, Loss: 1.8525800771859227\n",
            "Epoch 177, Loss: 1.8365343352969812\n",
            "Epoch 178, Loss: 1.830685327855908\n",
            "Epoch 179, Loss: 1.8208816100140006\n",
            "Epoch 180, Loss: 1.804723821124252\n",
            "Epoch 181, Loss: 1.7944966822254413\n",
            "Epoch 182, Loss: 1.7848802251475198\n",
            "Epoch 183, Loss: 1.779505541129988\n",
            "Epoch 184, Loss: 1.7705550333675073\n",
            "Epoch 185, Loss: 1.7538870451401691\n",
            "Epoch 186, Loss: 1.7420413731312265\n",
            "Epoch 187, Loss: 1.7378505152098986\n",
            "Epoch 188, Loss: 1.720300020612016\n",
            "Epoch 189, Loss: 1.7168379091486639\n",
            "Epoch 190, Loss: 1.69979827562157\n",
            "Epoch 191, Loss: 1.6900108791127497\n",
            "Epoch 192, Loss: 1.6823203369062774\n",
            "Epoch 193, Loss: 1.6693833719710915\n",
            "Epoch 194, Loss: 1.655711557183947\n",
            "Epoch 195, Loss: 1.6519577132195842\n",
            "Epoch 196, Loss: 1.6396071266154855\n",
            "Epoch 197, Loss: 1.6276730888960314\n",
            "Epoch 198, Loss: 1.6182786579034767\n",
            "Epoch 199, Loss: 1.6062021188589992\n",
            "Accuracy: 32.61%\n",
            "Training with params: {'batch_size': 256, 'lr': 0.005, 'optimizer': 'RMSprop'}\n",
            "Epoch 1, Loss: 5.526133075052378\n",
            "Epoch 2, Loss: 3.626505579267229\n",
            "Epoch 3, Loss: 3.382514782097875\n",
            "Epoch 4, Loss: 3.196723295717823\n",
            "Epoch 5, Loss: 3.0339162191566156\n",
            "Epoch 6, Loss: 2.8937013258739395\n",
            "Epoch 7, Loss: 2.7688503216724007\n",
            "Epoch 8, Loss: 2.644972871760933\n",
            "Epoch 9, Loss: 2.5552981143095055\n",
            "Epoch 10, Loss: 2.453162439015447\n",
            "Epoch 11, Loss: 2.3753432777463175\n",
            "Epoch 12, Loss: 2.304483952570935\n",
            "Epoch 13, Loss: 2.24074640565989\n",
            "Epoch 14, Loss: 2.1853849966915284\n",
            "Epoch 15, Loss: 2.1285827403165856\n",
            "Epoch 16, Loss: 2.0667841154701856\n",
            "Epoch 17, Loss: 2.034761889856689\n",
            "Epoch 18, Loss: 1.9791774968711697\n",
            "Epoch 19, Loss: 1.9347256422042847\n",
            "Epoch 20, Loss: 1.8988310129058605\n",
            "Epoch 21, Loss: 1.8644489694614799\n",
            "Epoch 22, Loss: 1.8202006372870232\n",
            "Epoch 23, Loss: 1.7878832227113295\n",
            "Epoch 24, Loss: 1.7565086386641677\n",
            "Epoch 25, Loss: 1.722035260224829\n",
            "Epoch 26, Loss: 1.6899573796865892\n",
            "Epoch 27, Loss: 1.6503571728054358\n",
            "Epoch 28, Loss: 1.6344652583404464\n",
            "Epoch 29, Loss: 1.5993749979807406\n",
            "Epoch 30, Loss: 1.5732146817810682\n",
            "Epoch 31, Loss: 1.5413873067923956\n",
            "Epoch 32, Loss: 1.5188765337272567\n",
            "Epoch 33, Loss: 1.5023362004027074\n",
            "Epoch 34, Loss: 1.471391504516407\n",
            "Epoch 35, Loss: 1.4466001318425548\n",
            "Epoch 36, Loss: 1.4293439625477304\n",
            "Epoch 37, Loss: 1.4008354644994347\n",
            "Epoch 38, Loss: 1.3876950752978423\n",
            "Epoch 39, Loss: 1.388075858962779\n",
            "Epoch 40, Loss: 1.3400283945458276\n",
            "Epoch 41, Loss: 1.3384330297002986\n",
            "Epoch 42, Loss: 1.3098933900497398\n",
            "Epoch 43, Loss: 1.2947665471203473\n",
            "Epoch 44, Loss: 1.2954422192914146\n",
            "Epoch 45, Loss: 1.2695949652365275\n",
            "Epoch 46, Loss: 1.2572196268913698\n",
            "Epoch 47, Loss: 1.2485888284079882\n",
            "Epoch 48, Loss: 1.2231669854752871\n",
            "Epoch 49, Loss: 1.2217622153004821\n",
            "Epoch 50, Loss: 1.1921717910742273\n",
            "Epoch 51, Loss: 1.2118320416431039\n",
            "Epoch 52, Loss: 1.1782570374863488\n",
            "Epoch 53, Loss: 1.1730545926458982\n",
            "Epoch 54, Loss: 1.1654138969523566\n",
            "Epoch 55, Loss: 1.1566251823488547\n",
            "Epoch 56, Loss: 1.141471037451102\n",
            "Epoch 57, Loss: 1.1438336500099726\n",
            "Epoch 58, Loss: 1.1357769713718064\n",
            "Epoch 59, Loss: 1.1282739113180005\n",
            "Epoch 60, Loss: 1.1105521260475626\n",
            "Epoch 61, Loss: 1.0989803851259\n",
            "Epoch 62, Loss: 1.0994930896832018\n",
            "Epoch 63, Loss: 1.0922828490028575\n",
            "Epoch 64, Loss: 1.080337972057109\n",
            "Epoch 65, Loss: 1.0767623827773698\n",
            "Epoch 66, Loss: 1.0648557489015618\n",
            "Epoch 67, Loss: 1.0555571618737007\n",
            "Epoch 68, Loss: 1.0420606601603177\n",
            "Epoch 69, Loss: 1.0559276780303644\n",
            "Epoch 70, Loss: 1.0345532532249178\n",
            "Epoch 71, Loss: 1.0224104350318715\n",
            "Epoch 72, Loss: 1.0321494000298637\n",
            "Epoch 73, Loss: 1.0206319105868438\n",
            "Epoch 74, Loss: 1.0156864931388778\n",
            "Epoch 75, Loss: 0.9921348681863473\n",
            "Epoch 76, Loss: 0.98661093140135\n",
            "Epoch 77, Loss: 1.0134335966134558\n",
            "Epoch 78, Loss: 0.9656859900878401\n",
            "Epoch 79, Loss: 0.9854755571910313\n",
            "Epoch 80, Loss: 0.9765815239171592\n",
            "Epoch 81, Loss: 0.9645766369542297\n",
            "Epoch 82, Loss: 0.9709170813463173\n",
            "Epoch 83, Loss: 0.9782207231132352\n",
            "Epoch 84, Loss: 0.9513531670886644\n",
            "Epoch 85, Loss: 0.958623789098798\n",
            "Epoch 86, Loss: 0.9282186241174231\n",
            "Epoch 87, Loss: 0.9631428919276412\n",
            "Epoch 88, Loss: 0.9321286301223599\n",
            "Epoch 89, Loss: 0.9349407298224313\n",
            "Epoch 90, Loss: 0.9211995784117251\n",
            "Epoch 91, Loss: 0.9115600035506852\n",
            "Epoch 92, Loss: 0.9354294401650526\n",
            "Epoch 93, Loss: 0.9246423109149446\n",
            "Epoch 94, Loss: 0.909024508327854\n",
            "Epoch 95, Loss: 0.9102195191140078\n",
            "Epoch 96, Loss: 0.893182742960599\n",
            "Epoch 97, Loss: 0.9188914785579759\n",
            "Epoch 98, Loss: 0.895000197449509\n",
            "Epoch 99, Loss: 0.8942716035003565\n",
            "Epoch 100, Loss: 0.9106886180079713\n",
            "Epoch 101, Loss: 0.8812430189276228\n",
            "Epoch 102, Loss: 0.893419477100275\n",
            "Epoch 103, Loss: 0.891247317499044\n",
            "Epoch 104, Loss: 0.859634553595465\n",
            "Epoch 105, Loss: 0.8996089061304015\n",
            "Epoch 106, Loss: 0.8870456194391056\n",
            "Epoch 107, Loss: 0.863945372858826\n",
            "Epoch 108, Loss: 0.8764027947065781\n",
            "Epoch 109, Loss: 0.8740332274108517\n",
            "Epoch 110, Loss: 0.8593062048359793\n",
            "Epoch 111, Loss: 0.8714991858115002\n",
            "Epoch 112, Loss: 0.8539399010490398\n",
            "Epoch 113, Loss: 0.8687896582545066\n",
            "Epoch 114, Loss: 0.8488400420364068\n",
            "Epoch 115, Loss: 0.8552306679134466\n",
            "Epoch 116, Loss: 0.8550521831731407\n",
            "Epoch 117, Loss: 0.8588527751212217\n",
            "Epoch 118, Loss: 0.8319599739446932\n",
            "Epoch 119, Loss: 0.8748182686311858\n",
            "Epoch 120, Loss: 0.8531719947956047\n",
            "Epoch 121, Loss: 0.8210815889190655\n",
            "Epoch 122, Loss: 0.8527773503138094\n",
            "Epoch 123, Loss: 0.8289046930719395\n",
            "Epoch 124, Loss: 0.8429008190120969\n",
            "Epoch 125, Loss: 0.8401617576577225\n",
            "Epoch 126, Loss: 0.845725767770592\n",
            "Epoch 127, Loss: 0.8303551667807053\n",
            "Epoch 128, Loss: 0.8370377016919\n",
            "Epoch 129, Loss: 0.8353966121770897\n",
            "Epoch 130, Loss: 0.8403470510122727\n",
            "Epoch 131, Loss: 0.7894418453987764\n",
            "Epoch 132, Loss: 0.8247062983561535\n",
            "Epoch 133, Loss: 0.8257279870461445\n",
            "Epoch 134, Loss: 0.8194996703948293\n",
            "Epoch 135, Loss: 0.8313829511099932\n",
            "Epoch 136, Loss: 0.7952718296829535\n",
            "Epoch 137, Loss: 0.8294638623388446\n",
            "Epoch 138, Loss: 0.7920777771850022\n",
            "Epoch 139, Loss: 0.8365134643960972\n",
            "Epoch 140, Loss: 0.8173103440476923\n",
            "Epoch 141, Loss: 0.7995612654758959\n",
            "Epoch 142, Loss: 0.8029216903508926\n",
            "Epoch 143, Loss: 0.7885622779021457\n",
            "Epoch 144, Loss: 0.8271268945263357\n",
            "Epoch 145, Loss: 0.7878116843651752\n",
            "Epoch 146, Loss: 0.8055703925843142\n",
            "Epoch 147, Loss: 0.8068734250810682\n",
            "Epoch 148, Loss: 0.7796481422015599\n",
            "Epoch 149, Loss: 0.8184093591205928\n",
            "Epoch 150, Loss: 0.8076753867219906\n",
            "Epoch 151, Loss: 0.8099552713790719\n",
            "Epoch 152, Loss: 0.7724841021153391\n",
            "Epoch 153, Loss: 0.8259364331559259\n",
            "Epoch 154, Loss: 0.7845562658443743\n",
            "Epoch 155, Loss: 0.7913375255100581\n",
            "Epoch 156, Loss: 0.7991721373127432\n",
            "Epoch 157, Loss: 0.7768772921087791\n",
            "Epoch 158, Loss: 0.8037744541557468\n",
            "Epoch 159, Loss: 0.7969228039894786\n",
            "Epoch 160, Loss: 0.7746354520929103\n",
            "Epoch 161, Loss: 0.8221903374912788\n",
            "Epoch 162, Loss: 0.7735510501934557\n",
            "Epoch 163, Loss: 0.7978459531251265\n",
            "Epoch 164, Loss: 0.7670983743606782\n",
            "Epoch 165, Loss: 0.7988284803470787\n",
            "Epoch 166, Loss: 0.7812139734017606\n",
            "Epoch 167, Loss: 0.7893002770689069\n",
            "Epoch 168, Loss: 0.7924263860498156\n",
            "Epoch 169, Loss: 0.7999706081285769\n",
            "Epoch 170, Loss: 0.7617436469513543\n",
            "Epoch 171, Loss: 0.7875244173468375\n",
            "Epoch 172, Loss: 0.7830381309803651\n",
            "Epoch 173, Loss: 0.7742314010250325\n",
            "Epoch 174, Loss: 0.7784501189480022\n",
            "Epoch 175, Loss: 0.7724311038547632\n",
            "Epoch 176, Loss: 0.7692016363143921\n",
            "Epoch 177, Loss: 0.8070631918250298\n",
            "Epoch 178, Loss: 0.7446836575257535\n",
            "Epoch 179, Loss: 0.7958021028613558\n",
            "Epoch 180, Loss: 0.7425027112571561\n",
            "Epoch 181, Loss: 0.8028199687904242\n",
            "Epoch 182, Loss: 0.7623810249627853\n",
            "Epoch 183, Loss: 0.7825891384664847\n",
            "Epoch 184, Loss: 0.7602114937439257\n",
            "Epoch 185, Loss: 0.7487638913551156\n",
            "Epoch 186, Loss: 0.778569617593775\n",
            "Epoch 187, Loss: 0.7772785397816677\n",
            "Epoch 188, Loss: 0.7610370220274342\n",
            "Epoch 189, Loss: 0.8116505358900342\n",
            "Epoch 190, Loss: 0.7871500565385332\n",
            "Epoch 191, Loss: 0.7357270443926052\n",
            "Epoch 192, Loss: 0.7701846098109167\n",
            "Epoch 193, Loss: 0.7747735590959082\n",
            "Epoch 194, Loss: 0.7423663663924956\n",
            "Epoch 195, Loss: 0.7655319045392834\n",
            "Epoch 196, Loss: 0.7781830919640405\n",
            "Epoch 197, Loss: 0.7710080806698117\n",
            "Epoch 198, Loss: 0.7544202416831133\n",
            "Epoch 199, Loss: 0.7807893413974314\n",
            "Accuracy: 22.56%\n",
            "Best Accuracy: 36.68% with params: {'batch_size': 256, 'lr': 0.1, 'optimizer': 'SGD'}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'lr': [0.1, 0.01, 0.001, 0.005],\n",
        "    'batch_size': [64, 128, 256],\n",
        "    'optimizer': ['Adam', 'SGD', 'RMSprop']\n",
        "}\n",
        "\n",
        "# Create a list of hyperparameter combinations\n",
        "grid = list(ParameterGrid(param_grid))\n",
        "\n",
        "# Function to train and evaluate the model with given hyperparameters\n",
        "def train_and_evaluate(lr, batch_size, optimizer_name):\n",
        "    # Update dataloader with the new batch size\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Define the model, loss function, and optimizer\n",
        "    model = ConvNetDA520().to('cuda')\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    if optimizer_name == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    elif optimizer_name == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    elif optimizer_name == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(1, 200):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, (data, target) in enumerate(trainloader):\n",
        "            data, target = data.to('cuda'), target.to('cuda')\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = loss_fn(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch}, Loss: {running_loss / len(trainloader)}')\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in testloader:\n",
        "            data, target = data.to('cuda'), target.to('cuda')\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    accuracy = 100 * (correct / total)\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "# Iterate over all combinations in the grid\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "for params in grid:\n",
        "    print(f\"Training with params: {params}\")\n",
        "    accuracy = train_and_evaluate(params['lr'], params['batch_size'], params['optimizer'])\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_params = params\n",
        "\n",
        "print(f\"Best Accuracy: {best_accuracy:.2f}% with params: {best_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial metrics were:\n",
        "\n",
        "*   Test set: Average loss: 0.3984, Accuracy: 3514/10000 (35.14%)\n",
        "\n",
        "After the parameter tuning:\n",
        "\n",
        "\n",
        "\n",
        "1. Best Accuracy: 36.68% with params: {'batch_size': 256, 'lr': 0.1,'optimizer': 'SGD'}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "19J9Fu9UlpB1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kw1TQGvn2As0"
      },
      "outputs": [],
      "source": [
        "# New Hyperparameters\n",
        "n_epoch = 200\n",
        "model = ConvNetDA520()\n",
        "model.apply(weights_init)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.1\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changelist for ConvNetDA520_20\n",
        "\n",
        "The whole update process was iterative and the final network I come up with is the one below. Here are the changes made.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> **I am are aware that using the hyperparameters from the previous network is sub-optimal but I had to move on with the hyperparams we found for the previous network due to computational time cost and time limitations.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- **Increased Convolutional Filters**:\n",
        "  - Conv1: Increased from 16 to 256\n",
        "  - Conv2: Increased from 32 to 256\n",
        "  - Added Conv3, Conv4, Conv5, Conv6, Conv7, Conv8 with 512 filters each\n",
        "\n",
        "- **Added Batch Normalization Layers**:\n",
        "  - After each convolutional layer (8 BatchNorm2d layers added)\n",
        "\n",
        "- **Modified Pooling Layers**:\n",
        "  - Added an additional pooling layer (4 MaxPool2d layers in total)\n",
        "\n",
        "- **Increased Dropout Regularization**:\n",
        "  - Added dropout after each pooling layer and before the fully connected layer (5 Dropout layers in total)\n",
        "\n",
        "- **Updated Fully Connected Layers**:\n",
        "  - Changed input dimension of fc1 to match new flattened size (512 * 2 * 2)\n",
        "  - Increased size of fc1 from 256 to 1024\n",
        "  - Added BatchNorm1d layer after fc1\n",
        "\n",
        "- **Adjusted Model Architecture**:\n",
        "  - Changed the forward pass to include the new layers and operations\n"
      ],
      "metadata": {
        "id": "981UhO3ikNg8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2nEyHF2r2Uye"
      },
      "outputs": [],
      "source": [
        "class ConvNetDA520_20(nn.Module):\n",
        "    def __init__(self):\n",
        "        # Initial image was 32 X 32 X 3\n",
        "        super(ConvNetDA520_20, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 256, 3, padding=1) # 32 X 32 X 256\n",
        "        self.bn1 = nn.BatchNorm2d(256) # 32 X 32 X 256\n",
        "        self.conv2 = nn.Conv2d(256, 256, 3, padding=1) # 32 X 32 X 256\n",
        "        self.bn2 = nn.BatchNorm2d(256) # 32 X 32 X 256\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # 16 X 16 X 256\n",
        "        self.dropout1 = nn.Dropout(0.2) # 16 X 16 X 256\n",
        "\n",
        "        self.conv3 = nn.Conv2d(256, 512, 3, padding=1) # 16 X 16 X 512\n",
        "        self.bn3 = nn.BatchNorm2d(512) # 16 X 16 X 512\n",
        "        self.conv4 = nn.Conv2d(512, 512, 3, padding=1) # 16 X 16 X 512\n",
        "        self.bn4 = nn.BatchNorm2d(512) # 16 X 16 X 512\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) # 8 X 8 X 512\n",
        "        self.dropout2 = nn.Dropout(0.2) # 8 X 8 X 512\n",
        "\n",
        "        self.conv5 = nn.Conv2d(512, 512, 3, padding=1) # 8 X 8 X 512\n",
        "        self.bn5 = nn.BatchNorm2d(512) # 8 X 8 X 512\n",
        "        self.conv6 = nn.Conv2d(512, 512, 3, padding=1) # 8 X 8 X 512\n",
        "        self.bn6 = nn.BatchNorm2d(512) # 8 X 8 X 512\n",
        "        self.pool3 = nn.MaxPool2d(2, 2) # 4 X 4 X 512\n",
        "        self.dropout3 = nn.Dropout(0.2) # 4 X 4 X 512\n",
        "\n",
        "        self.conv7 = nn.Conv2d(512, 512, 3, padding=1) # 4 X 4 X 512\n",
        "        self.bn7 = nn.BatchNorm2d(512) # 4 X 4 X 512\n",
        "        self.conv8 = nn.Conv2d(512, 512, 3, padding=1) # 4 X 4 X 512\n",
        "        self.bn8 = nn.BatchNorm2d(512) # 4 X 4 X 512\n",
        "        self.pool4 = nn.MaxPool2d(2, 2) # 2 X 2 X 512\n",
        "        self.dropout4 = nn.Dropout(0.2) # 2 X 2 X 512\n",
        "\n",
        "        self.flatten = nn.Flatten() # 1 X 2048\n",
        "        self.fc1 = nn.Linear(512 * 2 * 2, 1024) # 1 X 1024\n",
        "        self.dropout5 = nn.Dropout(0.2) # 1 X 1024\n",
        "        self.bn9 = nn.BatchNorm1d(1024, momentum=0.95, eps=0.005) # 1 X 1024\n",
        "        self.fc2 = nn.Linear(1024, 100) # 1 X 1024\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = F.relu(self.bn7(self.conv7(x)))\n",
        "        x = F.relu(self.bn8(self.conv8(x)))\n",
        "        x = self.pool4(x)\n",
        "        x = self.dropout4(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout5(x)\n",
        "        x = self.bn9(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use different convolutional and pooling layer configurations\n",
        "n_epoch = 200\n",
        "model = ConvNetDA520_20()\n",
        "model.to('cuda')\n",
        "# Initialize model weights\n",
        "model.apply(weights_init)\n",
        "# loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.1\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate,  weight_decay=1e-4) # Added L2 Regularization"
      ],
      "metadata": {
        "id": "N_vDHuz0obqT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "According to the previous experiments our final run hyperparameters are below\n",
        "\n",
        "*   Batch size = 256\n",
        "*   Model is on the cuda\n",
        "*   Epoch is 200\n",
        "*   Learning rate is 0.1\n",
        "*   Optimizer is SGD\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zdP1xC4TFBSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "#Train and test the new model\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # Learning rate scheduler\n",
        "\n",
        "for epoch in range(1, n_epoch + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij7TZJqHomC0",
        "outputId": "40ed4152-9dba-4fa2-929c-761e69f14ac8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 3.6805\n",
            "Test set: Average loss: 4.2720, Accuracy: 1176/10000 (11.76%)\n",
            "Epoch 2, Training Loss: 2.9624\n",
            "Test set: Average loss: 3.1406, Accuracy: 2315/10000 (23.15%)\n",
            "Epoch 3, Training Loss: 2.4966\n",
            "Test set: Average loss: 2.7183, Accuracy: 3206/10000 (32.06%)\n",
            "Epoch 4, Training Loss: 2.1746\n",
            "Test set: Average loss: 2.8371, Accuracy: 3203/10000 (32.03%)\n",
            "Epoch 5, Training Loss: 1.9455\n",
            "Test set: Average loss: 2.4998, Accuracy: 3722/10000 (37.22%)\n",
            "Epoch 6, Training Loss: 1.7484\n",
            "Test set: Average loss: 2.4806, Accuracy: 3755/10000 (37.55%)\n",
            "Epoch 7, Training Loss: 1.5795\n",
            "Test set: Average loss: 2.2259, Accuracy: 4417/10000 (44.17%)\n",
            "Epoch 8, Training Loss: 1.4376\n",
            "Test set: Average loss: 2.1093, Accuracy: 4716/10000 (47.16%)\n",
            "Epoch 9, Training Loss: 1.3134\n",
            "Test set: Average loss: 2.2320, Accuracy: 4450/10000 (44.50%)\n",
            "Epoch 10, Training Loss: 1.1920\n",
            "Test set: Average loss: 2.1043, Accuracy: 4712/10000 (47.12%)\n",
            "Epoch 11, Training Loss: 1.0883\n",
            "Test set: Average loss: 2.1424, Accuracy: 4670/10000 (46.70%)\n",
            "Epoch 12, Training Loss: 0.9868\n",
            "Test set: Average loss: 1.8025, Accuracy: 5345/10000 (53.45%)\n",
            "Epoch 13, Training Loss: 0.8929\n",
            "Test set: Average loss: 1.8687, Accuracy: 5303/10000 (53.03%)\n",
            "Epoch 14, Training Loss: 0.8054\n",
            "Test set: Average loss: 2.1578, Accuracy: 5171/10000 (51.71%)\n",
            "Epoch 15, Training Loss: 0.7189\n",
            "Test set: Average loss: 2.2093, Accuracy: 4995/10000 (49.95%)\n",
            "Epoch 16, Training Loss: 0.6462\n",
            "Test set: Average loss: 4.4472, Accuracy: 3713/10000 (37.13%)\n",
            "Epoch 17, Training Loss: 0.5768\n",
            "Test set: Average loss: 1.8997, Accuracy: 5540/10000 (55.40%)\n",
            "Epoch 18, Training Loss: 0.5084\n",
            "Test set: Average loss: 2.0270, Accuracy: 5416/10000 (54.16%)\n",
            "Epoch 19, Training Loss: 0.4567\n",
            "Test set: Average loss: 2.3091, Accuracy: 5287/10000 (52.87%)\n",
            "Epoch 20, Training Loss: 0.4047\n",
            "Test set: Average loss: 2.0231, Accuracy: 5578/10000 (55.78%)\n",
            "Epoch 21, Training Loss: 0.3488\n",
            "Test set: Average loss: 2.2675, Accuracy: 5369/10000 (53.69%)\n",
            "Epoch 22, Training Loss: 0.3106\n",
            "Test set: Average loss: 2.1758, Accuracy: 5414/10000 (54.14%)\n",
            "Epoch 23, Training Loss: 0.2794\n",
            "Test set: Average loss: 2.0537, Accuracy: 5700/10000 (57.00%)\n",
            "Epoch 24, Training Loss: 0.2425\n",
            "Test set: Average loss: 2.7302, Accuracy: 5121/10000 (51.21%)\n",
            "Epoch 25, Training Loss: 0.2106\n",
            "Test set: Average loss: 2.5373, Accuracy: 5336/10000 (53.36%)\n",
            "Epoch 26, Training Loss: 0.1989\n",
            "Test set: Average loss: 1.9734, Accuracy: 5914/10000 (59.14%)\n",
            "Epoch 27, Training Loss: 0.1868\n",
            "Test set: Average loss: 1.9082, Accuracy: 6076/10000 (60.76%)\n",
            "Epoch 28, Training Loss: 0.1626\n",
            "Test set: Average loss: 2.3396, Accuracy: 5546/10000 (55.46%)\n",
            "Epoch 29, Training Loss: 0.1464\n",
            "Test set: Average loss: 2.0266, Accuracy: 5914/10000 (59.14%)\n",
            "Epoch 30, Training Loss: 0.1283\n",
            "Test set: Average loss: 2.0193, Accuracy: 5876/10000 (58.76%)\n",
            "Epoch 31, Training Loss: 0.0899\n",
            "Test set: Average loss: 1.6221, Accuracy: 6497/10000 (64.97%)\n",
            "Epoch 32, Training Loss: 0.0691\n",
            "Test set: Average loss: 1.6403, Accuracy: 6531/10000 (65.31%)\n",
            "Epoch 33, Training Loss: 0.0592\n",
            "Test set: Average loss: 1.6181, Accuracy: 6554/10000 (65.54%)\n",
            "Epoch 34, Training Loss: 0.0562\n",
            "Test set: Average loss: 1.5902, Accuracy: 6567/10000 (65.67%)\n",
            "Epoch 35, Training Loss: 0.0539\n",
            "Test set: Average loss: 1.5855, Accuracy: 6555/10000 (65.55%)\n",
            "Epoch 36, Training Loss: 0.0507\n",
            "Test set: Average loss: 1.5736, Accuracy: 6583/10000 (65.83%)\n",
            "Epoch 37, Training Loss: 0.0479\n",
            "Test set: Average loss: 1.6269, Accuracy: 6560/10000 (65.60%)\n",
            "Epoch 38, Training Loss: 0.0455\n",
            "Test set: Average loss: 1.5770, Accuracy: 6588/10000 (65.88%)\n",
            "Epoch 39, Training Loss: 0.0458\n",
            "Test set: Average loss: 1.5734, Accuracy: 6567/10000 (65.67%)\n",
            "Epoch 40, Training Loss: 0.0448\n",
            "Test set: Average loss: 1.6441, Accuracy: 6533/10000 (65.33%)\n",
            "Epoch 41, Training Loss: 0.0426\n",
            "Test set: Average loss: 1.5863, Accuracy: 6577/10000 (65.77%)\n",
            "Epoch 42, Training Loss: 0.0427\n",
            "Test set: Average loss: 1.5776, Accuracy: 6613/10000 (66.13%)\n",
            "Epoch 43, Training Loss: 0.0411\n",
            "Test set: Average loss: 1.6026, Accuracy: 6589/10000 (65.89%)\n",
            "Epoch 44, Training Loss: 0.0408\n",
            "Test set: Average loss: 1.6225, Accuracy: 6597/10000 (65.97%)\n",
            "Epoch 45, Training Loss: 0.0385\n",
            "Test set: Average loss: 1.6452, Accuracy: 6563/10000 (65.63%)\n",
            "Epoch 46, Training Loss: 0.0387\n",
            "Test set: Average loss: 1.5955, Accuracy: 6621/10000 (66.21%)\n",
            "Epoch 47, Training Loss: 0.0368\n",
            "Test set: Average loss: 1.6187, Accuracy: 6594/10000 (65.94%)\n",
            "Epoch 48, Training Loss: 0.0348\n",
            "Test set: Average loss: 1.6003, Accuracy: 6543/10000 (65.43%)\n",
            "Epoch 49, Training Loss: 0.0352\n",
            "Test set: Average loss: 1.6283, Accuracy: 6566/10000 (65.66%)\n",
            "Epoch 50, Training Loss: 0.0368\n",
            "Test set: Average loss: 1.6296, Accuracy: 6616/10000 (66.16%)\n",
            "Epoch 51, Training Loss: 0.0350\n",
            "Test set: Average loss: 1.6301, Accuracy: 6590/10000 (65.90%)\n",
            "Epoch 52, Training Loss: 0.0355\n",
            "Test set: Average loss: 1.6012, Accuracy: 6587/10000 (65.87%)\n",
            "Epoch 53, Training Loss: 0.0326\n",
            "Test set: Average loss: 1.6480, Accuracy: 6600/10000 (66.00%)\n",
            "Epoch 54, Training Loss: 0.0330\n",
            "Test set: Average loss: 1.6097, Accuracy: 6592/10000 (65.92%)\n",
            "Epoch 55, Training Loss: 0.0330\n",
            "Test set: Average loss: 1.6386, Accuracy: 6588/10000 (65.88%)\n",
            "Epoch 56, Training Loss: 0.0320\n",
            "Test set: Average loss: 1.6164, Accuracy: 6596/10000 (65.96%)\n",
            "Epoch 57, Training Loss: 0.0314\n",
            "Test set: Average loss: 1.6341, Accuracy: 6582/10000 (65.82%)\n",
            "Epoch 58, Training Loss: 0.0309\n",
            "Test set: Average loss: 1.6105, Accuracy: 6629/10000 (66.29%)\n",
            "Epoch 59, Training Loss: 0.0309\n",
            "Test set: Average loss: 1.6236, Accuracy: 6599/10000 (65.99%)\n",
            "Epoch 60, Training Loss: 0.0302\n",
            "Test set: Average loss: 1.6365, Accuracy: 6638/10000 (66.38%)\n",
            "Epoch 61, Training Loss: 0.0308\n",
            "Test set: Average loss: 1.6130, Accuracy: 6612/10000 (66.12%)\n",
            "Epoch 62, Training Loss: 0.0295\n",
            "Test set: Average loss: 1.6105, Accuracy: 6636/10000 (66.36%)\n",
            "Epoch 63, Training Loss: 0.0300\n",
            "Test set: Average loss: 1.6016, Accuracy: 6621/10000 (66.21%)\n",
            "Epoch 64, Training Loss: 0.0295\n",
            "Test set: Average loss: 1.6598, Accuracy: 6643/10000 (66.43%)\n",
            "Epoch 65, Training Loss: 0.0279\n",
            "Test set: Average loss: 1.6017, Accuracy: 6608/10000 (66.08%)\n",
            "Epoch 66, Training Loss: 0.0296\n",
            "Test set: Average loss: 1.6322, Accuracy: 6621/10000 (66.21%)\n",
            "Epoch 67, Training Loss: 0.0288\n",
            "Test set: Average loss: 1.6585, Accuracy: 6612/10000 (66.12%)\n",
            "Epoch 68, Training Loss: 0.0282\n",
            "Test set: Average loss: 1.6271, Accuracy: 6612/10000 (66.12%)\n",
            "Epoch 69, Training Loss: 0.0278\n",
            "Test set: Average loss: 1.6130, Accuracy: 6607/10000 (66.07%)\n",
            "Epoch 70, Training Loss: 0.0287\n",
            "Test set: Average loss: 1.6277, Accuracy: 6621/10000 (66.21%)\n",
            "Epoch 71, Training Loss: 0.0285\n",
            "Test set: Average loss: 1.6481, Accuracy: 6621/10000 (66.21%)\n",
            "Epoch 72, Training Loss: 0.0283\n",
            "Test set: Average loss: 1.6609, Accuracy: 6627/10000 (66.27%)\n",
            "Epoch 73, Training Loss: 0.0292\n",
            "Test set: Average loss: 1.6014, Accuracy: 6640/10000 (66.40%)\n",
            "Epoch 74, Training Loss: 0.0284\n",
            "Test set: Average loss: 1.6374, Accuracy: 6639/10000 (66.39%)\n",
            "Epoch 75, Training Loss: 0.0289\n",
            "Test set: Average loss: 1.6284, Accuracy: 6592/10000 (65.92%)\n",
            "Epoch 76, Training Loss: 0.0285\n",
            "Test set: Average loss: 1.6670, Accuracy: 6608/10000 (66.08%)\n",
            "Epoch 77, Training Loss: 0.0282\n",
            "Test set: Average loss: 1.6567, Accuracy: 6602/10000 (66.02%)\n",
            "Epoch 78, Training Loss: 0.0276\n",
            "Test set: Average loss: 1.6263, Accuracy: 6603/10000 (66.03%)\n",
            "Epoch 79, Training Loss: 0.0275\n",
            "Test set: Average loss: 1.6496, Accuracy: 6611/10000 (66.11%)\n",
            "Epoch 80, Training Loss: 0.0280\n",
            "Test set: Average loss: 1.6495, Accuracy: 6607/10000 (66.07%)\n",
            "Epoch 81, Training Loss: 0.0276\n",
            "Test set: Average loss: 1.6045, Accuracy: 6624/10000 (66.24%)\n",
            "Epoch 82, Training Loss: 0.0271\n",
            "Test set: Average loss: 1.6177, Accuracy: 6630/10000 (66.30%)\n",
            "Epoch 83, Training Loss: 0.0274\n",
            "Test set: Average loss: 1.6153, Accuracy: 6608/10000 (66.08%)\n",
            "Epoch 84, Training Loss: 0.0285\n",
            "Test set: Average loss: 1.6216, Accuracy: 6628/10000 (66.28%)\n",
            "Epoch 85, Training Loss: 0.0279\n",
            "Test set: Average loss: 1.6411, Accuracy: 6649/10000 (66.49%)\n",
            "Epoch 86, Training Loss: 0.0274\n",
            "Test set: Average loss: 1.6197, Accuracy: 6634/10000 (66.34%)\n",
            "Epoch 87, Training Loss: 0.0276\n",
            "Test set: Average loss: 1.6338, Accuracy: 6610/10000 (66.10%)\n",
            "Epoch 88, Training Loss: 0.0281\n",
            "Test set: Average loss: 1.6429, Accuracy: 6591/10000 (65.91%)\n",
            "Epoch 89, Training Loss: 0.0270\n",
            "Test set: Average loss: 1.6155, Accuracy: 6628/10000 (66.28%)\n",
            "Epoch 90, Training Loss: 0.0263\n",
            "Test set: Average loss: 1.6235, Accuracy: 6615/10000 (66.15%)\n",
            "Epoch 91, Training Loss: 0.0282\n",
            "Test set: Average loss: 1.6269, Accuracy: 6622/10000 (66.22%)\n",
            "Epoch 92, Training Loss: 0.0276\n",
            "Test set: Average loss: 1.6514, Accuracy: 6587/10000 (65.87%)\n",
            "Epoch 93, Training Loss: 0.0272\n",
            "Test set: Average loss: 1.6329, Accuracy: 6608/10000 (66.08%)\n",
            "Epoch 94, Training Loss: 0.0272\n",
            "Test set: Average loss: 1.6201, Accuracy: 6629/10000 (66.29%)\n",
            "Epoch 95, Training Loss: 0.0268\n",
            "Test set: Average loss: 1.6037, Accuracy: 6618/10000 (66.18%)\n",
            "Epoch 96, Training Loss: 0.0270\n",
            "Test set: Average loss: 1.6297, Accuracy: 6621/10000 (66.21%)\n",
            "Epoch 97, Training Loss: 0.0284\n",
            "Test set: Average loss: 1.6079, Accuracy: 6614/10000 (66.14%)\n",
            "Epoch 98, Training Loss: 0.0269\n",
            "Test set: Average loss: 1.6173, Accuracy: 6601/10000 (66.01%)\n",
            "Epoch 99, Training Loss: 0.0272\n",
            "Test set: Average loss: 1.6096, Accuracy: 6618/10000 (66.18%)\n",
            "Epoch 100, Training Loss: 0.0263\n",
            "Test set: Average loss: 1.6400, Accuracy: 6614/10000 (66.14%)\n",
            "Epoch 101, Training Loss: 0.0278\n",
            "Test set: Average loss: 1.6296, Accuracy: 6639/10000 (66.39%)\n",
            "Epoch 102, Training Loss: 0.0275\n",
            "Test set: Average loss: 1.6338, Accuracy: 6596/10000 (65.96%)\n",
            "Epoch 103, Training Loss: 0.0271\n",
            "Test set: Average loss: 1.6441, Accuracy: 6641/10000 (66.41%)\n",
            "Epoch 104, Training Loss: 0.0266\n",
            "Test set: Average loss: 1.6024, Accuracy: 6620/10000 (66.20%)\n",
            "Epoch 105, Training Loss: 0.0282\n",
            "Test set: Average loss: 1.6840, Accuracy: 6611/10000 (66.11%)\n",
            "Epoch 106, Training Loss: 0.0273\n",
            "Test set: Average loss: 1.6359, Accuracy: 6612/10000 (66.12%)\n",
            "Epoch 107, Training Loss: 0.0273\n",
            "Test set: Average loss: 1.6063, Accuracy: 6605/10000 (66.05%)\n",
            "Epoch 108, Training Loss: 0.0268\n",
            "Test set: Average loss: 1.6496, Accuracy: 6613/10000 (66.13%)\n",
            "Epoch 109, Training Loss: 0.0274\n",
            "Test set: Average loss: 1.6269, Accuracy: 6623/10000 (66.23%)\n",
            "Epoch 110, Training Loss: 0.0275\n",
            "Test set: Average loss: 1.6194, Accuracy: 6628/10000 (66.28%)\n",
            "Epoch 111, Training Loss: 0.0277\n",
            "Test set: Average loss: 1.6292, Accuracy: 6625/10000 (66.25%)\n",
            "Epoch 112, Training Loss: 0.0266\n",
            "Test set: Average loss: 1.6577, Accuracy: 6619/10000 (66.19%)\n",
            "Epoch 113, Training Loss: 0.0264\n",
            "Test set: Average loss: 1.6281, Accuracy: 6620/10000 (66.20%)\n",
            "Epoch 114, Training Loss: 0.0274\n",
            "Test set: Average loss: 1.6382, Accuracy: 6608/10000 (66.08%)\n",
            "Epoch 115, Training Loss: 0.0278\n",
            "Test set: Average loss: 1.6561, Accuracy: 6591/10000 (65.91%)\n",
            "Epoch 116, Training Loss: 0.0269\n",
            "Test set: Average loss: 1.6201, Accuracy: 6619/10000 (66.19%)\n",
            "Epoch 117, Training Loss: 0.0270\n",
            "Test set: Average loss: 1.6289, Accuracy: 6600/10000 (66.00%)\n",
            "Epoch 118, Training Loss: 0.0272\n",
            "Test set: Average loss: 1.5990, Accuracy: 6615/10000 (66.15%)\n",
            "Epoch 119, Training Loss: 0.0271\n",
            "Test set: Average loss: 1.6327, Accuracy: 6610/10000 (66.10%)\n",
            "Epoch 120, Training Loss: 0.0278\n",
            "Test set: Average loss: 1.6314, Accuracy: 6634/10000 (66.34%)\n",
            "Epoch 121, Training Loss: 0.0269\n",
            "Test set: Average loss: 1.6407, Accuracy: 6593/10000 (65.93%)\n",
            "Epoch 122, Training Loss: 0.0276\n",
            "Test set: Average loss: 1.6602, Accuracy: 6627/10000 (66.27%)\n",
            "Epoch 123, Training Loss: 0.0271\n",
            "Test set: Average loss: 1.6172, Accuracy: 6618/10000 (66.18%)\n",
            "Epoch 124, Training Loss: 0.0270\n",
            "Test set: Average loss: 1.6280, Accuracy: 6631/10000 (66.31%)\n",
            "Epoch 125, Training Loss: 0.0273\n",
            "Test set: Average loss: 1.6334, Accuracy: 6627/10000 (66.27%)\n",
            "Epoch 126, Training Loss: 0.0280\n",
            "Test set: Average loss: 1.6544, Accuracy: 6610/10000 (66.10%)\n",
            "Epoch 127, Training Loss: 0.0277\n",
            "Test set: Average loss: 1.5867, Accuracy: 6611/10000 (66.11%)\n",
            "Epoch 128, Training Loss: 0.0265\n",
            "Test set: Average loss: 1.6414, Accuracy: 6629/10000 (66.29%)\n",
            "Epoch 129, Training Loss: 0.0272\n",
            "Test set: Average loss: 1.6202, Accuracy: 6605/10000 (66.05%)\n",
            "Epoch 130, Training Loss: 0.0275\n",
            "Test set: Average loss: 1.6070, Accuracy: 6605/10000 (66.05%)\n",
            "Epoch 131, Training Loss: 0.0267\n",
            "Test set: Average loss: 1.6355, Accuracy: 6617/10000 (66.17%)\n",
            "Epoch 132, Training Loss: 0.0263\n",
            "Test set: Average loss: 1.6331, Accuracy: 6611/10000 (66.11%)\n",
            "Epoch 133, Training Loss: 0.0269\n",
            "Test set: Average loss: 1.6471, Accuracy: 6632/10000 (66.32%)\n",
            "Epoch 134, Training Loss: 0.0273\n",
            "Test set: Average loss: 1.6187, Accuracy: 6608/10000 (66.08%)\n",
            "Epoch 135, Training Loss: 0.0280\n",
            "Test set: Average loss: 1.6563, Accuracy: 6630/10000 (66.30%)\n",
            "Epoch 136, Training Loss: 0.0269\n",
            "Test set: Average loss: 1.6301, Accuracy: 6631/10000 (66.31%)\n",
            "Epoch 137, Training Loss: 0.0271\n",
            "Test set: Average loss: 1.6370, Accuracy: 6619/10000 (66.19%)\n",
            "Epoch 138, Training Loss: 0.0279\n",
            "Test set: Average loss: 1.6069, Accuracy: 6638/10000 (66.38%)\n",
            "Epoch 139, Training Loss: 0.0274\n",
            "Test set: Average loss: 1.6505, Accuracy: 6614/10000 (66.14%)\n",
            "Epoch 140, Training Loss: 0.0274\n",
            "Test set: Average loss: 1.6240, Accuracy: 6608/10000 (66.08%)\n",
            "Epoch 141, Training Loss: 0.0270\n",
            "Test set: Average loss: 1.6140, Accuracy: 6590/10000 (65.90%)\n",
            "Epoch 142, Training Loss: 0.0274\n",
            "Test set: Average loss: 1.6222, Accuracy: 6601/10000 (66.01%)\n",
            "Epoch 143, Training Loss: 0.0270\n",
            "Test set: Average loss: 1.6589, Accuracy: 6628/10000 (66.28%)\n",
            "Epoch 144, Training Loss: 0.0274\n",
            "Test set: Average loss: 1.6471, Accuracy: 6601/10000 (66.01%)\n",
            "Epoch 145, Training Loss: 0.0280\n",
            "Test set: Average loss: 1.6510, Accuracy: 6616/10000 (66.16%)\n",
            "Epoch 146, Training Loss: 0.0268\n",
            "Test set: Average loss: 1.6181, Accuracy: 6620/10000 (66.20%)\n",
            "Epoch 147, Training Loss: 0.0270\n",
            "Test set: Average loss: 1.6099, Accuracy: 6628/10000 (66.28%)\n",
            "Epoch 148, Training Loss: 0.0274\n",
            "Test set: Average loss: 1.6054, Accuracy: 6607/10000 (66.07%)\n",
            "Epoch 149, Training Loss: 0.0272\n",
            "Test set: Average loss: 1.6132, Accuracy: 6608/10000 (66.08%)\n",
            "Epoch 150, Training Loss: 0.0276\n",
            "Test set: Average loss: 1.6108, Accuracy: 6623/10000 (66.23%)\n",
            "Epoch 151, Training Loss: 0.0267\n",
            "Test set: Average loss: 1.6366, Accuracy: 6639/10000 (66.39%)\n",
            "Epoch 152, Training Loss: 0.0266\n",
            "Test set: Average loss: 1.6357, Accuracy: 6581/10000 (65.81%)\n",
            "Epoch 153, Training Loss: 0.0269\n",
            "Test set: Average loss: 1.6399, Accuracy: 6597/10000 (65.97%)\n",
            "Epoch 154, Training Loss: 0.0270\n",
            "Test set: Average loss: 1.6035, Accuracy: 6611/10000 (66.11%)\n",
            "Epoch 155, Training Loss: 0.0272\n",
            "Test set: Average loss: 1.6254, Accuracy: 6613/10000 (66.13%)\n",
            "Epoch 156, Training Loss: 0.0276\n",
            "Test set: Average loss: 1.5905, Accuracy: 6607/10000 (66.07%)\n",
            "Epoch 157, Training Loss: 0.0268\n",
            "Test set: Average loss: 1.6299, Accuracy: 6655/10000 (66.55%)\n",
            "Epoch 158, Training Loss: 0.0272\n",
            "Test set: Average loss: 1.6576, Accuracy: 6604/10000 (66.04%)\n",
            "Epoch 159, Training Loss: 0.0281\n",
            "Test set: Average loss: 1.6375, Accuracy: 6602/10000 (66.02%)\n",
            "Epoch 160, Training Loss: 0.0279\n",
            "Test set: Average loss: 1.6299, Accuracy: 6599/10000 (65.99%)\n",
            "Epoch 161, Training Loss: 0.0277\n",
            "Test set: Average loss: 1.6257, Accuracy: 6594/10000 (65.94%)\n",
            "Epoch 162, Training Loss: 0.0279\n",
            "Test set: Average loss: 1.6191, Accuracy: 6627/10000 (66.27%)\n",
            "Epoch 163, Training Loss: 0.0277\n",
            "Test set: Average loss: 1.6528, Accuracy: 6592/10000 (65.92%)\n",
            "Epoch 164, Training Loss: 0.0284\n",
            "Test set: Average loss: 1.6439, Accuracy: 6618/10000 (66.18%)\n",
            "Epoch 165, Training Loss: 0.0273\n",
            "Test set: Average loss: 1.6192, Accuracy: 6607/10000 (66.07%)\n",
            "Epoch 166, Training Loss: 0.0283\n",
            "Test set: Average loss: 1.6456, Accuracy: 6640/10000 (66.40%)\n",
            "Epoch 167, Training Loss: 0.0282\n",
            "Test set: Average loss: 1.6124, Accuracy: 6605/10000 (66.05%)\n",
            "Epoch 168, Training Loss: 0.0281\n",
            "Test set: Average loss: 1.6611, Accuracy: 6597/10000 (65.97%)\n",
            "Epoch 169, Training Loss: 0.0278\n",
            "Test set: Average loss: 1.6437, Accuracy: 6613/10000 (66.13%)\n",
            "Epoch 170, Training Loss: 0.0274\n",
            "Test set: Average loss: 1.6241, Accuracy: 6607/10000 (66.07%)\n",
            "Epoch 171, Training Loss: 0.0267\n",
            "Test set: Average loss: 1.6135, Accuracy: 6598/10000 (65.98%)\n",
            "Epoch 172, Training Loss: 0.0275\n",
            "Test set: Average loss: 1.6273, Accuracy: 6598/10000 (65.98%)\n",
            "Epoch 173, Training Loss: 0.0271\n",
            "Test set: Average loss: 1.6165, Accuracy: 6616/10000 (66.16%)\n",
            "Epoch 174, Training Loss: 0.0273\n",
            "Test set: Average loss: 1.6354, Accuracy: 6636/10000 (66.36%)\n",
            "Epoch 175, Training Loss: 0.0276\n",
            "Test set: Average loss: 1.6229, Accuracy: 6593/10000 (65.93%)\n",
            "Epoch 176, Training Loss: 0.0279\n",
            "Test set: Average loss: 1.6205, Accuracy: 6603/10000 (66.03%)\n",
            "Epoch 177, Training Loss: 0.0284\n",
            "Test set: Average loss: 1.6222, Accuracy: 6614/10000 (66.14%)\n",
            "Epoch 178, Training Loss: 0.0278\n",
            "Test set: Average loss: 1.6396, Accuracy: 6635/10000 (66.35%)\n",
            "Epoch 179, Training Loss: 0.0270\n",
            "Test set: Average loss: 1.6130, Accuracy: 6602/10000 (66.02%)\n",
            "Epoch 180, Training Loss: 0.0272\n",
            "Test set: Average loss: 1.6188, Accuracy: 6617/10000 (66.17%)\n",
            "Epoch 181, Training Loss: 0.0268\n",
            "Test set: Average loss: 1.6129, Accuracy: 6654/10000 (66.54%)\n",
            "Epoch 182, Training Loss: 0.0270\n",
            "Test set: Average loss: 1.6165, Accuracy: 6604/10000 (66.04%)\n",
            "Epoch 183, Training Loss: 0.0271\n",
            "Test set: Average loss: 1.6027, Accuracy: 6627/10000 (66.27%)\n",
            "Epoch 184, Training Loss: 0.0272\n",
            "Test set: Average loss: 1.6084, Accuracy: 6612/10000 (66.12%)\n",
            "Epoch 185, Training Loss: 0.0272\n",
            "Test set: Average loss: 1.6318, Accuracy: 6611/10000 (66.11%)\n",
            "Epoch 186, Training Loss: 0.0266\n",
            "Test set: Average loss: 1.6613, Accuracy: 6613/10000 (66.13%)\n",
            "Epoch 187, Training Loss: 0.0268\n",
            "Test set: Average loss: 1.6089, Accuracy: 6623/10000 (66.23%)\n",
            "Epoch 188, Training Loss: 0.0270\n",
            "Test set: Average loss: 1.5939, Accuracy: 6639/10000 (66.39%)\n",
            "Epoch 189, Training Loss: 0.0272\n",
            "Test set: Average loss: 1.6381, Accuracy: 6617/10000 (66.17%)\n",
            "Epoch 190, Training Loss: 0.0272\n",
            "Test set: Average loss: 1.6081, Accuracy: 6620/10000 (66.20%)\n",
            "Epoch 191, Training Loss: 0.0284\n",
            "Test set: Average loss: 1.6465, Accuracy: 6612/10000 (66.12%)\n",
            "Epoch 192, Training Loss: 0.0279\n",
            "Test set: Average loss: 1.6208, Accuracy: 6598/10000 (65.98%)\n",
            "Epoch 193, Training Loss: 0.0263\n",
            "Test set: Average loss: 1.6249, Accuracy: 6610/10000 (66.10%)\n",
            "Epoch 194, Training Loss: 0.0273\n",
            "Test set: Average loss: 1.6609, Accuracy: 6608/10000 (66.08%)\n",
            "Epoch 195, Training Loss: 0.0271\n",
            "Test set: Average loss: 1.6554, Accuracy: 6619/10000 (66.19%)\n",
            "Epoch 196, Training Loss: 0.0264\n",
            "Test set: Average loss: 1.6569, Accuracy: 6610/10000 (66.10%)\n",
            "Epoch 197, Training Loss: 0.0274\n",
            "Test set: Average loss: 1.6590, Accuracy: 6595/10000 (65.95%)\n",
            "Epoch 198, Training Loss: 0.0276\n",
            "Test set: Average loss: 1.6339, Accuracy: 6620/10000 (66.20%)\n",
            "Epoch 199, Training Loss: 0.0272\n",
            "Test set: Average loss: 1.6437, Accuracy: 6625/10000 (66.25%)\n",
            "Epoch 200, Training Loss: 0.0271\n",
            "Test set: Average loss: 1.6720, Accuracy: 6611/10000 (66.11%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(test_losses, label='Testing Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Testing Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Aqa2o7JlEUzd",
        "outputId": "a9cfe173-4b75-485c-9414-572441c9a864"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0EAAAHWCAYAAACxAYILAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACAhklEQVR4nO3dd3wT9f8H8NdlNOlKSxdtoey9NwIylC0iy/lFAb8qDnB8Eb+IP0WcOHGA4gYHLvwCLhABQQRB9h4CllLoooUOupPc749PLqO7Jek16ev5ePSR5HLJvXO5Xu5978/nc5IsyzKIiIiIiIjqCY3aARAREREREdUmJkFERERERFSvMAkiIiIiIqJ6hUkQERERERHVK0yCiIiIiIioXmESRERERERE9QqTICIiIiIiqleYBBERERERUb3CJIiIiIiIiOoVJkFERHXMtGnT0KxZsxq9dv78+ZAkyb0B1TFnzpyBJElYtmyZ2qFUatmyZZAkCWfOnFE7FCIicsIkiIioiiRJqtLf5s2b1Q613mvWrFmVvit3JVIvvvgiVq9e7Zb3chclIU5PT1c7FCKiOkendgBERN7i888/d3n82WefYf369aWmt2/f/oqW8+GHH8JqtdbotU8++SQef/zxK1q+L3jzzTdx+fJl++M1a9bgq6++whtvvIGIiAj79P79+7tleS+++CJuvPFGjB8/3mX6HXfcgVtvvRUGg8EtyyEiIvdgEkREVEW33367y+MdO3Zg/fr1paaXlJeXh4CAgCovR6/X1yg+ANDpdNDpuGsvmYykpKTgq6++wvjx42vc1LAmtFottFptrS2PiIiqhs3hiIjcaMiQIejUqRP27NmDQYMGISAgAE888QQA4Pvvv8eYMWMQGxsLg8GAli1b4rnnnoPFYnF5j5J9gpQ+MK+99ho++OADtGzZEgaDAb1798auXbtcXltWnyBJkjBz5kysXr0anTp1gsFgQMeOHfHLL7+Uin/z5s3o1asXjEYjWrZsiffff7/K/Yz++OMP3HTTTWjSpAkMBgPi4uLwn//8B/n5+aU+X1BQEM6fP4/x48cjKCgIkZGRmD17dql1kZmZiWnTpiEkJAShoaGYOnUqMjMzK42lqr744gv07NkT/v7+CAsLw6233orExESXeU6ePIlJkyYhOjoaRqMRjRs3xq233oqsrCwAYv3m5ubi008/tTezmzZtGoCy+wQ1a9YM119/PbZu3Yo+ffrAaDSiRYsW+Oyzz0rFd/DgQQwePBj+/v5o3Lgxnn/+eSxdutSt/Yx+++03DBw4EIGBgQgNDcW4ceNw7Ngxl3lycnLwyCOPoFmzZjAYDIiKisLw4cOxd+/eKq8nIqK6hKcLiYjcLCMjA6NHj8att96K22+/HQ0bNgQgDoiDgoIwa9YsBAUF4bfffsO8efOQnZ2NV199tdL3/fLLL5GTk4N7770XkiThlVdewcSJE/HPP/9UWj3aunUrVq5ciQceeADBwcF4++23MWnSJJw9exbh4eEAgH379mHUqFGIiYnBM888A4vFgmeffRaRkZFV+twrVqxAXl4e7r//foSHh2Pnzp1YtGgRzp07hxUrVrjMa7FYMHLkSPTt2xevvfYaNmzYgNdffx0tW7bE/fffDwCQZRnjxo3D1q1bcd9996F9+/ZYtWoVpk6dWqV4KvPCCy/gqaeews0334y7774bFy5cwKJFizBo0CDs27cPoaGhKCoqwsiRI1FYWIgHH3wQ0dHROH/+PH766SdkZmYiJCQEn3/+Oe6++2706dMH06dPBwC0bNmywmWfOnUKN954I+666y5MnToVn3zyCaZNm4aePXuiY8eOAIDz58/jmmuugSRJmDt3LgIDA/HRRx+5tWndhg0bMHr0aLRo0QLz589Hfn4+Fi1ahAEDBmDv3r32ZPy+++7Dd999h5kzZ6JDhw7IyMjA1q1bcezYMfTo0aNK64mIqE6RiYioRmbMmCGX3I0OHjxYBiC/9957pebPy8srNe3ee++VAwIC5IKCAvu0qVOnyk2bNrU/jo+PlwHI4eHh8sWLF+3Tv//+exmA/OOPP9qnPf3006ViAiD7+fnJp06dsk87cOCADEBetGiRfdrYsWPlgIAA+fz58/ZpJ0+elHU6Xan3LEtZn2/BggWyJElyQkKCy+cDID/77LMu83bv3l3u2bOn/fHq1atlAPIrr7xin2Y2m+WBAwfKAOSlS5dWGpPi1VdflQHI8fHxsizL8pkzZ2StViu/8MILLvMdOnRI1ul09un79u2TAcgrVqyo8P0DAwPlqVOnlpq+dOlSl+XKsiw3bdpUBiBv2bLFPi0tLU02GAzyo48+ap/24IMPypIkyfv27bNPy8jIkMPCwkq9Z1mUbeHChQvlztOtWzc5KipKzsjIsE87cOCArNFo5ClTptinhYSEyDNmzCj3faq6noiI6go2hyMicjODwYA777yz1HR/f3/7/ZycHKSnp2PgwIHIy8vD8ePHK33fW265BQ0aNLA/HjhwIADgn3/+qfS1w4YNc6lOdOnSBSaTyf5ai8WCDRs2YPz48YiNjbXP16pVK4wePbrS9wdcP19ubi7S09PRv39/yLKMffv2lZr/vvvuc3k8cOBAl8+yZs0a6HQ6e2UIEH1sHnzwwSrFU5GVK1fCarXi5ptvRnp6uv0vOjoarVu3xqZNmwDAXsFYt24d8vLyrni5ig4dOti/PwCIjIxE27ZtXT7/L7/8gn79+qFbt272aWFhYZg8ebJbYkhOTsb+/fsxbdo0hIWF2ad36dIFw4cPx5o1a+zTQkND8ddffyEpKanM9/LUeiIi8hQmQUREbtaoUSP4+fmVmn7kyBFMmDABISEhMJlMiIyMtA+qUJV+E02aNHF5rCREly5dqvZrldcrr01LS0N+fj5atWpVar6yppXl7Nmz9gNqpZ/P4MGDAZT+fEajsVQzO+d4ACAhIQExMTEICgpyma9t27ZViqciJ0+ehCzLaN26NSIjI13+jh07hrS0NABA8+bNMWvWLHz00UeIiIjAyJEj8c4771xxP5fKvg9AfP4r+T4qk5CQAKDs9dm+fXukp6cjNzcXAPDKK6/g8OHDiIuLQ58+fTB//nyXhM1T64mIyFPYJ4iIyM2cKyKKzMxMDB48GCaTCc8++yxatmwJo9GIvXv3Ys6cOVUaEru8UcZkWfboa6vCYrFg+PDhuHjxIubMmYN27dohMDAQ58+fx7Rp00p9PrVHTLNarZAkCWvXri0zFufE6/XXX8e0adPw/fff49dff8VDDz2EBQsWYMeOHWjcuHGNlu/p78Pdbr75ZgwcOBCrVq3Cr7/+ildffRUvv/wyVq5caa8UemI9ERF5CpMgIqJasHnzZmRkZGDlypUYNGiQfXp8fLyKUTlERUXBaDTi1KlTpZ4ra1pJhw4dwt9//41PP/0UU6ZMsU9fv359jWNq2rQpNm7ciMuXL7skJSdOnKjxeypatmwJWZbRvHlztGnTptL5O3fujM6dO+PJJ5/En3/+iQEDBuC9997D888/DwBVGj2vupo2bVrj76Oq7w+UvT6PHz+OiIgIBAYG2qfFxMTggQcewAMPPIC0tDT06NEDL7zwgktzycrWExFRXcHmcEREtUA58+98pr+oqAjvvvuuWiG50Gq1GDZsGFavXu3S7+PUqVNYu3ZtlV4PuH4+WZbx1ltv1Tim6667DmazGUuWLLFPs1gsWLRoUY3fUzFx4kRotVo888wzpaovsiwjIyMDAJCdnQ2z2ezyfOfOnaHRaFBYWGifFhgY6NahuwFg5MiR2L59O/bv32+fdvHiRSxfvtwt7x8TE4Nu3brh008/dYn98OHD+PXXX3HdddcBEOu8ZLO2qKgoxMbG2tdBVdcTEVFdwUoQEVEt6N+/Pxo0aICpU6fioYcegiRJ+Pzzz+tU86f58+fj119/xYABA3D//ffDYrFg8eLF6NSpk8uBeFnatWuHli1bYvbs2Th//jxMJhP+97//Vam/UnnGjh2LAQMG4PHHH8eZM2fQoUMHrFy50i39TFq2bInnn38ec+fOxZkzZzB+/HgEBwcjPj4eq1atwvTp0zF79mz89ttvmDlzJm666Sa0adMGZrMZn3/+ObRaLSZNmmR/v549e2LDhg1YuHAhYmNj0bx5c/Tt2/eKYvzvf/+LL774AsOHD8eDDz5oHyK7SZMmuHjxYpWrTwsXLix1sV6NRoMnnngCr776KkaPHo1+/frhrrvusg+RHRISgvnz5wMQg3g0btwYN954I7p27YqgoCBs2LABu3btwuuvvw4AVV5PRER1BZMgIqJaEB4ejp9++gmPPvoonnzySTRo0AC33347hg4dipEjR6odHgBxIL927VrMnj0bTz31FOLi4vDss8/i2LFjlY5ep9fr8eOPP9r7gRiNRkyYMAEzZ85E165daxSPRqPBDz/8gEceeQRffPEFJEnCDTfcgNdffx3du3ev0Xs6e/zxx9GmTRu88cYbeOaZZwAAcXFxGDFiBG644QYAQNeuXTFy5Ej8+OOPOH/+PAICAtC1a1esXbsWV111lf29Fi5ciOnTp+PJJ59Efn4+pk6desVJUFxcHDZt2oSHHnoIL774IiIjIzFjxgwEBgbioYcegtForNL7LFiwoNQ0rVaLJ554AsOGDcMvv/yCp59+GvPmzYNer8fgwYPx8ssvo3nz5gCAgIAAPPDAA/j111/to+q1atUK7777rn3kvqquJyKiukKS69JpSCIiqnPGjx+PI0eO4OTJk2qHQgAeeeQRvP/++7h8+bLqA0wQEXkr9gkiIiK7/Px8l8cnT57EmjVrMGTIEHUCqudKfh8ZGRn4/PPPcfXVVzMBIiK6AqwEERGRXUxMDKZNm4YWLVogISEBS5YsQWFhIfbt24fWrVurHV69061bNwwZMgTt27dHamoqPv74YyQlJWHjxo0uowwSEVH1sE8QERHZjRo1Cl999RVSUlJgMBjQr18/vPjii0yAVHLdddfhu+++wwcffABJktCjRw98/PHHTICIiK4QK0FERERERFSvsE8QERERERHVK0yCiIiIiIioXvHqPkFWqxVJSUkIDg6u8kXjiIiIiIjI98iyjJycHMTGxkKjqbjW49VJUFJSEuLi4tQOg4iIiIiI6ojExEQ0bty4wnm8OgkKDg4GID6oyWRSORoiIiIiIlJLdnY24uLi7DlCRbw6CVKawJlMJiZBRERERERUpW4yHBiBiIiIiIjqFSZBRERERERUrzAJIiIiIiKiesWr+wQRERERke+wWCwoLi5WOwyqo7RaLXQ6nVsujcMkiIiIiIhUd/nyZZw7dw6yLKsdCtVhAQEBiImJgZ+f3xW9D5MgIiIiIlKVxWLBuXPnEBAQgMjISLec6SffIssyioqKcOHCBcTHx6N169aVXhC1IkyCiIiIiEhVxcXFkGUZkZGR8Pf3VzscqqP8/f2h1+uRkJCAoqIiGI3GGr8XB0YgIiIiojqBFSCqzJVUf1zexy3vQkRERERE5CWYBBERERERUb3CJIiIiIiIqI5o1qwZ3nzzzSrPv3nzZkiShMzMTI/F5IuYBBERERERVZMkSRX+zZ8/v0bvu2vXLkyfPr3K8/fv3x/JyckICQmp0fKqyteSLY4OR0RERERUTcnJyfb733zzDebNm4cTJ07YpwUFBdnvy7IMi8UCna7yQ+/IyMhqxeHn54fo6OhqvYZYCaKKJO0DPp8ApBxSOxIiIiKqR2RZRl6RWZW/ql6sNTo62v4XEhICSZLsj48fP47g4GCsXbsWPXv2hMFgwNatW3H69GmMGzcODRs2RFBQEHr37o0NGza4vG/J5nCSJOGjjz7ChAkTEBAQgNatW+OHH36wP1+yQrNs2TKEhoZi3bp1aN++PYKCgjBq1CiXpM1sNuOhhx5CaGgowsPDMWfOHEydOhXjx4+v8Xd26dIlTJkyBQ0aNEBAQABGjx6NkydP2p9PSEjA2LFj0aBBAwQGBqJjx45Ys2aN/bWTJ0+2D5HeunVrLF26tMaxVAUrQVS+gyuA078BMd2A6M5qR0NERET1RH6xBR3mrVNl2UefHYkAP/ccIj/++ON47bXX0KJFCzRo0ACJiYm47rrr8MILL8BgMOCzzz7D2LFjceLECTRp0qTc93nmmWfwyiuv4NVXX8WiRYswefJkJCQkICwsrMz58/Ly8Nprr+Hzzz+HRqPB7bffjtmzZ2P58uUAgJdffhnLly/H0qVL0b59e7z11ltYvXo1rrnmmhp/1mnTpuHkyZP44YcfYDKZMGfOHFx33XU4evQo9Ho9ZsyYgaKiImzZsgWBgYE4evSovVr21FNP4ejRo1i7di0iIiJw6tQp5Ofn1ziWqmASROUzF4hbS5G6cRARERF5oWeffRbDhw+3Pw4LC0PXrl3tj5977jmsWrUKP/zwA2bOnFnu+0ybNg233XYbAODFF1/E22+/jZ07d2LUqFFlzl9cXIz33nsPLVu2BADMnDkTzz77rP35RYsWYe7cuZgwYQIAYPHixfaqTE0oyc+2bdvQv39/AMDy5csRFxeH1atX46abbsLZs2cxadIkdO4sTqy3aNHC/vqzZ8+ie/fu6NWrFwBRDfM0JkFUPmux7dasbhxERERUr/jrtTj67EjVlu0uykG94vLly5g/fz5+/vlnJCcnw2w2Iz8/H2fPnq3wfbp06WK/HxgYCJPJhLS0tHLnDwgIsCdAABATE2OfPysrC6mpqejTp4/9ea1Wi549e8JqtVbr8ymOHTsGnU6Hvn372qeFh4ejbdu2OHbsGADgoYcewv33349ff/0Vw4YNw6RJk+yf6/7778ekSZOwd+9ejBgxAuPHj7cnU57CPkFUPost+bEUqxsHERER1SuSJCHAT6fKnyRJbvscgYGBLo9nz56NVatW4cUXX8Qff/yB/fv3o3PnzigqqrjVjV6vL7V+KkpYypq/qn2dPOXuu+/GP//8gzvuuAOHDh1Cr169sGjRIgDA6NGjkZCQgP/85z9ISkrC0KFDMXv2bI/GwySIysdKEBEREZHbbNu2DdOmTcOECRPQuXNnREdH48yZM7UaQ0hICBo2bIhdu3bZp1ksFuzdu7fG79m+fXuYzWb89ddf9mkZGRk4ceIEOnToYJ8WFxeH++67DytXrsSjjz6KDz/80P5cZGQkpk6dii+++AJvvvkmPvjggxrHUxVsDkflszAJIiIiInKX1q1bY+XKlRg7diwkScJTTz1V4yZoV+LBBx/EggUL0KpVK7Rr1w6LFi3CpUuXqlQFO3ToEIKDg+2PJUlC165dMW7cONxzzz14//33ERwcjMcffxyNGjXCuHHjAACPPPIIRo8ejTZt2uDSpUvYtGkT2rdvDwCYN28eevbsiY4dO6KwsBA//fST/TlPYRJE5VOSHyZBRERERFds4cKF+Pe//43+/fsjIiICc+bMQXZ2dq3HMWfOHKSkpGDKlCnQarWYPn06Ro4cCa228v5QgwYNcnms1WphNpuxdOlSPPzww7j++utRVFSEQYMGYc2aNfameRaLBTNmzMC5c+dgMpkwatQovPHGGwDEtY7mzp2LM2fOwN/fHwMHDsTXX3/t/g/uRJLVbiB4BbKzsxESEoKsrCyYTCa1w/E9y28GTq4DOk4EbvLsWO1ERERUfxUUFCA+Ph7NmzeH0WhUO5x6x2q1on379rj55pvx3HPPqR1OhSraVqqTG7ASROVjnyAiIiIin5OQkIBff/0VgwcPRmFhIRYvXoz4+Hj861//Uju0WsOBEah87BNERERE5HM0Gg2WLVuG3r17Y8CAATh06BA2bNjg8X44dQkrQVQ+JkFEREREPicuLg7btm1TOwxVsRJE5WNzOCIiIiLyQUyCqHxKJYgXSyUiIiIiH8IkyF1W3gu8EAPsWaZ2JO5jHyLbom4cRERERERuxCTIXaxmoDgPKMpTOxL3sfcJYiWIiIiIiHwHkyB30fuL22IfSoLYJ4iIiIiIfBCTIHfRB4jb4nx143Ani9n1loiIiIjIBzAJchd7JciHkiBWgoiIiIjqhPnz56Nbt25qh+EzmAS5i70S5EPN4SxF4pZ9goiIiIhcSJJU4d/8+fOv6L1Xr17tMm327NnYuHHjlQVdBfUl2eLFUt3FFytBSjM4VoKIiIiIXCQnJ9vvf/PNN5g3bx5OnDhhnxYUFOTW5QUFBbn9PeszVoLcxZcHRmCfICIiIqpNsgwU5arzJ8tVCjE6Otr+FxISAkmSXKZ9/fXXaN++PYxGI9q1a4d3333X/tqioiLMnDkTMTExMBqNaNq0KRYsWAAAaNasGQBgwoQJkCTJ/rhkhWbatGkYP348XnvtNcTExCA8PBwzZsxAcbGjBU9ycjLGjBkDf39/NG/eHF9++SWaNWuGN998s8ZfzaFDh3DttdfC398f4eHhmD59Oi5fvmx/fvPmzejTpw8CAwMRGhqKAQMGICEhAQBw4MABXHPNNQgODobJZELPnj2xe/fuGsdyJVgJche/QHHrS0mQhX2CiIiISAXFecCLseos+4kkx3FdDS1fvhzz5s3D4sWL0b17d+zbtw/33HMPAgMDMXXqVLz99tv44Ycf8O2336JJkyZITExEYmIiAGDXrl2IiorC0qVLMWrUKGi12nKXs2nTJsTExGDTpk04deoUbrnlFnTr1g333HMPAGDKlClIT0/H5s2bodfrMWvWLKSlpdX4c+Xm5mLkyJHo168fdu3ahbS0NNx9992YOXMmli1bBrPZjPHjx+Oee+7BV199haKiIuzcuROSJAEAJk+ejO7du2PJkiXQarXYv38/9Hp9jeO5EkyC3MXXmsPJMiDbLpLKPkFEREREVfb000/j9ddfx8SJEwEAzZs3x9GjR/H+++9j6tSpOHv2LFq3bo2rr74akiShadOm9tdGRkYCAEJDQxEdHV3hcho0aIDFixdDq9WiXbt2GDNmDDZu3Ih77rkHx48fx4YNG7Br1y706tULAPDRRx+hdevWNf5cX375JQoKCvDZZ58hMFAkiosXL8bYsWPx8ssvQ6/XIysrC9dffz1atmwJAGjfvr399WfPnsVjjz2Gdu3aAcAVxXKlmAS5i68NjGBxSnxYCSIiIqLapA8QFRm1ln0FcnNzcfr0adx11132igwAmM1mhISEABBN2YYPH462bdti1KhRuP766zFixIhqL6tjx44ulaKYmBgcOnQIAHDixAnodDr06NHD/nyrVq3QoEGDmn40HDt2DF27drUnQAAwYMAAWK1WnDhxAoMGDcK0adMwcuRIDB8+HMOGDcPNN9+MmJgYAMCsWbNw99134/PPP8ewYcNw00032ZOl2sY+Qe7ia5Ug5+oP+wQRERFRbZIk0SRNjT9b062aUvrHfPjhh9i/f7/97/Dhw9ixYwcAoEePHoiPj8dzzz2H/Px83HzzzbjxxhurvaySTckkSYLVar2i+K/U0qVLsX37dvTv3x/ffPMN2rRpY//c8+fPx5EjRzBmzBj89ttv6NChA1atWqVKnEyC3IWVICIiIqJ6r2HDhoiNjcU///yDVq1aufw1b97cPp/JZMItt9yCDz/8EN988w3+97//4eLFiwBEcmOxWK4ojrZt28JsNmPfvn32aadOncKlS5dq/J7t27fHgQMHkJuba5+2bds2aDQatG3b1j6te/fumDt3Lv7880906tQJX375pf25Nm3a4D//+Q9+/fVXTJw4EUuXLq1xPFeCzeHcxecqQU6JD/sEEREREVXZM888g4ceegghISEYNWoUCgsLsXv3bly6dAmzZs3CwoULERMTg+7du0Oj0WDFihWIjo5GaGgoADFC3MaNGzFgwAAYDIYaNWFr164dhg0bhunTp2PJkiXQ6/V49NFH4e/vbx+ooDz5+fnYv3+/y7Tg4GBMnjwZTz/9NKZOnYr58+fjwoULePDBB3HHHXegYcOGiI+PxwcffIAbbrgBsbGxOHHiBE6ePIkpU6YgPz8fjz32GG688UY0b94c586dw65duzBp0qRqfzZ3YBLkLr6WBCkXSgUA2QpYrYCGhUMiIiKiytx9990ICAjAq6++isceewyBgYHo3LkzHnnkEQAioXjllVdw8uRJaLVa9O7dG2vWrIHGdqz1+uuvY9asWfjwww/RqFEjnDlzpkZxfPbZZ7jrrrswaNAgREdHY8GCBThy5AiMRmOFr/v777/RvXt3l2lDhw7Fhg0bsG7dOjz88MPo3bs3AgICMGnSJCxcuBAAEBAQgOPHj+PTTz9FRkYGYmJiMGPGDNx7770wm83IyMjAlClTkJqaioiICEycOBHPPPNMjT7blZJkuYqDoddB2dnZCAkJQVZWFkwmk7rB5KQAr7cFJA0w7+IVtydV3aUE4K0ujsdPXgB0furFQ0RERD6roKAA8fHxaN68eaUH6FRz586dQ1xcHDZs2IChQ4eqHU6NVLStVCc3YCXIXZQ+QbJVVFF0BnXjuVIl+wFZzQCYBBERERF5i99++w2XL19G586dkZycjP/+979o1qwZBg0apHZoqmMS5C5KczhAXG3Y25MgS4l+QOwXRERERORViouL8cQTT+Cff/5BcHAw+vfvj+XLl6t2gdK6hEmQu2j1gEYvkgVf6BdUMumxXtkIJURERERUu0aOHImRI0eqHUadVGd6ur/00kuQJMneYcwr2YfJ9oEkqGQlqORjIiIiIiIvVSeSoF27duH9999Hly5dKp+5LrOPEOcD1woqs08QERERked48XhdVEvctY2ongRdvnwZkydPxocfflijMdDrFF8aJpt9goiIiKiWaLVaAEBRUVElc1J9l5cnig1X2q9J9T5BM2bMwJgxYzBs2DA8//zzFc5bWFiIwsJC++Ps7GxPh1c99uZwvlAJYp8gIiIiqh06nQ4BAQG4cOEC9Hq9/Xo5RApZlpGXl4e0tDSEhobaE+eaUjUJ+vrrr7F3717s2rWrSvMvWLBAtQsqVYkvV4LYJ4iIiIg8RJIkxMTEID4+HgkJCWqHQ3VYaGgooqOjr/h9VEuCEhMT8fDDD2P9+vVVvijW3LlzMWvWLPvj7OxsxMXFeSrE6vPzoUpQqeZw7BNEREREnuPn54fWrVuzSRyVS6/XX3EFSKFaErRnzx6kpaWhR48e9mkWiwVbtmzB4sWLUVhYWOpDGgwGGAx1+Po7Pt0cjpUgIiIi8iyNRlPlk+NEV0K1JGjo0KE4dOiQy7Q777wT7dq1w5w5c9yW5dUqX24Oxz5BREREROQjVEuCgoOD0alTJ5dpgYGBCA8PLzXda/hUJahE8zf2CSIiIiIiH8GhN9zJpytB7BNERERERL5B9SGynW3evFntEK6MT10slX2CiIiIiMg3sRLkTvbmcL5QCSpR+WGfICIiIiLyEUyC3MmXkqCSlR/2CSIiIiIiH8EkyJ18aWAES4kx+tkniIiIiIh8BJMgd1L6BBX5QhJUsjkcK0FERERE5BuYBLmTTw+MwD5BREREROQbmAS5ky/1CSrZB4h9goiIiIjIRzAJcidfuk5QqUoQ+wQRERERkW9gEuROPjUwAvsEEREREZFvYhLkTj5dCWKfICIiIiLyDUyC3MkvUNz6QhLEPkFERERE5KOYBLmTT40OV7I5HPsEEREREZFvYBLkTkoSZC32/spJqYulevnnISIiIiKyYRLkTsrACID3V4NKJnHsE0REREREPoJJkDtp/QDJtkq9vV+Q0vxN6yduvb2yRURERERkwyTInSTJd4bJVpIeexM/9gkiIiIiIt/AJMjdfGWYbKUPkI5JEBERERH5FiZBbpJdUIyzGXmw6nwkCWIliIiIiIh8lE7tAHzFrG8OYMOxVOyP8EMo4P3N4ZSkR0mC2CeIiIiIiHwEK0FuYvIX+WSRZBATWAkiIiIiIqqTmAS5icmoBwAUKElQUa6K0biB0idIGeiBSRARERER+QgmQW5i8rclQbJtSGlWgoiIiIiI6iQmQW5iMormcHlQkiAv7xOkJEE6o+tjIiIiIiIvxyTITZRKUK7VRypBbA5HRERERD6KSZCbKH2CLvtKEmRRRoezVYKYBBERERGRj2AS5CbK6HA5FpEMeX1zOFaCiIiIiMhHMQlyE6USlG1WkiBvrwSxTxARERER+SYmQW4SYusTlGm2XX/W6ytBSnM4VoKIiIiIyLcwCXITpRLkM83hOEQ2EREREfkoJkFuEmQbIjsftoulen1zuCJxyySIiIiIiHwMkyA30WokBBt0yJd94DpBsgzIFnFfSYLYJ4iIiIiIfASTIDcy+etR4AuVIOeEx14JsqgTCxERERGRmzEJcqNgow758IFKkNUpCdL5l55GREREROTFmAS5kcmoR77sq5Ug9gkiIiIiIt/AJMiNTP465PlCczjnhId9goiIiIjIxzAJciOTUY8CX2gOpyQ8khbQ2K57xD5BREREROQjmAS5kcnfqTlckRcnQUr/H61e/AFsDkdEREREPoNJkBuZnAdGsBR6b/VEqQRp/ZwqQWwOR0RERES+gUmQG5n89Y6LpQLe2y9ISYI0OkDDShARERER+RYmQW5kMupRCL1jgrcmQc7N4TRacd/CJIiIiIiIfAOTIDcy+esgQ4NC+whxXtovyF4JYp8gIiIiIvI9TILcyGQUCUOBpIwQ562VIFvCo9WxTxARERER+RwmQW5k8hdJUJ7sQ5UgpU+QbAWsVvViIiIiIiJyEyZBbqRUgvJkb68EldEnCGCTOCIiIiLyCUyC3MjkL5qOOZIgb60E2ZIdjc7RJwhgEkREREREPoFJkBsFGUQSlO/tAyO4VIJ0pacTEREREXkxJkFupNNqEGTQocDbm8O5XCzVuRLkpRd/JSIiIiJywiTIzUxGnfdXgixF4lajAzQaAJJtOitBREREROT9mAS5WbBRj3x4eSXIPkS23vWWfYKIiIiIyAcwCXIzk78O+b40RDbAawURERERkU9hEuRmJqPeqTmct1aCnAZGAJySIPYJIiIiIiLvxyTIzUz+PtAcznmIbOdb9gkiIiIiIh/AJMjNTEan5nBFueoGU1PlVoLYJ4iIiIiIvB+TIDfzjUpQiT5B9oERWAkiIiIiIu/HJMjNTEY9Crx9iGx7JUhpDqe1TWefICIiIiLyfkyC3EyMDuftlSBliGzb51AqQuwTREREREQ+gEmQm/nE6HD2i6WyTxARERER+R4mQW4m+gT5WHM49gkiIiIiIh/CJMjNTEa908VSvbUSpAyRrVSC2CeIiIiIiHwHkyA3M/nrvH90uFJDZLNPEBERERH5DiZBbubcJ0gu9tLrBJUcIpt9goiIiIjIhzAJcrNgo1MlqCgXkGV1A6oJJdlhnyAiIiIi8kFMgtxMp9UgXx8GAJDMBUBhtsoR1UCpShD7BBERERGR72AS5AF+/kHIkgPEg+xkdYOpCfYJIiIiIiIfxiTIA0xGPVJkUQ1CTpK6wdSEpWQSxD5BREREROQ7mAR5QLBRh1S5gXjgjZWgks3hlL5B7BNERERERD6ASZAHmPy9vBJUqjmckgSxTxAREREReT8mQR5gMuqQAl+oBNmSH/YJIiIiIiIfwiTIA0z+eqTaK0FemATZh8hmnyAiIiIi8j2qJkFLlixBly5dYDKZYDKZ0K9fP6xdu1bNkNxCDIygVIK8sDkc+wQRERERkQ9TNQlq3LgxXnrpJezZswe7d+/Gtddei3HjxuHIkSNqhnXFTP46pMjh4oE3JkHsE0REREREPkyn5sLHjh3r8viFF17AkiVLsGPHDnTs2FGlqK6cSyUo94KorCgJhTew2Jq9sU8QEREREfkgVZMgZxaLBStWrEBubi769etX5jyFhYUoLCy0P87Ozq6t8KrF5K/HRQTDDB10MAM5KUBonNphVV25lSD2CSIiIiIi76f6wAiHDh1CUFAQDAYD7rvvPqxatQodOnQoc94FCxYgJCTE/hcXVzcTC5NRDxkapEteOjiC/WKpfrZbJkFERERE5DtUT4Latm2L/fv346+//sL999+PqVOn4ujRo2XOO3fuXGRlZdn/EhMTaznaqjH5i6QhDV46OEKpIbKZBBERERGR71C9OZyfnx9atWoFAOjZsyd27dqFt956C++//36peQ0GAwwGQ22HWG0mo2hGlmRtgC4SvK8SVKo5HPsEEREREZHvUL0SVJLVanXp9+ONwoJEM7LzFm+vBLFPEBERERH5HlUrQXPnzsXo0aPRpEkT5OTk4Msvv8TmzZuxbt06NcO6YsEGHYx6DVKstiTI6ypBJS6Wyj5BRERERORDVE2C0tLSMGXKFCQnJyMkJARdunTBunXrMHz4cDXDumKSJCEy2IDUTNvACNlelgSxTxARERER+TBVk6CPP/5YzcV7VFSwESmXlOZw59UNprrYJ4iIiIiIfFid6xPkKyKDDEiG0xDZsqxuQFUly46Kj71PkFbcshJERERERD6ASZCHRJkMSJNtlSBzAZB/Sd2Aqso50VH6AikVISZBREREROQDmAR5SFSwAYXwQ642REzwlsERLEWO+8rFUtkniIiIiIh8CJMgD4kMFtczytCEiwneMjiCc78fDfsEEREREZHvYRLkIVHBRgBAKpRhsp2uFZRxWvzVRS7N4dgniIiIiIh8D5MgD1EqQefNyghxtkpQ3kXgg2uAj4YBxQUqRVcBpdojaQFJEvfZJ4iIiIiIfAiTIA+JsiVBCcUmMUGpBB3+H1CYBeRfBDITVIquAiWHxwbYJ4iIiIiIfAqTIA8JC/SDJAHJcokLpu5f7pjp0plaj6tS9gulOidB7BNERERERL6DSZCH6LQahAcakKIMk52dBKQeBZL2OWa6VBcrQbZqj9bpOrr2PkGW2o+HiIiIiMjNmAR5UFSwAalKJSgnybUKBHhPJcjeJ4iVICIiIiLyfkyCPCgy2OBoDpeXARz4WtxvOVTc1sUkiH2CiIiIiMjHMQnyoKhgAzIRBLNku+hoXjoQGAn0uUc8rotJkKWsJIh9goiIiIjIdzAJ8iAxTLaEbH2EY2KXW4Dw1uL+pTOALKsRWvnKHBiBfYKIiIiIyHcwCfIgZZjsDE24Y2K3fwGhcQAkoDgXyE1XJ7jylNUcrrb6BBXlAsuuB/543bPLISIiIqJ6jUmQB0UGGwHAMThCTDegYUdAZwBMjcS0utYkzmLr96NxHh2ulvoEndsNnPkD+OMNVp2IiIiIyGOYBHlQlElUgrbKnQFIwICHHU82aCZu61oSVObACEqfIA8nQfmXxG1RDpB6xLPLIiIiIqJ6i0mQBynN4T7NHwj5iSSg00THk3U1CaqwT5CHk6CCTMf9szs8uywiIiIiqreYBHlQpC0Jyi+24LJV7/pkXU2C1OwTlJ/puJ/IJIiIiIiIPINJkAcF+OkQZBD9aS7kFLo+WVeTIDX7BLlUgv7y7LKIiIiIqN5iEuRhSjUozVuSoIr6BMlWwGr13LILshz3s88BmYmeWxYRERER1VtMgjxMSYJKV4Kaitvs84C5xHNqshSJW62fY5rSJwjwbDXIuTkcACSyGkRERERE7sckyMPKrQQFRgL6AAAykHWu9gMrT1nN4ZyrQp7sF6Q0hwuwXVeJgyMQERERkQcwCfKwKHsSVOD6hCQ5NYmLr92gKlJmczinhKg2KkGtR4hbJkFERERE5AFMgjwsynbB1FLN4YC62S+ozCGynStBHryIqVIJUpKgtCNAQbbnlkdERERE9RKTIA8rt08QUDeTIHslyHl0OA0ASdy3eLA5nFIJimwn1o1sBc7t8tzyiIiIiKheYhLkYVHelgTZ+wSVuK6R/VpBHmoOJ8uO0eH8Q4G4q8R9NokjIiIiIjdjEuRh5Q6MANTNJKisPkGA07WCPFQJKswBZFtTO2Mo0KSvuM+LphIRERGRmzEJ8jClEnQxtwjFlhLX2LEnQQmiElIX2PsE6VynK5UhT/UJUvoDaf0Avb+jEnRuj2eb4BERERFRvcMkyMMaBPhBpxH9adIvl6gGhTYRt4XZQP6lWo6sHEpzt1KVINu1gjyVkCj9gYyhYuS8yHaAMQQozgVSDnlmmURERERULzEJ8jCNRkJEkK1JXHaJJEjvDwTHiPt1ZZjssi6WCni+T5BSCfIPFbcaDdCol7iffMAzyyQiIiKieolJUC2IMnnR4AhlDZENeL5PkHMlSGGyJYh56Z5ZJhERERHVS0yCakFkUAWDI4Q2FbcJ2x0js6nJ3hyuZJ8gJQnycJ8gpRIEAAHh4jbvomeWSURERET1EpOgWqBUglKzC0o/GdlW3O76EHirK/DHQnUP+iurBHmqT5AyPLZzJUhJgnJZCSIiIiIi92ESVAsaNwgAACReyiv9ZN97gUGPAQERQPY5YOMzwOLeYshoNZQ3RLan+wTZm8OFOKYFRIjbvAzPLJOIiIiI6iUmQbWgabhIghIyykiC/AKBa58E/nMEGL9EJAF56UDywVqO0qbcIbI93CeowuZwTIKIiIiIyH2YBNWCpmGBAMpJghR6I9DtX47r46QdrYXIylDuENke7hNU1sAI7BNERERERB7AJKgWNLFVgtIvF+JyYSXNyRp2ELepRzwcVTlU6xOUKW6dK0GBShLEPkFERERE5D5MgmpBiL8eYYHiujsJGbkVzxzVUdyqVglSu09QqGOaUgkqzgOKKqiiERERERFVA5OgWtIkTFSDzlbUJA5wVILSjgGy7OGoymApJwlSo0+QweSoSOWzSRwRERERuQeToFrSzNYk7kxlSVB4a5FwFGYDWYm1EFkJlV4stRb7BEkSh8kmIiIiIrdjElRLmoSLwRHOXqykOZzOD4hoI+6nqtAkrrzmcJ7sEyTLZVeCAI4QR0RERERuxySoltgrQelV6NsSpTSJU2FwBIutz0/JIbI92SeoKNfxvs6VIAAICBO3HCGOiIiIiNykRklQYmIizp07Z3+8c+dOPPLII/jggw/cFpivUa4VdPZiFZIg+whxdakSpHV93p2UKpBGJ66b5CyQF0wlIiIiIveqURL0r3/9C5s2bQIApKSkYPjw4di5cyf+7//+D88++6xbA/QVTW3N4ZKy8lForqRfjZojxJXbJ0ipBHmgT5BzfyBJcn0ugMNkExEREZF71SgJOnz4MPr06QMA+Pbbb9GpUyf8+eefWL58OZYtW+bO+HxGeKAfAv20kGUg8WJ+xTMrlaD0vwFzkeeDc2a/WGqJ5nCe7BNUXn8goGp9gqxW4LPxwNeT1RlRj4iIiIi8So2SoOLiYhgMBgDAhg0bcMMNNwAA2rVrh+TkZPdF50MkSbJXgyq9VlBInBge2moGMk7WQnROyqsEebJPUEGWuC3ZHwioWhKUlQj8swk4/hObzRERERFRpWqUBHXs2BHvvfce/vjjD6xfvx6jRo0CACQlJSE8PNytAfoSpV9QQmXDZEsSENVe3K/tfkEWW+Wpun2C4rcA+7+q2TKV5nAVVYJyK0huclIc9zNO1ywGIiIiIqo3apQEvfzyy3j//fcxZMgQ3HbbbejatSsA4IcffrA3k6PSqlwJAtQbIc7eHM7PdXpFfYJkGfh2CrD6vpolIUpzuJpWgnKSHPcvMgkiIiIioorpKp+ltCFDhiA9PR3Z2dlo0KCBffr06dMREBDgtuB8jTJMdkKVRoizDY5Q65UgpTlcNfoEZScB+ZfE/bSjQHjL6i2zKpWgCpMgp0rQxX+qt2wiIiIiqndqVAnKz89HYWGhPQFKSEjAm2++iRMnTiAqKsqtAfqSJlVtDgc4VYJqOQkqb4jsivoEpZ9wuv936ecLc4Csc6WnK+yVoJDSzzkPkV3eoAfZTpUgNocjIiIiokrUKAkaN24cPvvsMwBAZmYm+vbti9dffx3jx4/HkiVL3BqgL2lmaw537lIezBZrxTMrI8RlJToGDvA0WXYkOaWGyFb6BJWVBJ0s+77im9uBt7sDSfvKXq7zENkl+dsulipbHMlSSawEEREREVE11CgJ2rt3LwYOHAgA+O6779CwYUMkJCTgs88+w9tvv+3WAH1JtMkIP50GxRYZyVkFFc/s3wAIjhX30455PjjANcEpNUR2BZWgCxVUgsyFwJmtYsCFzS+XvdyKhsjWGwG/IHE/72LZr89xGpHw4j8cJpuIiIiIKlSjJCgvLw/BwcEAgF9//RUTJ06ERqPBVVddhYSEBLcG6Es0GglNwqrRJK5hBU3iZBk4+oNrU7Ar5dzfp1QlqII+Qc6JT/pJ1yTkwglH4vT3WiD5YOnXV1QJAirvF+ScBBVmc5hsIiIiIqpQjZKgVq1aYfXq1UhMTMS6deswYsQIAEBaWhpMJpNbA/Q1TW1J0JnqjBCXcrj0c4dWAN/eAXx3l/uCcx7+ulSfIFsSVGZzOKckqDAbuJzqeJxaIvYtr5Z+fUWVIMBpmOz00s/JMpBtS4KURI39goiIiIioAjVKgubNm4fZs2ejWbNm6NOnD/r16wdAVIW6d+/u1gB9TbWGyW7cW9yeWFt6aOp9n4vbs38Cl864JziLU4JTXiWoZBKUn+lIeoIailvn5nGptiG+mw8Wt8d+KN2870oqQYU5QLFtXcZ0E7ccJpuIiIiIKlCjJOjGG2/E2bNnsXv3bqxbt84+fejQoXjjjTfcFpwvqvIFUwGgzUjRNygnCTi9yTE96xwQ/4fj8eH/uSe47YvErX8DQFNi0ygvCVIGQgiOAWJ72KY5VYZSDonbzjcC7W8Q9/943fG8LFdeCXIeIa4kpSmcIQSI7izuc3AEIiIiIqpAjZIgAIiOjkb37t2RlJSEc+fE8Md9+vRBu3bt3BacL6pWEqQzAJ1vFveVyg8AHPwWgAxoDeLxoe+uPLDdS4GttgR25ILSzyuVoZJ9gpThsSPaABGtbdNsiZEsO5rDNewEDHpM3D/8P0eTteJ8MWgCULNKkJIEBUc7rk/E5nBEREREVIEaJUFWqxXPPvssQkJC0LRpUzRt2hShoaF47rnnYLVWMvRzPWdvDncxF3JVRjHrcYe4Pf4zkGu7Vs7Bb8S0a58EtH5i4ASl2VlNnFwP/PyouD/kCaDbbaXnKW+IbKXqE9FG/DlPy0kRiYukAaLaAzFdgDajANkKbF0o5lGG/5a0gCG47PgCbMNkl5UEKf2BTDFAWAtxn5UgIiIiIqpAjZKg//u//8PixYvx0ksvYd++fdi3bx9efPFFLFq0CE899ZS7Y/QpjRv4w0+rQUGxFecu5Vf+gujOQExXMWjBoRVA8gHgwnFAZwR6TgVai0EpalwNSjkMrJgmrsPT9V/A4P+WPV95F0u9YEt4Its6JUG2SpBSBQpvDej9xf2rZ4nbwytFfx7nC6VKUtnLrrASZBsdLzgGCLNVgjhMNhERERFVoEZJ0KeffoqPPvoI999/P7p06YIuXbrggQcewIcffohly5a5OUTfotdq0DZaVDwOn6/iRVC726pB+z4HDnwt7rcdLRKHTpPE40PfOQ78ZRk4uwPISS39XiVtewsougw0HwSMfav8RKTcPkHOlSBbc7jsc0DhZUcSFN3JMX9cHyC8FVCcBxz70TEoQnn9gQAgoKI+QbYLpQbHAA2aAZA4TDYRERERVahGSdDFixfL7PvTrl07XLxYzgUtya5TIzGM+OGkKiZBnW8U/X9SDwN7lolpXW1N1tqMEhcTzToLJO4UI7z9PAv4ZCTw3b8rf29lpLarHgB0fuXPV1afIHMhcCle3I9oI5qtKQlLxinH0N4NOzpeI0lA11vF/QNfOVWCQstfdkVDZGc7VYL0RiCksW357BdERERERGWrURLUtWtXLF68uNT0xYsXo0uXLlcclK/rGBsCADh8PrtqL/BvALQfK+6b80Wi0fJa8dgvAGh3vbi/Zynw5c3A7k/E45SDFTcLs1pFsgI4mrKVp6w+QRf/Ef17DCYxMIHz+6SfdBoUobPreymDPcT/4ejLVGElSGkOV0aCrVSCTDHiNqy5IzYiIiIiojLoavKiV155BWPGjMGGDRvs1wjavn07EhMTsWbNGrcG6Is6NVKSoCzIsgypvCZozrrfDhy29fvpfKPrxUw73wgc/FpUVgBAHyCamxVmA/mXHAMLlJR9TiRVGj0Q2rTi5ZfVJ0i5HlBEa0czuojW4tpFKQcdfYOcm8MBQIOmQNOrgYStjoStokqQMkR2YZaoRDl/dvvocLHiNqwlEL+F1woiIiIionLVqBI0ePBg/P3335gwYQIyMzORmZmJiRMn4siRI/j8888rf4N6rl10MLQaCRm5RUjNLqzai5oPFn1pJC3QbbLrcy2GOKolwTHAnWuBIFtlpqILqSr9ecJaANpK8uGy+gTZ+wO1dUxTKkHHfhCDLfiHiZhKUprEZZ8XtxVVgowhYoQ5wLUaZLU69QmyfV4Ok01ERERElahRJQgAYmNj8cILL7hMO3DgAD7++GN88MEHVxyYLzPqtWgdFYTjKTk4fD4L0SHGyl+k0QBTfwJyL4ihpp1p9cB1rwEn1gLD5gMhjUS15XKKSIIa9Sj7PdOVpnCtq7D8MvoEKUlQpFNTOiUJUpKv6E5lD7bQYRywZjZgLhCPjSEVLFsrmgTmZQB56UBwQzE994JItCABQbZpHCabiIiIiCpR44ul0pWx9wuq6uAIgOj3UjIBUnSaCEz6UCRAgG2kNACZCeW/n72SU5UkSOkTZHFMu+B0oVRFyfcq2R9IYTQB7cY4PQ6tePlljRCnNIULinJUsjhMNhERERFVgkmQSuwjxFV1cITqUpKgiprDZdj67FQ2KALg1CfIVglyGVTBqTlcaBMxkp2iZH8gZ12dLspaUXM4oOxrBdn7Azk1t+Mw2URERERUCSZBKlEGRzhSnUpQdSgDHVTYJ8iWBIVXpRJUok9Q9jkx+IJG70i4AFExCm/leNywgiSoxTVAYJS4ryQ55VEGd3AeJrusJIjDZBMRERFRJarVJ2jixIkVPp+ZmVmthS9YsAArV67E8ePH4e/vj/79++Pll19G27ZtK3+xl2sfY4IkAclZBUi/XIiIIEPlL6oOeyWonOZwBdmOJCKiVdnzOCvZJ0hpShfesvSgChGtgbQjInGKrOC71OqAcYvFRVNbDa94+coIcc4DI2Tb4jeVGHghrDmQlSgqXU36Vvy+RERERFTvVKsSFBISUuFf06ZNMWXKlCq/3++//44ZM2Zgx44dWL9+PYqLizFixAjk5uZW+4N4myCDDs0jAgEAR5I80CROSYKyEsUFVEtSmrIFRolBBypTsk9Q6lFxW1Z/IqV5XUQbQFdJctdmpEiE9JUMDlHV5nCAo1/Q9zOANzoBX0wCdn5Y8fsTERERUb1RrUrQ0qVL3brwX375xeXxsmXLEBUVhT179mDQoEFuXVZd1Ck2BP9cyMXh81kY3CbSvW8eHANo/QBLkRiGukGJ6wApTeGqMigCULpP0LEfxW3TAaXnbT4Q2PKK44Ku7mBPgippDgcAXW4BTv4qPndWovg7tQFof4NjZDkiIiIiqrdqPES2J2Rlif4xYWFlX9yzsLAQhYWO6+pkZ3toUIFa0qmRCT8cSPJMvyCNRgxSkHFK9AsqmQRlVDMJcu4TdDEeOLdTXLun44TS8zYfBMw+5WjC5g5lVoKUawSVSIKa9gNmHRVN5y6cAL6dAuSmAZlnmQQRERERUd0ZGMFqteKRRx7BgAED0KlT2Z3pFyxY4NL8Li4urpajdK9OyjDZnh4hrqxhsu3DY1dhZDjAqU+QGTj8nbjffJDjIqUlBUWWfX2gmipriOzsJHFbsk+Q/TVhIiFSrh2Ufc598RARERGR16ozSdCMGTNw+PBhfP311+XOM3fuXGRlZdn/EhMTazFC91OuFXT2Yh6y8oormbsGKhomuzojwwFOfYKKgYMrxP3ON11JdNWjjA6XkyqG5zYXAvm2QRJKVoJKMsWK26zznouPiIiIiLxGnUiCZs6ciZ9++gmbNm1C48aNy53PYDDAZDK5/HmzkAA94sL8AQBHkj3QJK68YbKtFsfw0dXtE1ScB6SfENcCaj/WLWFWSXhLwC9INGvb8a6jP5DWUPnADsoFZJXKERERERHVa6omQbIsY+bMmVi1ahV+++03NG/eXM1wVKE0iTviiSZx5Q2TnXkWsBSKBCK0SdXeS1Oi+1ibEYAx5IpDrDJjCDDyBXF/47PA6U3ifnB05c3uTLbEms3hiIiIiAgqJ0EzZszAF198gS+//BLBwcFISUlBSkoK8vPz1QyrVikXTd2fmOn+Ny+vOZwyPHZ4S0czt8oofYIUtdkUTtFjKtB6hEjgfnlcTFOaulVEqQSxORwRERERQeUkaMmSJcjKysKQIUMQExNj//vmm2/UDKtW9WgimnLtPXvJ/W+ujAiXlw4UXnZMtw+KUMWmcIBrsmQwiWSktkkScMMi0fzNXCCmVdYfCABMSnM4JkFEREREVAeaw5X1N23aNDXDqlVd40Kg1UhIzipAUqabK2DGEEd/GecR4pQkqKqDIgCOPkGA6Auk97/y+GoiOBoYs9DpcRWSoBBbc7icFMDigQEoiIiIiMir1ImBEeqzAD8dOsSIAR72JHiiGtRM3Do3iUu3NYer6vDYgGufoM43XmlUV6bTRKDLreJ+bPfK5w+IsDXnkx0DKhARERFRvcUkqA7o2VRUazySBJU1Qpy9OVyrqr+PzgB0v11UgZoNclt4NTZ+CfDADqDTpMrn1WgcfYc4QhwRERFRvcckqA7o0dST/YKaiVtlhLj8TDHMNFC95nAAMO4d4JYvAK2u8nk9TaMBotqL26pQmsRlcYQ4IiIiovqOSVAdoFSCjiRlI6/I7N43L9kc7q/3xW1oE8Do3ddZqhYOjkBERERENkyC6oDYECOiTUZYrDIOnnPzRVOdk6Dkg8CWV8Tja+e5dzl1HYfJJiIiIiIbJkF1gCRJnusXpAyTnZkArL4fsJpFvx61BzeobawEEREREZENk6A6wt4vyN1JUEgcIGnEdXVSDwMB4cCYN8Q1d+oTJQlinyAiIiKieo9JUB1hrwSdvQRZlt33xlq9Y1AAALj+DSAo0n3v7y2U5nAcHY6IiIio3mMSVEd0jDXBqNcgM68Y/6TnuvfNw1qK2043Ah3Gufe9vYXJlgjmpgHmQnVjISIiIiJVMQmqI/RaDbo0DgXggX5Bw54GBj4KXL/Qve/rTQLCAJ1R3Gc1iIiIiKheYxJUh/T0VL+g2O7A0HmAMcS97+tNJImDIxARERERACZBdUrPJiIJ2u3uJIgEDpNNRERERGASVKcoI8SdSruMS7lFKkfjg1gJIiIiIiIwCapTwgL90CoqCADwV3yGytH4ICZBRERERAQmQXXO1a0iAABbT6WrHIkPYnM4IiIiIgKToDpngC0J2naKlSC3U4bJzuYFU4mIiIjqMyZBdUzfFmHQaiTEp+fifGa+2uH4FlaCiKg8VovaERARUS1iElTHmIx6dGkshrLexiZx7qX0Ccq/CBTlqRsLEdUdJ34Bnm8IrJ8HyLLa0RAReZecFCD5gNpRVBuToDroanuTOCZBbmUMAfSB4n5OsrqxkHsV5VZtPlkGrNYrW5bVClw4Uf1EuigXWDsH+OFBYN8XQPopHnDXBeZCYO1/AWsxsO0tYMurakdUO3JSAUux2lHUfVYr15OnpR2v+j7c1xQXABf+rt1lXjoDZJwW+77yWMxAQTZQXEmLJKsV2PUxsLgPsGJa5fPXMTq1A6DSBrSKwKLfTmHbqXTIsgxJktQOyTdIkmgSl/43kHUOCG+pdkTeQ5aB7CTAFCvWY13y65PAjveAUQuAPveUPY8sA3//Aqz7P3FAM24R0GJI1ZdRkAX8/Stwaj1waiOQlw406gncuRbQGSp/fU4q8NUtQNI+8XjvZ+I2MAoY8jjQ69+u67U4H/hns+sPSsNOQGSbqseshpxU4NQGILYb0LCj2tFUza6PgcwEQB8AFOcBm14AjKFA3+llz5+TClw8DcRdBWiqeB4x7yKw/ing5AZgwENA3/sAjbb0fOZC4PhPwL7lgNEEjFkIBITV+KOVYrUCp38DdrwLnN4IRHcGbl8FBEVW7fVpx4FdHwENmgH9Z7ovrpIKcwC/oNL7mrTjwM+PAuEtgGueBIIbVv09E3cCkhZo3LPqr7kYD3xzB5B+AojrCzQfDDQfCEgaIPcCcDkN8A8F2t9Q9vdZF106A/wyF0jaD/S4A+h9T9W/f2eWYrEfu5wGxHQV//PBMeX/Pshy2c/99rw48eAXDHSeBHSfAjTqUfd+Zzwhfos4KXbpjNieBz9W8/eymMWttoLDekux+O53fWibIAFBDYGAcMBcIPZ/xXnid8diu0yLRg/0vgsYPKf0vijtOPDjw0DiDvE4vCWQlwGENK7556hlkix776nI7OxshISEICsrCyaTSe1w3KbQbEG3Z9Yjv9iCXx4ZiHbRvvPZVPfZeOCfTcD4JUDb0cCh78QZqG6Ta/ZDUF3HfwY2PicO2FteU/3Xy7I4ePn9VbGzue1rIKKV++N0ZrUC3z8AHPgK6DhBrDu9v+P5vIvA0dXiAKGsxNJSDGj1lS/nUgKQf0n8oFb1B/DgCmDl3eK+RgdM+xlocpXrPOmngF/miINzZ/1mAtc+BeiN5b9/1nlxwLjnU6Aop/Tzfe4FrnvFddq53UDRZfE5/BsAqUeBL28GshIB/zCg621A0l7g/F7AYjsT1/Ja4IZFIina+ymw5TXgcorr+0oaYOKHQOcbK18vZTm/Bzi8EghtCnS/HfALcH2+KFc0Z0jaJ2LLywB63w20G1P591GYA/y5GPhzEVBsO6PbpJ94ffuxVUsUa6K4ANj3ufjhjrtKHIg5L6swB0g5BJzdAST+BWScEj/mXW4Wz+dnAm93E9vd2LdFhXjzAvHcxA8d8ylSDgOfjRNJcHRnYMgTYj9S0YHf4f+JCmCeU2U/ri8w7h0gorVY7+f3Aid/BfYvF+td0aAZcOuXV5ZQXk4T75+0FziySpwEchbRFpjyPWCKKf8znNsNbH0DOPGzY/ptX4vP7jzftrdEc+PedwOhTcqPSZaB9JNi+zcEAwaTODF1/Gfxl3oIiOoI3PA20LiXeM2pDcCKO4HCbPHYLxgY/F+RUEoakchejBcnCkoue/cnwE//Efc7jAdGPA+ExlW83uK3AN9OEdtGZZoPEttLcHTl81amKA84+6fY72r14q8oD7icKv7MhUC324CwFtV7X3MRsH0x8PsrgNnp5IrWAHS9RayXqPYVJzKKrPPAd/92HPwqgmOAPtOBq+53/EbkXwI2vQjs/xLo9i9g+HOOfe6OJcAvj5d+/4adgL73Ap1vrnj/XJHsZLHuAiOq/pr8S8DRH8Q+pNOkqv1u1URhjmh6u/sT1+mTPq54/37Wtr5L/cadBL66Vew7Bj8ukpaSsV++AKyYCiRsE491/q7bQWWMIWLf2bAjkLgLOLdLnFCxFosWNkPniZOQdeBkQHVyAyZBddTUT3bi978v4Mkx7XH3wGru7Kh8388QTZEi2gCZiY6dgM5f7DgGPAwERVXvPa1WkSBEtnX8YJclNx1Y3EvsaI2hwL1bgAZNq7YMWQbifwc2LXD94QltCty1vmpnRGUZOPaD2MmbC0RyYi223ZrFrV8A0P8hoPVwx2t+egTYs8zxPo17A7d+JX5cjqwC1jwmDvAkrThoHPSYOBN07EfxujN/ADHdxE6y5bWOH1hLsdipn1wHnFwPXDgupjfqKZKTypLEtOPAh9eIA2BTYzHqX3AMcO8fIqG1FAN/LBRnGa3F4oxWvxniIEr58YnqKCoxrYYCfoGOuOK3AAe/EQewVtsZtvBWIiFoNVxUhr6ZLKbf9CnQcbw4OFnzmEhiFA2ai++9KEe8/l/fOhJFc6GoQmx8RnwfhhDxQ5N1VjxvaiReD4jlpR4SB3uTPhI/0FVRnA/8vU4kcol/OaYHhANXPSCS2vgt4sAz/nfH2T9nrUeKRK9BM7E9ZJ0TB9J5GeLvcqqoXOSmifnDWoozm7JtoAFjCNByqNimWg0r+/8rcac4gI7uLA5+/BtU/tmS9gGr7nNsN4A4oItsKw4yLqc5ErKSRr8ilrN+nlhuZDvgvm3iB/yXx4G/3gMgie3lmv8T/xdJ+4HPx5c+KI7tDnS5RbxHVHtxQJ+8XxwknFwvtn8AiGwvvrdtb4ntQWcUSVDqUce6AsQ23PVWse1lnhUHGOMWif/15P1A8kHx/Q14WFQhFNlJwOoHxPeo9RPrQqMpHa9fMNBjCtBmhJg/+7zYzqb+KF539k8gYbtIGLMSxfddrDT9lMS+M/0EEBQNzNjh+K62vS2qXYA4IdH5ZuDqR8T34SzrHPDzbODvteV/t3aSOKhu0ExUfGWLSCAtxSKpA8SJhcIc8T8OiM9wzRNAvwfFWfEd74mTIM50/sDAR0UyUfKstaVY7LfWzhHLi+0utpeUg8A/v4ttVecHBEYCARHAma1iOwuIACa8D7QeVoXPVQYlYV4/r/Jr2RlDgBs/Ef9PivxMkUi3GFL6f+zC3yKhu3BMPG42UOyr9ywTJ0ecGUJEIhkcLb7j4IZASBOx3wprIeZfOV0kuwaTSIRTDov/Q2U7NjUGhj4l9msbn3VN7KO7ADctE4n1Klu19Zr/Ewf2ez8Xv1HmAjE9IEJUyTvfKLY75bfDagXO7wZObxIV06b9ReIEiP3dzg/EyU5AnIxqNUwkqkENRYXRECzeq9hW/chMEEna0e8dy45sB1z3mqj8uYvFLH5XNr3oGKW2551im935vrid+mPpJAcQ39WPD4v73SYDI18U///xfwDf3A4UZDrmjWgLjHxBfF+XU8WxzsZnxHblFwxM+hBoM0p8L5lnxWv1ASJxdbkNAM5uF/97qYfL/kxtRon1VNlJhVrEJMgHfLDlNF5ccxzXtI3E0jv7qB2O79i0APj9JcfjqI7iB01ppqTzF808Bj7qWu2oyIZngK0LxQ5s8ndAi8Flz7dyutgBKmK6Af9eV/GZLkux2DFvX+wUo1HsOP/+BbgULw4cp60RPwYZp8VBf8YpcQa+2+1AYLiosqyZLX4kq6LLLcDIBcAfr4kDaEjAoNnAzg/FDjO0CRDVQcQAiAOC3AvivqQRP47OO2VFs4FAh3HiwPD0JsdZXUAkUVq940eo2UCRXBhCxI9WQLjtIDNIHPR8eK04GG8xBLj5M+CjYeJx88Gi0rb6AXHQCACtRwCjXnIkICd+EQmxcnZeZxQJmjEUOLHGNfamV4sDztbDXc+QKgfQBpNIbtbPA87tFOsqNE78uCia9AduXV5206b0k+Jg/vxu8TiooUgke0xxVDWsVuBHW18iSSsSoWYDxfd58lfxI678uOv9RZOtS/GuB1MavUjikvaJ+csSHCuaosR2E+3BdywRB5c6oziYSDsOFGaV/dqwFsDQp8X3m5MimsrsWVq6/13TAeIgv8M4UQXZMN/1/8IvGOhzN9DlVnGwfX6PqFD5BYltLqo9kHYM+ON1cdAVGAk07iOSPOdqiyKooThwbnKV+P/Y/bGY3vd+kQxbCsX312akY12vme2YL6wF0P9BYP188dkb9RIHEXs/A/563ylBKIfWT3yfAx4R+5rMRODHh8RZVOf13qQv0PkmkXRqdaLCumKaSGrKEhwrKiWth4sqycrprgebdpJIRGK7i/XQaZLYVwAiWf10rNhWdUbH/15JGr3YJwx4WGzb710t9jHdJgPj3xXJ3vKbAMjiYNT5gKlxb7FuW48U39GG+aJSqtE5Ehhzvtj3thoqttHGfcR+7ODXrnF0/Rcw9k0Rz4EvxXsp+x2dvzj4V7btRr3EAezWN8TjAY+Ig+k1/xWJnvN6jOstks3UQ6K/n3IyoPNNokJb0W9B+klRoUo9JB63GCIO9KM72yorseL/XpLECIQX44G0oyLB1AeI/1mNzvVERVBDkYRYzI7/v+Bo8flSj4oEUNIAw58V38GOd8W2WJgtqsk3LQOaDRDvlbBdVAkKMkVSMfJFkQBJkki8zu4Q/wfJ+8X/h3NCXpGYrmI5SkWqOF+cYPvtOfHZnEW2A3pOE99pXob4XzYXiBNMfe8X+2tl35p/SSRDOz9wfR//MFFd9m8g9nnKSReFwSROZNn3NxKAGhzeRnUQiYPyv9RhnDgxcemM+LMUiRMSDZqJ/wVzoZg3N138frUaJn5vnPf1Vos4KbjpBUclNrQJcMNicbxgtYgk9fhP4nfu7g2ulb5dHwM/z3KNMzhWVNa2vSW2kcZ9xP/2llfK2Q9AnIi79avqN6u2WkSV+o+FYvto3Ef8XzfpK45j6ljTRSZBPuBIUhbGvL0VAX5aHHh6BPRajmHhFuknRRk/qoM4wxRnSzBPbQA2v+Q4EG3QHLh+oTgwrsj+r4DV9zke+wUDd64BYrq4zndqI/DFRAAScPOnwI+PiDNpPacBY98q/b7mQtH2fvs7jgNZnVEcGF89SzRdufgP8PEIcRDQbCAQ1lyckXf+EdMaxE75n03iYE3rJ86shrUQO2yNrbmFRiduz2wVZ8Flq6OPBCCa7nS/Xay/5TeJA2xAvH7QbBFT6iFg88uisgOInXSPO4B214tK2a6PSlcaAiLEQVzr4WJdW4rFwe3uT8quSihnonV+oplTcCxw3x+iKpV2XCRGzmf/jaHAmNfFj0PJHfXlNPEDcuzH0klBYKSIu/sd5fchsBQDy653rcwZQ4BJn4izwXkXxcF7cZ74DipqEmYxi4RBtopllmyqBoiD8x9mih8jSWMbVKEKu++ghkCPqaLSqRxYHf6fSNwvHBcHx+2uF0mz89lWQBwQ/vyoo5oBiG0lvJWtLXmY+NGO7iKa+en8SsRsEUmMkqw5jx6kM4rPUZwHQBIHqGnHyj/jWJaOE4DrXheJviyL/4kLJ8SBUlCUiNEQ5JhflkVzoM0vOqY1GyjOvpbcPv5eJ868OidxTfoDk78VB66AaGKyZ5k4gEw7Jv4vZKs4gx7XWxwotLu+dDNRpVlrQbbYB5XXht5iBjY8LQ5wjSEiOY3uLA42L552xH9mKwBZPDfuXXGG2FxkO2CLc8RblqzzwGc3iKQGkkhimvYHojsBIXHiYC2ksev2e/Yv4JORYpmjXhZntguzxP5p7Nui+d3WheKgriyN+4jkIqqd7XMqVZwSTXhObRTN2DLPAsOeFomM8/dUmCOqECGNRPVBksQZ/V8edz3BMngOMGSu48D/0HfixFLKobIP+o0hwMDZIvmtygFecYE4W27va1GC1k8kJ3np5SeagNjnXj1LnIgrL/EyF4oD4n1fiMcavaMKpjRxkrTAiOdEH86V94pEv3Fv0YSxouZh5kKxj794Wuwfc1JEs9xLCWL7UP4Xet8NjHih7BN4xfniN+SPheLxkLmimZRWL6qV/7vb0SSryy3A+PfK7ltnMQPHfwR2LxXJYcn1ZjCJpLnwsnhe+b79G4jtsNe/xfo8/Zv4fT+3W8xTmOP02yI5EtE2I8V+slEPRxO+3R+L/+fqkrTipItGJ7bdrHOO78i/gfiO+9zj+h0X5QJLrxP7Er9goN11Yv92KcFRyew3U+xPvn9A7OsUHSfamqkbRUXw91dE7BqdYz8Y3Rm49kmxbfs4JkE+wGqV0euFDbiYW4QV9/VD72Zu7BxLZVOai62d49jZtxktDgaCY0TzpJiujrbzCdvFwYOlSPxYnt8HJGwVO5y71juauhXlAe9eJQ60+94PjH7JlhRNgjiIeEmc0TGGiBiOfi8OfC6dEa8PjBI7zF7/Lv0DlrQfWDZGnFlVtB4hDrr3L3c96Gw6ALj+zcrPAp3bIw62046Kx9e95jrgQG6GeL44T1SLGnZwfX3qEfEjEneVayfNzLOir0vGKaDZ1SLO2B5l/wBmJoof0ktnHD9c2cmu/WQ0OlEBa9LXMe3Qd8D/7hL3Ww0XB1rl9XVQyLI48D6+Riyr7WhxxrEqbZuzzouz4vkXxdnOW7/07IAbVgvw/UxxFhwQyUebUeLguDhfrKeiXPHD16C5SHaVs9AlybI4sKis4qkcsOddFCcPlCS0JrLOAYdWAAe+djRja9wHGP2yOPiwWkUzqS2vieZHUe1F88jY7uLzpR0VyUZxPjBwVtWbBZbk3ETqnk1i2WXJzxQHt/s+F2f4b/3S0WyyLMp3EBjp3rOjlmKxvSvvWZQnOpTveBf2RLjXv8X/Y036UBRkiQqh0o+tKn6Za1u+TdxVIpl03jayzjsS4NObxIHw0HlAr7uqPqiEuUj8f1Wnv03WOZHAnv5NNLUaNLvs+Ypyxec+t0ssp2FHsb8PbVqz7y/lsHiv1MPifsbJ0mfldf4i+WvQTCxT2b/FdBEJgym28uXIsqjK//K4SOKiOwOD/itOJP30H+DQt67ztx0jqsdlnVypjsLL4veuKoN1FOWKOJ1PQgAiudn5vviNGDynav1uzEVif5Dwp0gkWwwRFXplW7NaxDrPSRXVv8r2acqoaFq/ir/n5AOi4usXJH7PGzQTr7mUIH6bshLFsgLCxQm93AvAibVA2pHS72UwiSbI/WY4KrElZSeLJrfOTXwV/R8SlT9JEv//G58V+6W+94ltvOT/k9Va9f8xH8MkyEfM/HIvfjqYjIeGtsas4dUsX1LNFWSLsvXOD8o+CxTRVuyED38nfuDajwVu+kz8mC29TuwAw1qIphQB4eKs6MGvxZnKGTscZ2U3v+x6RrpBc3GApZwJD4oGrplrO8NeQRXh9CZR3YruJEaYUZICWbYt+xtxkNzl1uodeOz7TOzYO46v2mtqQ06qOFOWclAcGDu3iVcc/V7ctr+hdsr0qUfFwVbPqRWfcXcXq1W00w5rXrUDprpIlsV3WJQnzpiW9T15+kf87F/igK4qbf4vXxAnIOpYsw8kbBeJSMcJQKeJtbvsolxgSX9xMGhqDEzfVHF/SnOROLFQmx2nCy+XPgivbeYi26AGtpHkGjRz3zpIPiiSieaDHNumkiCtmyuam/W+W/RpqgMd1uuNi/FidE+dQSTUoU3Evroq34HVKhLpo6uBI6uBnCRRORo6r/T+p7wR9+o5JkE+4ptdZzHnf4fQpXEIfph5tdrh1D8ph0Sfl+xkURnKTBQJjnNiFNNVDJOsnB3OThJN1Eq2iQZKj6ZktYqKz5HVjs7wgDhTOOAhceanqj/g3BkSUW1LPgBsfVNUWrxlSPT6IvWo+D1qNZS/Dd7KahWVr+oO1lTPMQnyERdyCtHnxQ2QZeDPx69FbGgVO+qT5+RfEqOx/LNJdIQc/XLps/FZ50V77cspthG0LoqmaNfMLf99lf4jWefEj5a3nuEnIiIiUgmTIB9y45I/sTvhEp65oSOm9m+mdjhERERERHVSdXKD+tlryouM7Cg6g647klLJnEREREREVBVMguq4ER3FRTD/ir+IzLyyhgwmIiIiIqLqYBJUxzUND0S76GBYrDI2Hkur/AVERERERFQhJkFeYASbxBERERERuQ2TIC8wooNoErfl5AXkF5VxdWsiIiIiIqoyJkFeoGOsCY1C/VFQbMWWkxfUDoeIiIiIyKsxCfICkiTZR4n79UiqytEQEREREXk3JkFeQhklbuPxVJgtVpWjISIiIiLyXkyCvETvZmEIC/RDZl4x/jydoXY4RERERERei0mQl9BqJIzpHAMAWLXvvMrREBERERF5LyZBXmRCj0YAgF8OpyC30KxyNERERERE3olJkBfpHheK5hGByC+24JfDvGYQEREREVFNMAnyIpIkYUJ3UQ1ikzgiIiIiopphEuRllCRo2+l0pGQVqBwNEREREZH3YRLkZeLCAtC7WQPIMvD9flaDiIiIiIiqi0mQF5rYozEAYOXe85BlWeVoiIiIiIi8C5MgL3Rd5xj46TQ4kZqDo8nZaodDRERERORVmAR5oRB/PYa1jwIArNrLJnFERERERNXBJMhLTegumsR9fyAJZotV5WiIiIiIiLwHkyAvNbhNJMIC/XAhpxBbT6WrHQ4RERERkddgEuSl/HQajO0SA4DXDCIiIiIiqg4mQV5sgm2UuHVHUnC50KxyNERERERE3oFJkBfr2jgELSICUVBsxS+HU9QOh4iIiIjIKzAJ8mKSJGFij0YAgJV7z6kcDRERERGRd2AS5OXGdRNJ0PZ/MpCcla9yNEREREREdR+TIC8XFxaAPs3DIMvA6n1JaodDRERERFTnMQnyARO7O5rEybKscjRERERERHUbkyAfcF2XGPjpNDiZdhmHzmepHQ4RERERUZ3GJMgHmIx6jO4UDQD49M8ElaMhIiIiIqrbmAT5iDsHNAcA/HggCRdyClWOhoiIiIio7mIS5CO6xYWie5NQFFmsWP4Xq0FEREREROVhEuRD/m2rBn2xIwGFZovK0RARERER1U1MgnzIqE7RiDYZkX65CD8eSFY7HCIiIiKiOolJkA/RazWY0r8pAGDptngOl01EREREVAYmQT7mtt5NYNRrcCQpGzvjL6odDhERERFRncMkyMc0CPTDhO6NAQBLt51RNxgiIiIiojqISZAPunNAMwDAr0dTkHgxT91giIiIiIjqGCZBPqhNw2AMbB0Bqwx8+ucZtcMhIiIiIqpTmAT5KGW47G92J+JyoVnlaIiIiIiI6g5Vk6AtW7Zg7NixiI2NhSRJWL16tZrh+JTBbSLRIiIQOQVm/G/PObXDISIiIiKqM1RNgnJzc9G1a1e88847aobhkzQaCdNsfYOW/XkGViuHyyYiIiIiAgCdmgsfPXo0Ro8erWYIPm1Sj8Z4dd0JxKfnYvPfabi2XUO1QyIiIiIiUp1X9QkqLCxEdna2yx+VL9Cgw6294wAAn2w9o24wRERERER1hFclQQsWLEBISIj9Ly4uTu2Q6rwp/ZpBIwFbT6XjREqO2uEQEREREanOq5KguXPnIisry/6XmJiodkh1XlxYAEZ0iAYAvL/ltMrREBERERGpz6uSIIPBAJPJ5PJHlbt/SEsAwPf7k3jxVCIiIiKq97wqCaKa6RoXioGtI2Cxyvhgyz9qh0NEREREpCpVk6DLly9j//792L9/PwAgPj4e+/fvx9mzZ9UMyyc9MKQVAHHx1LScApWjISIiIiJSj6pJ0O7du9G9e3d0794dADBr1ix0794d8+bNUzMsn3RVizD0aBKKIrMVH2+NVzscIiIiIiLVqJoEDRkyBLIsl/pbtmyZmmH5JEmSMOMaUQ36YnsCsvKKVY6IiIiIiEgd7BNUj1zbLgrtooORW2TBp9vPqB0OEREREZEqmATVI5Ik4QFbNeiTbfHIKWA1iIiIiIjqHyZB9cyYzjFoGRmIzLxifLL1jNrhEBERERHVOiZB9YxWI+GRYW0AAB/98Q8y84pUjoiIiIiIqHYxCaqHxnSOQbvoYOQUmvHhH7xuEBERERHVL0yC6iGNRsKs4aIatHTbGWRcLlQ5IiIiIiKi2sMkqJ4a3qEhujQOQV6RBe/9flrtcIiIiIiIag2ToHpKkhzVoM+2JyA1u0DliIiIiIiIageToHpscJtI9GraAIVmK97ddErtcIiIiIiIagWToHpMkiTMGiGqQV/uPItzl/JUjoiIiIiIyPOYBNVz/VtGoH/LcBRbZCz+jdUgIiIiIvJ9TIIIj9qqQSv2nMOZ9FyVoyEiIiIi8iwmQYSeTcMwpG0kLFYZb288qXY4REREREQexSSIAACPDm8LAFi1/zxOpeWoHA0RERERkecwCSIAQOfGIRjRoSFkGXhjPatBREREROS7mASR3awRbSBJwM+HkrHjnwy1wyEiIiIi8ggmQWTXLtqE2/o0AQA8ufowisxWlSMiIiIiInI/JkHkYs7IdggP9MOptMv4aOs/aodDREREROR2TILIRUiAHv83pj0A4O2NJ5F4kRdQJSIiIiLfwiSISpnQvRGuahGGgmIr5v9wBLIsqx0SEREREZHbMAmiUiRJwvPjO0GvlbDxeBrWHUlVOyQiIiIiIrdhEkRlahUVjOmDWgAA5n1/GFn5xSpHRERERETkHkyCqFwPXtsaLSICkZZTiBd/PqZ2OEREREREbsEkiMpl1Gvx8o1dAADf7E7EtlPpKkdERERERHTlmARRhXo3C8OUfk0BAI+vPIi8IrPKERERERERXRkmQVSp/45qh9gQIxIv5uP1X/9WOxwiIiIioivCJIgqFWTQ4cWJnQEAn2yLx96zl1SOiIiIiIio5pgEUZUMaRuFiT0aQZaBOd8dRKHZonZIREREREQ1wiSIquypMR0QEeSHk2mX8e6m02qHQ0RERERUI0yCqMoaBPrhmRs6AQDe2XQKx1OyVY6IiIiIiKj6mARRtVzXORojOjSE2Srjv98dhNliVTskIiIiIqJqYRJE1SJJEp4f3wnBRh0OnsvCu5vZLI6IiIiIvAuTIKq2KJMRT4/tCABYuP5v/HQwSeWIiIiIiIiqjkkQ1ciNPRvjzgHNAACzvj2APQkX1Q2IiIiIiKiKmARRjT05pgOGd2iIIrMV93y2BwkZuWqHRERERERUKSZBVGNajYS3bu2GLo1DcDG3CHcu3YVLuUVqh0VEREREVCEmQXRFAvx0+GhqLzQK9cc/6bm49/M9vJAqEREREdVpTILoikUFG7H0zt4INuiw88xF/Pe7g5BlWe2wiIiIiIjKxCSI3KJNw2Asub0ndBoJ3+9PwhsbTqodEhERERFRmZgEkdtc3ToCL07oDAB4e+NJ/G/POZUjIiIiIiIqjUkQudXNveMw45qWAIC5qw7hQGKmugEREREREZXAJIjc7tHhbe1DZ9/3xR5cyClUOyQiIiIiIjsmQeR2Go2EhTd3RYvIQCRnFWDGl3tRbLGqHRYREREREQAmQeQhwUY9PrijF4IMOuyMv4jnfzqqdkhERERERACYBJEHtYoKwhu3dAMAfLo9Ac/8eAQWK4fOJiIiIiJ1MQkijxreoSGeHNMeALB02xlM/2w3cgvNKkdFRERERPUZkyDyuLsHtsA7/+oBg06DjcfTcNN725Gcla92WERERERUTzEJoloxpksMvpp+FSKC/HA0ORuj3/oDaw8lqx0WEREREdVDTIKo1vRo0gCrHhiAjrEmZOYV4/7lezHr2/3ILihWOzQiIiIiqkeYBFGtigsLwKoHBmDGNS2hkYCVe89j9Jt/4PD5LLVDIyIiIqJ6gkkQ1To/nQaPjWyHb+/th7gwf5zPzMeN7/2Jnw+yeRwREREReR6TIFJNr2Zh+PmhgRjcJhIFxVbM+HIvFq7/G1YOo01EREREHsQkiFRlMurxybTeuGdgcwDA2xtPYvrne3Axt0jlyIiIiIjIVzEJItVpNRL+b0wHvHpjF/hpNdhwLBWj3tyCrSfT1Q6NiIiIiHwQkyCqM27qFYdVM/qjZWQg0nIKcfvHf+G5n44iPj0XsswmckRERETkHpLsxUeX2dnZCAkJQVZWFkwmk9rhkJvkF1nw/M9Hsfyvs/ZpjUL9MaBVOAa0ikD/lhGIDDaoGCERERER1TXVyQ2YBFGdtfFYKj7Y8g/2nr2EYovrZtouOhhXt4rA1P7NEBcWoFKERERERFRXMAkin5JXZMauM5ew7VQ6tp5Mx9HkbPtzfloN7ujXFDOvaYUGgX4qRklEREREamISRD4t43Ih/jydga93ncW2UxkAgGCjDv8e0Bw39mzMyhARERFRPcQkiOoFWZax5WQ6Xlp7HMecqkO9mjbA+O6NcEO3WJiMehUjJCIiIqLawiSI6hWrVcZPh5KxYncitp1Kh3KtVX+9Fjd0jcXkq5qgS+NQVWMkIiIiIs9iEkT1Vmp2AX7Yn4QVexLxd+pl+/Q+zcKw6F/d0dBkVDE6IiIiIvIUJkFU78myjD0Jl7D8r7P4+WAyiixWxIQY8cm03mgfw22FiIiIyNdUJzfgxVLJJ0mShF7NwvDGLd2wYdZgtIwMRHJWAW5c8ic2nUhTOzwiIiIiUhGTIPJ5TcIDsPL+AejXIhy5RRbctWwXnv/pKFKyCtQOjYiIiIhUwOZwVG8Uma14YtUhfLfnHABAr5UwvlsjTBvQDO2jTdBoJJUjJCIiIqKaYp8gonLIsoxNJ9Lw3uZ/sPPMRft0k1GHbk0aoHtcKBqF+sPkr4fJXweTUY8Qfz1MRj2CjDpomSgRERER1UlMgoiqYE/CJXyw5TR+//sCCoqtVXpNsEFnS5D0MBlt940iYQo26hFs0CHIqEOAnxZajQSNJEEjiT5Kyn2NRkKAXotAgw6BBh38dK6tUv20Ghj0Ghh0GvhpNZAkJl5ERERElfG6JOidd97Bq6++ipSUFHTt2hWLFi1Cnz59Kn0dkyByh2KLFceTc7Av8RIOJGbhYm4hsgvMyM4vRnZBMbLyi6ucJHmCQScSIoNe67iv00KvlSADsNr+hQ06Lfz1Wvj7aRFg+/PXi4RMIwHKP7osAzJk263jMeyPxXMAYNBr7JWwYKMekuT6eqvT7kMjSdBpJGg1EnRaCVqNxvHYfquBRgNIkCBJ4jWSBEgQiaJ9GmC/D6f7Yl6RTMJpXo0kwaDXwKjT2ps1mi1W5BdbUGi2QiNJ0EoStFoRixIrm0ASERH5jurkBrpaiqlc33zzDWbNmoX33nsPffv2xZtvvomRI0fixIkTiIqKUjs8qgf0Wg06Nw5B58YhQL+y5yk0W5BjT4zEbZYtScrONyMrvxiXC4uRWyjmyysywyrLsMoiqbDaEgarDFisVuQVWnC50IzcQjOKbVd3lSCSkCKztcSyrSg0W4ECs2dXhI/w02ogQ0axpWrnd5RkSEmOrLIMi9WR5CnfmzOlOGdP3lymiaxOcppXSfoc08RrUPJ9SswDiO1HUdYnco5BWV6xRUaxxYpiixVmq+ySLEqSI6l0STg1EqxW521Vhk6jgU4rkle9ViS3eo0GGo2EgmIL8ossyC+2wGyRnd7TsRxNqWqo03JtcSgfr+TpOOVzy07PyU5rwBGbBKsMFNoS3iKztcxla53j0Ig4LFYZBcVW8VqL1b6ORBVXvIdWksTrNbDf12gAq9V2wgBw+l9X/t9t062y/T2UkwEap5MCMgCLVYbZIrY5s1WGxSq+M8B1fWo0Tuu2jO3Bef0p68l1WtnzotJ5S29/Jb8rvVYDP60EP51GrFfb/5DVKtvuO06YaGzrUjkhotNo7N+LMr/ZItv/D5Vbs9P7SRDrT5IAbYnvSInPsd04TvjU1JWcK67pK6/k9HRN472iM+I1fLFa30tJzi0unPeJJfdfykk4ZT8g4nC9dY6t5P+McgKxrOdQ5nNl7AdLvDecXuO8Tvx0GvjrtTDqtdBpJce+3aoci8j2+bW2/bxWI0G2/c9ZZBlWK2C2WmGximMXZV2V/M2SJMCo1+L3x66pdF3XJapXgvr27YvevXtj8eLFAACr1Yq4uDg8+OCDePzxxyt8LStB5ItkWRzAF5ot9gRIOcBzvl9sERUO5YiosNiK/GIz8opsB6dFFuTZDlStslzq4Nr54Bxw7MicD6gLii3ILrAleQVmyJBdDvqVg1lAHBSKgxWr0wGd860VFot47HyAoiSKykGjvTrldIDpOKhx3FemExERkboMOg1OPD9a7TC8pxJUVFSEPXv2YO7cufZpGo0Gw4YNw/bt20vNX1hYiMLCQvvj7OzsWomTqDZJkgQ/nTirGqx2MF5AOWtVaLaioNiCArMVGgn2M2AGnQayDHEmWXYkZS5/sgyL7eyz8xlme0XDlviVqkrIpac5n3V2blpY4TxwJIXKvDIcVSFFye5hzmcNnd9fpxHNJvVacbYdTomjVRbJJoAS1UrZXjFRBgAxW2WYLVYUW0QSW2wRjy2yDKPe1uxSr4NOaZpZRgXNtRLq/Dzsybk9MS/xOZV03PlzK9Uj59i0GglGvWgm6qfTlFqm1eoak4hDfF6jTgujXmPvm6fMaylxxlR5vcU2TakWOTftVJp6KmeKlVidtzWz1Qqr7USHUnVybkaqs293kj1Oi7X0+itr3ZS1rUhOW1HJ9Wi/X9Z6Lne7c51XfBeiAldsEWeMtRrnipq4VbapUv93VkfFRyuJdVDytUoFTdk2lXVgKbF+LE4lW5cKbIkTPtVVk5fVbFnVf1FNllOzz1OD2Gq0nBq8pkZLEsqqmiqVXNllf4Fy918ibsc+rKL9l/K45HPOr3E+Men6+rKeK73vlCBBhowis2gSnl9ksbcI0Nh+2ySnijfg+L8U+yXb/s2phYROK/7/xDqDy/pwrm55G1WToPT0dFgsFjRs2NBlesOGDXH8+PFS8y9YsADPPPNMbYVHRF5Asu2gdVoNAg1l79IkCfBj/x8iIiKy8aqLpc6dOxdZWVn2v8TERLVDIiIiIiIiL6NqJSgiIgJarRapqaku01NTUxEdHV1qfoPBAIPBUFvhERERERGRD1K1EuTn54eePXti48aN9mlWqxUbN25Ev37lDNNFRERERER0BVQfInvWrFmYOnUqevXqhT59+uDNN99Ebm4u7rzzTrVDIyIiIiIiH6R6EnTLLbfgwoULmDdvHlJSUtCtWzf88ssvpQZLICIiIiIicgfVrxN0JXidICIiIiIiAqqXG3jV6HBERERERERXikkQERERERHVK0yCiIiIiIioXmESRERERERE9QqTICIiIiIiqleYBBERERERUb3CJIiIiIiIiOoVJkFERERERFSv6NQO4Eoo13nNzs5WORIiIiIiIlKTkhMoOUJFvDoJysnJAQDExcWpHAkREREREdUFOTk5CAkJqXAeSa5KqlRHWa1WJCUlITg4GJIkqRpLdnY24uLikJiYCJPJpGosvorr2LO4fj2P69izuH49j+vYs7h+PY/r2LPUXr+yLCMnJwexsbHQaCru9ePVlSCNRoPGjRurHYYLk8nEfyoP4zr2LK5fz+M69iyuX8/jOvYsrl/P4zr2LDXXb2UVIAUHRiAiIiIionqFSRAREREREdUrTILcxGAw4Omnn4bBYFA7FJ/FdexZXL+ex3XsWVy/nsd17Flcv57HdexZ3rR+vXpgBCIiIiIioupiJYiIiIiIiOoVJkFERERERFSvMAkiIiIiIqJ6hUkQERERERHVK0yC3OSdd95Bs2bNYDQa0bdvX+zcuVPtkLzSggUL0Lt3bwQHByMqKgrjx4/HiRMnXOYZMmQIJEly+bvvvvtUitj7zJ8/v9T6a9eunf35goICzJgxA+Hh4QgKCsKkSZOQmpqqYsTepVmzZqXWryRJmDFjBgBuvzWxZcsWjB07FrGxsZAkCatXr3Z5XpZlzJs3DzExMfD398ewYcNw8uRJl3kuXryIyZMnw2QyITQ0FHfddRcuX75ci5+i7qpo/RYXF2POnDno3LkzAgMDERsbiylTpiApKcnlPcra7l966aVa/iR1V2Xb8LRp00qtv1GjRrnMw224fJWt37L2yZIk4dVXX7XPw224fFU5NqvKscPZs2cxZswYBAQEICoqCo899hjMZnNtfhQXTILc4JtvvsGsWbPw9NNPY+/evejatStGjhyJtLQ0tUPzOr///jtmzJiBHTt2YP369SguLsaIESOQm5vrMt8999yD5ORk+98rr7yiUsTeqWPHji7rb+vWrfbn/vOf/+DHH3/EihUr8PvvvyMpKQkTJ05UMVrvsmvXLpd1u379egDATTfdZJ+H22/15ObmomvXrnjnnXfKfP6VV17B22+/jffeew9//fUXAgMDMXLkSBQUFNjnmTx5Mo4cOYL169fjp59+wpYtWzB9+vTa+gh1WkXrNy8vD3v37sVTTz2FvXv3YuXKlThx4gRuuOGGUvM+++yzLtv1gw8+WBvhe4XKtmEAGDVqlMv6++qrr1ye5zZcvsrWr/N6TU5OxieffAJJkjBp0iSX+bgNl60qx2aVHTtYLBaMGTMGRUVF+PPPP/Hpp59i2bJlmDdvnhofSZDpivXp00eeMWOG/bHFYpFjY2PlBQsWqBiVb0hLS5MByL///rt92uDBg+WHH35YvaC83NNPPy137dq1zOcyMzNlvV4vr1ixwj7t2LFjMgB5+/bttRShb3n44Yflli1bylarVZZlbr9XCoC8atUq+2Or1SpHR0fLr776qn1aZmambDAY5K+++kqWZVk+evSoDEDetWuXfZ61a9fKkiTJ58+fr7XYvUHJ9VuWnTt3ygDkhIQE+7SmTZvKb7zxhmeD8xFlreOpU6fK48aNK/c13Iarrirb8Lhx4+Rrr73WZRq34aoreWxWlWOHNWvWyBqNRk5JSbHPs2TJEtlkMsmFhYW1+wFsWAm6QkVFRdizZw+GDRtmn6bRaDBs2DBs375dxch8Q1ZWFgAgLCzMZfry5csRERGBTp06Ye7cucjLy1MjPK918uRJxMbGokWLFpg8eTLOnj0LANizZw+Ki4tdtud27dqhSZMm3J5roKioCF988QX+/e9/Q5Ik+3Ruv+4THx+PlJQUl202JCQEffv2tW+z27dvR2hoKHr16mWfZ9iwYdBoNPjrr79qPWZvl5WVBUmSEBoa6jL9pZdeQnh4OLp3745XX31V1WYu3mjz5s2IiopC27Ztcf/99yMjI8P+HLdh90lNTcXPP/+Mu+66q9Rz3IarpuSxWVWOHbZv347OnTujYcOG9nlGjhyJ7OxsHDlypBajd9CpslQfkp6eDovF4vKlAkDDhg1x/PhxlaLyDVarFY888ggGDBiATp062af/61//QtOmTREbG4uDBw9izpw5OHHiBFauXKlitN6jb9++WLZsGdq2bYvk5GQ888wzGDhwIA4fPoyUlBT4+fmVOrhp2LAhUlJS1AnYi61evRqZmZmYNm2afRq3X/dStsuy9sHKcykpKYiKinJ5XqfTISwsjNt1NRUUFGDOnDm47bbbYDKZ7NMfeugh9OjRA2FhYfjzzz8xd+5cJCcnY+HChSpG6z1GjRqFiRMnonnz5jh9+jSeeOIJjB49Gtu3b4dWq+U27EaffvopgoODSzXz5jZcNWUdm1Xl2CElJaXM/bTynBqYBFGdNWPGDBw+fNilvwoAlzbQnTt3RkxMDIYOHYrTp0+jZcuWtR2m1xk9erT9fpcuXdC3b180bdoU3377Lfz9/VWMzPd8/PHHGD16NGJjY+3TuP2StyouLsbNN98MWZaxZMkSl+dmzZplv9+lSxf4+fnh3nvvxYIFC2AwGGo7VK9z66232u937twZXbp0QcuWLbF582YMHTpUxch8zyeffILJkyfDaDS6TOc2XDXlHZt5IzaHu0IRERHQarWlRsBITU1FdHS0SlF5v5kzZ+Knn37Cpk2b0Lhx4wrn7du3LwDg1KlTtRGazwkNDUWbNm1w6tQpREdHo6ioCJmZmS7zcHuuvoSEBGzYsAF33313hfNx+70yynZZ0T44Ojq61EA1ZrMZFy9e5HZdRUoClJCQgPXr17tUgcrSt29fmM1mnDlzpnYC9DEtWrRARESEfb/Abdg9/vjjD5w4caLS/TLAbbgs5R2bVeXYITo6usz9tPKcGpgEXSE/Pz/07NkTGzdutE+zWq3YuHEj+vXrp2Jk3kmWZcycOROrVq3Cb7/9hubNm1f6mv379wMAYmJiPBydb7p8+TJOnz6NmJgY9OzZE3q93mV7PnHiBM6ePcvtuZqWLl2KqKgojBkzpsL5uP1emebNmyM6Otplm83OzsZff/1l32b79euHzMxM7Nmzxz7Pb7/9BqvVak9CqXxKAnTy5Els2LAB4eHhlb5m//790Gg0pZpwUdWcO3cOGRkZ9v0Ct2H3+Pjjj9GzZ0907dq10nm5DTtUdmxWlWOHfv364dChQy7JvHJCpUOHDrXzQUpSZTgGH/P111/LBoNBXrZsmXz06FF5+vTpcmhoqMsIGFQ1999/vxwSEiJv3rxZTk5Otv/l5eXJsizLp06dkp999ll59+7dcnx8vPz999/LLVq0kAcNGqRy5N7j0UcflTdv3izHx8fL27Ztk4cNGyZHRETIaWlpsizL8n333Sc3adJE/u233+Tdu3fL/fr1k/v166dy1N7FYrHITZo0kefMmeMyndtvzeTk5Mj79u2T9+3bJwOQFy5cKO/bt88+OtlLL70kh4aGyt9//7188OBBedy4cXLz5s3l/Px8+3uMGjVK7t69u/zXX3/JW7dulVu3bi3fdtttan2kOqWi9VtUVCTfcMMNcuPGjeX9+/e77JeVEZ3+/PNP+Y033pD3798vnz59Wv7iiy/kyMhIecqUKSp/srqjonWck5Mjz549W96+fbscHx8vb9iwQe7Ro4fcunVruaCgwP4e3IbLV9k+QpZlOSsrSw4ICJCXLFlS6vXchitW2bGZLFd+7GA2m+VOnTrJI0aMkPfv3y//8ssvcmRkpDx37lw1PpIsy7LMJMhNFi1aJDdp0kT28/OT+/TpI+/YsUPtkLwSgDL/li5dKsuyLJ89e1YeNGiQHBYWJhsMBrlVq1byY489JmdlZakbuBe55ZZb5JiYGNnPz09u1KiRfMstt8inTp2yP5+fny8/8MADcoMGDeSAgAB5woQJcnJysooRe59169bJAOQTJ064TOf2WzObNm0qc78wdepUWZbFMNlPPfWU3LBhQ9lgMMhDhw4tte4zMjLk2267TQ4KCpJNJpN85513yjk5OSp8mrqnovUbHx9f7n5506ZNsizL8p49e+S+ffvKISEhstFolNu3by+/+OKLLgfw9V1F6zgvL08eMWKEHBkZKev1erlp06byPffcU+pEKrfh8lW2j5BlWX7//fdlf39/OTMzs9TruQ1XrLJjM1mu2rHDmTNn5NGjR8v+/v5yRESE/Oijj8rFxcW1/GkcJFmWZQ8VmYiIiIiIiOoc9gkiIiIiIqJ6hUkQERERERHVK0yCiIiIiIioXmESRERERERE9QqTICIiIiIiqleYBBERERERUb3CJIiIiIiIiOoVJkFERERERFSvMAkiIqJ6S5IkrF69Wu0wiIioljEJIiIiVUybNg2SJJX6GzVqlNqhERGRj9OpHQAREdVfo0aNwtKlS12mGQwGlaIhIqL6gpUgIiJSjcFgQHR0tMtfgwYNAIimakuWLMHo0aPh7++PFi1a4LvvvnN5/aFDh3DttdfC398f4eHhmD59Oi5fvuwyzyeffIKOHTvCYDAgJiYGM2fOdHk+PT0dEyZMQEBAAFq3bo0ffvjBsx+aiIhUxySIiIjqrKeeegqTJk3CgQMHMHnyZNx66604duwYACA3NxcjR45EgwYNsGvXLqxYsQIbNmxwSXKWLFmCGTNmYPr06Th06BB++OEHtGrVymUZzzzzDG6++WYcPHgQ1113HSZPnoyLFy/W6uckIqLaJcmyLKsdBBER1T/Tpk3DF198AaPR6DL9iSeewBNPPAFJknDfffdhyZIl9ueuuuoq9OjRA++++y4+/PBDzJkzB4mJiQgMDAQArFmzBmPHjkVSUhIaNmyIRo0a4c4778Tzzz9fZgySJOHJJ5/Ec889B0AkVkFBQVi7di37JhER+TD2CSIiItVcc801LkkOAISFhdnv9+vXz+W5fv36Yf/+/QCAY8eOoWvXrvYECAAGDBgAq9WKEydOQJIkJCUlYejQoRXG0KVLF/v9wMBAmEwmpKWl1fQjERGRF2ASREREqgkMDCzVPM1d/P39qzSfXq93eSxJEqxWqydCIiKiOoJ9goiIqM7asWNHqcft27cHALRv3x4HDhxAbm6u/flt27ZBo9Ggbdu2CA4ORrNmzbBx48ZajZmIiOo+VoKIiEg1hYWFSElJcZmm0+kQEREBAFixYgV69eqFq6++GsuXL8fOnTvx8ccfAwAmT56Mp59+GlOnTsX8+fNx4cIFPPjgg7jjjjvQsGFDAMD8+fNx3333ISoqCqNHj0ZOTg62bduGBx98sHY/KBER1SlMgoiISDW//PILYmJiXKa1bdsWx48fByBGbvv666/xwAMPICYmBl999RU6dOgAAAgICMC6devw8MMPo3fv3ggICMCkSZOwcOFC+3tNnToVBQUFeOONNzB79mxERETgxhtvrL0PSEREdRJHhyMiojpJkiSsWrUK48ePVzsUIiLyMewTRERERERE9QqTICIiIiIiqlfYJ4iIiOokttYmIiJPYSWIiIiIiIjqFSZBRERERERUrzAJIiIiIiKieoVJEBERERER1StMgoiIiIiIqF5hEkRERERERPUKkyAiIiIiIqpXmAQREREREVG98v/KPNCKJNQJPQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "- **Increased Convolutional Filters**: Improved feature extraction with deeper layers.\n",
        "- **Added Batch Normalization**: Enhanced training stability and convergence.\n",
        "- **Modified Pooling Layers**: Better spatial feature selection with additional pooling.\n",
        "- **Increased Dropout Regularization**: Reduced overfitting with added dropout layers.\n",
        "- **Updated Fully Connected Layers**: Enhanced capacity and stability in dense layers.\n",
        "- **Overall Performance**: Achieved 66.11% accuracy on the test set, indicating significant improvements in model performance.\n",
        "\n",
        "Test set: Average loss: 1.6720, Accuracy: 6611/10000 (66.11%)\n"
      ],
      "metadata": {
        "id": "jEPoB4IpAr-Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8noZ-NF05Lo"
      },
      "source": [
        "##### Save the Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Ji_seiu70ipd"
      },
      "outputs": [],
      "source": [
        "PATH = f\"model_cifar100_{groupid}\"\n",
        "torch.save(model, PATH)\n",
        "hyperparameters = {\n",
        "    'activation_fn': activation_fn,\n",
        "    'batch_size': batch_size,\n",
        "    'd_in': d_in,\n",
        "    'd_out': d_out,\n",
        "    'learning_rate': learning_rate,\n",
        "    'n_epoch': 200,\n",
        "}\n",
        "import pickle\n",
        "PATH_HYP = f\"hyperparameters_cifar100\"\n",
        "with open(PATH_HYP, 'wb') as f:\n",
        "    pickle.dump(hyperparameters, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}